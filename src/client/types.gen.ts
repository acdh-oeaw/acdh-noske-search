// This file is auto-generated by @hey-api/openapi-ts

export type _corp_info = {
  /**
   * A list of WPOS (`Word Part Of Speech`). Presented as a pair of WPOS names and regular expression matching the WPOS tags.
   */
  wposlist?: Array<Array<string>>;
  /**
   * A list of LPOS (`Lemma Part Of Speech`). Presented as a pair of LPOS names and LPOS tags. Used in Concordance forms.
   */
  lposlist?: Array<Array<string>>;
  /**
   * Has the same format as LPOSLIST but WSPOSLIST is used in Word Sketch and Thesaurus forms.
   */
  wsposlist?: Array<Array<string>>;
  /**
   * A list of objects containing detailed information about attributes occuring in specified corpora.
   */
  attributes?: Array<{
    /**
     * Name of attribute. Lempos_lc = Lemma part of speech lowercase.
     */
    name?: string;
    /**
     * The number of attributes of the given name in the corpus. Each is counted only once even if it appears in corpus many times.
     */
    id_range?: number;
    /**
     * An extra description.
     */
    label?: string;
    /**
     * Represents the rule according which the attribute should be derived from the original attribute. The attribute `lempos_lc` is derived from `lempos` to save disk space etc. [Read more](https://www.sketchengine.eu/documentation/corpus-configuration-file-all-features/#Dynamicattributes).
     */
    dynamic?: string;
    /**
     * A name of attribute this attribute is derived from. Empty string if the attribute is not derived from any.
     */
    fromattr?: string;
  }>;
  /**
   * A list of structures in the corpus.
   */
  structs?: Array<string>;
  /**
   * The full name of the corpus.
   */
  name?: string;
  /**
   * The language of the corpus.
   */
  lang?: string;
  /**
   * An URL with more information about the corpus. Empty string if none.
   */
  infohref?: string;
  /**
   * More information about the corpus.
   */
  info?: string;
  /**
   * The used character encoding in the corpus.
   */
  encoding?: string;
  /**
   * An URL with more information about the POS tagger used in the corpus. Information like meanings of POS tags, comparition with other tagsets for specified language etc.
   */
  tagsetdoc?: string;
  /**
   * The default attribute for the corpus. Usually `word` or `lc`.
   */
  defaultattr?: string;
  starattr?: string;
  /**
   * A boolean value indicating if the corpus is unicameral (not distinguishing between upper and lower case).
   */
  unicameral?: boolean;
  /**
   * The order of writing in language of the used corpus.
   */
  righttoleft?: boolean;
  errsetdoc?: string;
  /**
   * The attribute name for which word sketches are computed, e.g. `lempos`.
   */
  wsattr?: string;
  /**
   * A path to the `used` word sketches grammar definition file.
   */
  wsdef?: string;
  /**
   * A path to the term grammar definition file used in the corpus.
   */
  termdef?: string;
  /**
   * A list of diachronic subcorporas. Diachronic corpus is corpus with timestamps to watch development of the language in time.
   */
  diachronic?: Array<string>;
  /**
   * A list of aligned corpora names. `Example used here is from different corpus because BNC corpus is not parallel.`
   */
  aligned?: Array<Array<string>>;
  /**
   * Is shown only if the specified corpus is parallel. A list of dictionaries containing detailed information about each aligned subcorpus.
   */
  aligned_details?: Array<{
    /**
     * The name of the aligned corpus.
     */
    name?: string;
    /**
     * The language of the aligned corpus.
     */
    language_name?: string;
    Wposlist?: Array<{
      /**
       * The name of the part of speech category
       */
      n?: string;
      /**
       * A regex matching the category
       */
      v?: string;
    }>;
    Lposlist?: Array<{
      /**
       * Name of part of speech category
       */
      n?: string;
      /**
       * The shortcut for better representation of a speech category.
       */
      v?: string;
    }>;
    /**
     * Represent if the language of aligned corpus differentiates between upper case and lower case.
     */
    has_case?: boolean;
    /**
     * Represent if the language of aligned corpus has lemma of not.
     */
    has_lemma?: boolean;
    /**
     * URL with closer information.
     */
    tagsetdoc?: string;
  }>;
  /**
   * A list of attributes (text types) that will be used for Frequency. Text types are metadata attached to the corpus structures. You can access it via Sketch Engine dashboard -> Corpus Info -> Text Type Analysis.
   */
  freqttattrs?: Array<string>;
  /**
   * A list of subcorpus attributes for the corpus.
   */
  subcorpattrs?: Array<string>;
  /**
   * The attribute of a structure to display as a default reference in the left-hand column of a concordance. The syntax is like `=structure.attribute`, e.g. `=doc.id` for displaying only the value of `doc.id`.
   */
  shortref?: string;
  /**
   * A structures that is considered as individual documents. Usually `doc`.
   */
  docstructure?: string;
  /**
   * Information about the new version of the corpus, if available. Empty string if not.
   */
  newversion?: string;
  /**
   * A list of structures appearing in corpus.
   */
  structures?: Array<{
    /**
     * The name of the structure.
     */
    name?: string;
    /**
     * Just some extra information. Empty string if none.
     */
    label?: string;
    /**
     * More detailed information.
     */
    attributes?: Array<{
      /**
       * Name of attribute.
       */
      name?: string;
      /**
       * An extra information about attribute. Empty string if none.
       */
      label?: string;
      /**
       * Dynamic (derived) attribute. Empty string if none. [Read more](https://www.sketchengine.eu/documentation/corpus-configuration-file-all-features/#Dynamicattributes)
       */
      dynamic?: string;
      /**
       * A name of attribute this attribute is derived from. Empty string if none.
       */
      fromattr?: string;
      /**
       * Number of occurences.
       */
      size?: number;
    }>;
    /**
     * Number of occurences of `head` structure in this case.
     */
    size?: number;
  }>;
  /**
   * A boolean value indicating if the corpus is an error corpus (not compiled etc.).
   */
  is_error_corpus?: boolean;
  /**
   * The structural context for the corpus. Empty string if none.
   */
  structctx?: string;
  /**
   * A boolean value indicating if the default filter link is enabled.
   */
  deffilerlink?: boolean;
  /**
   * A list of default structures for the corpus.
   */
  defaultstructs?: Array<string>;
  /**
   * The text types for which the word highlights [Read more](https://www.sketchengine.eu/find-x-word-highlights/) are computed.
   */
  wsttattrs?: string;
  /**
   * A boolean value indicating if the terms file is compiled.
   */
  terms_compiled?: boolean;
  /**
   * A date of compilation in format `mm/dd/yyyy hh:mm:ss`.
   */
  compiled?: string;
  gramrels?: Array<string>;
  sizes?: {
    tokencount?: string;
    wordcount?: string;
    /**
     * Document counter.
     */
    doccount?: string;
    /**
     * Paragraph counter.
     */
    parcount?: string;
    /**
     * Sentences counter.
     */
    sentcount?: string;
    /**
     * Wordcount after normalization.
     */
    normsum?: string;
  };
  /**
   * A list of tuples containing sizes of aligned corpora.
   */
  alsizes?: Array<string>;
  /**
   * The registry dump for the corpus (detailed information about corpus setting), if the registry parameter is set.
   */
  registry_dump?: string;
  /**
   * The registry text for the corpus (detailed information about corpus setting), if the registry parameter is set.
   */
  registry_text?: string;
  subcorpora?: Array<{
    /**
     * A name of subcorpus.
     */
    n?: string;
    /**
     * A name of subcorpus.
     */
    name?: string;
    /**
     * Represent if subcorpora is created by user or not.
     */
    user?: number;
    /**
     * Number of tokens in subcorpus.
     */
    tokens?: number;
    /**
     * The percentage of subcorpus size from total corpus size.
     */
    relsize?: number;
    /**
     * Number of words in subcorpus.
     */
    words?: number;
    struct?: string;
    query?: string;
  }>;
  /**
   * Current API version.
   */
  api_version?: string;
  /**
   * Current version of Manatee.
   */
  manatee_version?: string;
  /**
   * Just summary section of parsed query parameters used in this endpoint call. These parameters are all documented in the beggining of every endpoint box (after you unwrap the endpoint).
   */
  request?: {
    subcorpora?: string;
    struct_attr_stats?: string;
    corpname?: string;
  };
};

export type _wordlist = {
  new_maxitems?: number;
  /**
   * Word list limit, amount of words to be display.
   */
  wllimit?: number;
  lastpage?: number;
  /**
   * An additional note to displayed results.
   */
  note?: string;
  /**
   * Number if displayed items (word - frequency).
   */
  total?: number;
  /**
   * Sum of all frequencies.
   */
  totalfrq?: number;
  /**
   * A result list.
   */
  items?: Array<{
    /**
     * The word to which the frequency has been calculated.
     */
    str?: string;
    /**
     * The word frequency.
     */
    frq?: number;
    /**
     * The relative word frequency. Relative frequency is a way of expressing how often something happens compared to other events or items in a given group.
     */
    relfreq?: number;
  }>;
  wlattr_label?: string;
  /**
   * Frequency type. Other possible values can be `average reduced frequency`, `document frequency`, `score`.
   */
  frtp?: string;
  api_version?: string;
  manatee_version?: string;
  /**
   * Just summary section of parsed query parameters used in this endpoint call. These parameters are all documented in the beggining of every endpoint box.
   */
  request?: {
    wlminfreq?: string;
    random?: string;
    include_nonwords?: string;
    wltype?: string;
    wlmaxitems?: string;
    wlsort?: string;
    wlicase?: string;
    wlpage?: string;
    reldocf?: string;
    wlpat?: string;
    relfreq?: string;
    wlattr?: string;
    wlmaxfreq?: string;
    corpname?: string;
  };
};

export type _concordance = {
  Lines?: Array<{
    toknum?: number;
    hitlen?: number;
    Refs?: Array<string>;
    Tbl_refs?: Array<string>;
    Left?: Array<{
      strc?: string;
    }>;
    Kwic?: Array<{
      str?: string;
      coll?: number;
    }>;
    Right?: Array<{
      str?: string;
    }>;
    Links?: Array<string>;
    linegroup?: string;
    linegroup_id?: number;
  }>;
  fromp?: number;
  concsize?: number;
  concordance_size_limit?: number;
  Sort_idx?: Array<string>;
  righttoleft?: boolean;
  Aligned_rtl?: Array<string>;
  numofcolls?: number;
  finished?: number;
  fullsize?: number;
  relsize?: number;
  q?: Array<string>;
  Desc?: {
    op?: string;
    arg?: string;
    nicearg?: string;
    rel?: number;
    size?: number;
    tourl?: string;
  };
  port?: number;
  gdex_scores?: Array<string>;
  sc_strcts?: Array<Array<string>>;
  api_version?: string;
  manatee_version?: string;
  request?: {
    concordance_query?: Array<{
      queryselector?: string;
      iquery?: string;
    }>;
    corpname?: string;
    kwicleftctx?: string;
    structs?: string;
    viewmode?: string;
    attr_allpos?: string;
    fromp?: string;
    json?: string;
    kwicrightctx?: string;
    refs?: string;
    cup_hl?: string;
    attrs?: string;
    pagesize?: string;
  };
};

export type _subcorp = {
  subcname?: string;
  SubcorpList?: Array<{
    n?: string;
    name?: string;
    user?: number;
  }>;
  api_version?: string;
  manatee_version?: string;
  request?: {
    corpname?: string;
  };
};

export type _extract_keywords = {
  keywords?: Array<{
    item?: string;
    score?: number;
    frq1?: number;
    frq2?: number;
    rel_frq1?: number;
    rel_frq2?: number;
    query?: string;
  }>;
  referece_corpus_name?: string;
  reference_corpus_size?: number;
  reference_subcorpus_size?: number;
  subcorpus_size?: number;
  corpus_size?: number;
  total?: number;
  totalfrq1?: number;
  totalfrq2?: number;
  wllimit?: number;
  note?: string;
  api_version?: string;
  manatee_version?: string;
  request?: {
    alnum?: string;
    maxfreq?: string;
    minfreq?: string;
    wlpat?: string;
    attr?: string;
    keywords?: string;
    ref_corpname?: string;
    simple_n?: string;
    k_attr?: string;
    include_nonwords?: string;
    reldocf?: string;
    icase?: string;
    onealpha?: string;
    max_keywords?: string;
    corpname?: string;
  };
};

export type _attr_vals = {
  /**
   * The regular expression from query parameter `avpat`.
   */
  query?: string;
  /**
   * Suggestions for avattr `bncdoc.author`.
   */
  suggestions?: Array<string>;
  /**
   * Represent if the `suggestion` list is complete.
   */
  no_more_values?: boolean;
  api_version?: string;
  manatee_version?: string;
  request?: {
    avpat?: string;
    avmaxitems?: string;
    ajax?: string;
    corpname?: string;
    avfrom?: string;
    icase?: string;
    avattr?: string;
  };
};

export type _collx = {
  Head?: Array<{
    n?: string;
    s?: string;
    style?: string;
  }>;
  Items?: Array<{
    str?: string;
    freq?: number;
    coll_freq?: number;
    Stats?: Array<{
      s?: string;
      n?: string;
    }>;
    pfilter?: string;
    nfilter?: string;
  }>;
  lastpage?: number;
  wllimit?: number;
  concsize?: number;
  Desc?: Array<{
    op?: string;
    arg?: string;
    nicearg?: string;
    rel?: number;
    size?: number;
    tourl?: string;
  }>;
  api_version?: string;
  manatee_version?: string;
  /**
   * Just summary section of parsed query parameters used in this endpoint call. These parameters are all documented in the beggining of every endpoint box (after you unwrap the endpoint).
   */
  request?: {
    csortfn?: string;
    corpname?: string;
    q?: string;
  };
};

export type _freqml = {
  fcrit?: string;
  FCrit?: Array<{
    fcrit?: string;
  }>;
  Blocks?: Array<{
    Head?: Array<{
      n?: string;
      s?: number;
      id?: string;
    }>;
    total?: number;
    totalfrq?: number;
    Items?: Array<{
      Word?: Array<{
        n?: string;
      }>;
      frq?: number;
      rel?: number;
      reltt?: number;
      norm?: number;
      fbar?: number;
      relbar?: number;
      freqbar?: number;
      pfilter?: string;
      nfilter?: string;
      pfilter_list?: Array<Array<string>>;
      poc?: number;
      fpm?: number;
    }>;
  }>;
  paging?: number;
  concsize?: number;
  fullsize?: number;
  Desc?: Array<{
    op?: string;
    arg?: string;
    nicearg?: string;
    rel?: number;
    size?: number;
    tourl?: string;
  }>;
  numofcolls?: number;
  hitlen?: number;
  wllimit?: number;
  lastpage?: number;
  ml?: boolean;
  api_version?: string;
  manatee_version?: string;
  /**
   * Just summary section of parsed query parameters used in this endpoint call. These parameters are all documented in the beggining of every endpoint box.
   */
  request?: {
    concordance_query?: Array<{
      queryselector?: string;
      iquery?: string;
    }>;
    format?: string;
    fpage?: string;
    showpoc?: string;
    freqlevel?: string;
    group?: string;
    freq_sort?: string;
    ml1ctx?: string;
    showreltt?: string;
    ml2attr?: string;
    ml1attr?: string;
    ml2ctx?: string;
    fmaxitems?: string;
    corpname?: string;
    showrel?: string;
  };
};

export type _struct_wordlist = {
  fcrit?: string;
  FCrit?: Array<{
    fcrit?: string;
  }>;
  Blocks?: Array<{
    Head?: Array<{
      n?: string;
      s?: number;
      id?: string;
    }>;
    total?: number;
    totalfrq?: number;
    Items?: Array<{
      Word?: Array<{
        n?: string;
      }>;
      frq?: number;
      rel?: number;
      reltt?: number;
      norm?: number;
      fbar?: number;
      relbar?: number;
      freqbar?: number;
      pfilter?: string;
      nfilter?: string;
      pfilter_list?: Array<Array<string>>;
      poc?: number;
      fpm?: number;
    }>;
  }>;
  paging?: number;
  concsize?: number;
  fullsize?: number;
  Desc?: Array<{
    op?: string;
    arg?: string;
    nicearg?: string;
    rel?: number;
    size?: number;
    tourl?: string;
  }>;
  numofcolls?: number;
  hitlen?: number;
  wllimit?: number;
  lastpage?: number;
  ml?: boolean;
  api_version?: string;
  manatee_version?: string;
  /**
   * Just summary section of parsed query parameters used in this endpoint call. These parameters are all documented in the beggining of every endpoint box.
   */
  request?: {
    wlmaxfreq?: string;
    wlpage?: string;
    random?: string;
    wlstruct_attr1?: string;
    wltype?: string;
    fmaxitems?: string;
    wlpat?: string;
    wlnums?: string;
    wlattr?: string;
    wlicase?: string;
    wlmaxitems?: string;
    wlstruct_attr3?: string;
    relfreq?: string;
    include_nonwords?: string;
    wlsort?: string;
    wlstruct_attr2?: string;
    corpname?: string;
    reldocf?: string;
    wlminfreq?: string;
  };
};

export type _freq_distrib = {
  dots?: Array<{
    frq?: number;
    pos?: number;
    beg?: number;
    end?: number;
  }>;
  granularity?: number;
  api_version?: string;
  manatee_version?: string;
  request?: {
    concordance_query?: Array<{
      queryselector?: string;
      lemma?: string;
      lpos?: string;
      qmcase?: number;
    }>;
    structs?: string;
    fc_lemword_type?: string;
    attrs?: string;
    json?: string;
    res?: string;
    fc_lemword_window_type?: string;
    normalize?: string;
    format?: string;
    attr_allpos?: string;
    fc_pos_type?: string;
    fc_pos_wsize?: string;
    refs?: string;
    viewmode?: string;
    lpos?: string;
    corpname?: string;
    default_attr?: string;
    fc_lemword_wsize?: string;
    fc_pos_window_type?: string;
  };
};

export type _fullref = {
  Refs?: Array<{
    name?: string;
    id?: string;
    val?: string;
  }>;
  bncdoc_id?: string;
  bncdoc_author?: string;
  bncdoc_year?: string;
  bncdoc_title?: string;
  bncdoc_info?: string;
  bncdoc_allava?: string;
  bncdoc_alltim?: string;
  bncdoc_alltyp?: string;
  bncdoc_genre?: string;
  u_who?: string;
  s_audio?: string;
  api_version?: string;
  manatee_version?: string;
  request?: {
    corpname?: string;
    pos?: string;
  };
};

export type _textypes_with_norms = {
  Blocks?: Array<{
    Line?: Array<{
      name?: string;
      label?: string;
      attr_doc?: string;
      attr_doc_label?: string;
      Values?: Array<{
        v?: string;
        xcnt?: number;
      }>;
    }>;
  }>;
  Normlist?: Array<{
    n?: string;
    label?: string;
  }>;
  api_version?: string;
  manatee_version?: string;
  request?: {
    corpname?: string;
  };
};

export type _widectx = {
  wrapdetail?: string;
  deletewrap?: boolean;
  content?: Array<{
    str?: string;
    class?: string;
  }>;
  leftlink?: string;
  rightlink?: string;
  pos?: number;
  maxcontent?: number;
  api_version?: string;
  manatee_version?: string;
  request?: {
    detail_left_ctx?: string;
    corpname?: string;
    hitlen?: string;
    pos?: string;
    detail_right_ctx?: string;
    structs?: string;
  };
};

export type _subcorpus_rename = {
  status?: string;
  corpus?: string;
  subcorp_id2name?: {
    test?: string;
  };
  api_version?: string;
  manatee_version?: string;
  request?: {
    subcorp_id?: string;
    new_subcorp_name?: string;
    corpname?: string;
  };
};

export type _subcorp_info = {
  subcorp?: string;
  corpsize?: number;
  subcsize?: number;
  api_version?: string;
  manatee_version?: string;
  request?: {
    subcname?: string;
    corpname?: string;
  };
};

export type _freqdist = {
  lc?: string;
  freqdist?: {
    /**
     * The name is variable according selected period (wlattr).
     */
    "2021-11"?: {
      frq?: number;
      rel_frq?: number;
      period_size?: number;
    };
  };
  removed_freqdist?: {
    /**
     * The name is variable according selected period (wlattr).
     */
    "2023-08"?: {
      frq?: number;
      rel_frq?: number;
      period_size?: number;
    };
  };
  average_norm?: number;
  norm_limit?: number;
};

/**
 * Request for post method to set `name`, `language`, `tagset`, and additional information to the corpus.
 */
export type _corpora_request = {
  /**
   * The additional information for a newly created corpus.
   */
  info?: string;
  /**
   * Language iso-code. `ISO 639-1`.
   */
  language_id?: string;
  /**
   * Unique `corpus name` for a newly created corpus.
   */
  name?: string;
  /**
   * Name of used tagset.
   */
  tagset_id?: string;
};

export type _compile_request = {
  /**
   * `Structures` and `structure attributes` in corpus which should be compiled. Usually: `all`.
   */
  structures?: string;
};

export type _corpus_ids = {
  corpus_ids?: Array<number>;
};

export type _align_req = {
  /**
   * According to which structure the document should be aligned. Usually, `/<s>`.
   */
  alignstruct?: string;
  /**
   * True, when documents are not compiled. Sketch Engine will align them automatically.
   */
  auto?: boolean;
  corpus_ids?: Array<number>;
};

/**
 * In this documentation, an empty request is used mostly used with the `RPC style` method where the content in a request is not needed (in most cases). RPC style endpoints focus on `performing` one action right (procedures, command) easier than REST API-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
 */
export type _empty_request = {
  [key: string]: unknown;
};

/**
 * All possible paramaters that can be changed in user corpus. In corpus update `you don't have to use all parameters`, just the parameters you change.
 */
export type _corpus_update = {
  /**
   * Set to `True` if you are hard-core.
   */
  expert_mode?: boolean;
  /**
   * Corpus name. `Given by user`.
   */
  name?: string;
  /**
   * Additional info about corpus.
   */
  info?: string;
  /**
   * Can be set to enforce document order within the corpus.
   */
  document_order?: Array<number>;
  lang_filter?: boolean;
  /**
   * Available structures or tags in the corpus. Structures like `s` (sentence), `g` (glue), `doc` (document).
   */
  structures?: Array<{
    /**
     * Structure name. Example: `s`
     */
    name?: string;
    /**
     * A list of used attributes in corpus.
     */
    attributes?: Array<{
      /**
       * The name of used attribute.
       */
      name?: string;
    }>;
  }>;
  /**
   * The structure in which individual documents should be wrapped. Usually `doc`.
   */
  file_structure?: string;
  /**
   * The structure for deduplication. Usually `p` (paragraph), `doc` or `Null` (no deduplication).'
   */
  onion_structure?: string;
  /**
   * Structure in which individual documents should be wrapped. Usually `doc`.
   */
  docstructure?: string;
  /**
   * `Sketch grammar ID` (name of sketch grammar file). For sketch grammars querying. Sketch grammar is a series of rules written in the CQL query  language that search for collocations in a text corpus and categorize them according to their  grammatical relations. Example: `preloaded/english-penn_tt-3.3.wsdef.m4`.
   */
  sketch_grammar_id?: string;
  /**
   * `Term grammar ID` (name of term grammar file). Term grammar tells Sketch Engine which words and phrases should indentify as terms. Example: `/corpora/wsdef/english-penn_tt-terms-3.1.termdef.m4`.
   */
  term_grammar_id?: string;
};

export type _doc_put_req = {
  /**
   * Name of documents.
   */
  filename_display?: string;
  /**
   * Unique numeric `document ID` to identify individual documents.
   */
  id?: number;
  /**
   * Represents whether the currently edited document is in use.
   */
  inProgress?: boolean;
  /**
   * Represents if the updated document is in a format like .zip (created via some archive manager).
   */
  isArchive?: boolean;
  /**
   * Metadata of document. For example, additional `attributes and values`.
   */
  metadata?: {
    [key: string]: unknown;
  };
  /**
   * Parameters for plaintext extraction.
   */
  parameters?: {
    /**
     * Encoding standard of the document. Usually, `UTF-8`.
     */
    encoding?: string;
    /**
     * Represent the list of unimportant words, in a specified language, from an NLP point of view.
     */
    justext_stoplist?: string;
    permutation?: Array<number>;
    /**
     * TMX (translation memory exchange). Language of document used for parallel corpus creation.
     */
    tmx_lang?: string;
    /**
     * Alignment structure to be used for multilingual documents, `align` is the most used structure. Used within segment distinction, which sentence is in which language and to put sentences with the same meaning into one segment.
     */
    tmx_struct?: string;
    /**
     * Placeholder for empty segments in multilingual documents. The segments which have no counterpart in a second language of parallel corpus.
     */
    tmx_untranslated?: string;
    /**
     * File format (.csv, .doc, .docx, .htm, .html etc.).
     */
    type?: string;
    /**
     * Convert `all-caps` text to `normal case`.
     */
    unlegalese?: boolean;
  };
  /**
   * Is document temporary or not.
   */
  temporary?: boolean;
  /**
   * Total number of `words` (tokens minus punctuation etc.) in document.
   */
  word_count?: number;
  /**
   * Progress of `vertical file` creation.
   */
  vertical_progress?: number;
  /**
   * An error occured while creating the vertical file. If the creation was succesfull the value is `Null`.
   */
  vertical_error?: string;
};

export type _doc_preview = {
  /**
   * Automatically insert paragraph breaks (`\<p>`) in place of blank lines.
   */
  auto_paragraphs?: string;
  /**
   * Encoding standard of the document. Usually `UTF-8`.
   */
  encoding?: string;
  /**
   * Represent the list of unimportant words, in a specified language, from an NLP point of view.
   */
  justext_stoplist?: string;
  /**
   * Changing the order of columns (applies only to `type=vert`).
   */
  permutation?: Array<number>;
  /**
   * TMX (translation memory exchange). Language of document used for parallel corpus creation.
   */
  tmx_lang?: string;
  /**
   * Alignment structure to be used for multilingual documents, `align` is the most used structure. Used within segment distinction, which sentence is in which language and to put sentences with the same meaning into one segment.
   */
  tmx_struct?: string;
  /**
   * Placeholder for empty segments in multilingual documents. The segments which have no counterpart in a second language of parallel corpus.
   */
  tmx_untranslated?: string;
  /**
   * File format (`.csv`, `.doc`, `.docx`, `.htm`, `.html` etc.).
   */
  type?: string;
  /**
   * Convert `all-caps` text to `normal case`.
   */
  unlegalese?: boolean;
};

export type _doc_metadata = Array<{
  /**
   * Unique numeric `document ID`.
   */
  id?: number;
  /**
   * Pairs of `attribute_name`:`value`.
   */
  metadata?: {
    [key: string]: unknown;
  };
}>;

export type _filesets_creation = {
  /**
   * Stands for: `blacklist max total keyword`. Means that web page or document will be discarded if it contains more words from the denylist (blacklist) than this limit.
   */
  bl_max_total_kw?: number;
  /**
   * Stands for: `blacklist max unique keyword`. Means that web page or document will be discarded if it contains more unique words from the denylist (blacklist) than this limit.
   */
  bl_max_unique_kw?: number;
  /**
   * A list (separated by whitespaces) of `blocked words`, words you don't want to see in your future corpus.
   */
  black_list?: string;
  /**
   * Input types the web-crawler will works with. Example: `urls`
   */
  input_type?: string;
  /**
   * Web pages and documents with a size `over` this limit (`in kB`) will be ignored.
   */
  max_cleaned_file_size?: number;
  /**
   * Web pages and documents with a size `over` this limit (`in kB`) will be ignored.
   */
  max_file_size?: number;
  /**
   * Web pages and documents `smaller` than this limit (`in kB`) after cleaning will be ignored. Cleaning involves conversion to plain text, removing boilerplate text (e.g. navigation menus, legal text, disclaimers and other repetitive content).
   */
  min_cleaned_file_size?: number;
  /**
   * Web pages and documents with a `size below` this limit (`in kB`) will be ignored.
   */
  min_file_size?: number;
  /**
   * Texts will be organized into a corpus folder `with this name`.
   */
  name?: number;
  /**
   * A list of words according to which the `URLs` were chosen to be searched.
   */
  seed_words?: Array<string>;
  /**
   * A list (separated by whitespaces) of `allowed words`, words you want to see in your future corpus.
   */
  white_list?: string;
  /**
   * Stands for: `whitelist minimal keywords ratio`. Means that web page or document will be included only if the `percentage` of allowlist words compared to total words is `higher` than this limit.
   */
  wl_min_kw_ratio?: number;
  /**
   * Stands for: `whitelist minimal total keywords`. Means that web page or document will be included only if it contains `more words` from the `allowlist` (whitelist) than this limit.
   */
  wl_min_total_kw?: number;
  /**
   * Stands for: `whitelist minimal unique keywords`. Means that a web page or document will be included only if it contains `more words` from the `allowlist` (whitelist) than this limit.
   */
  wl_min_unique_kw?: number;
};

export type _somefiles_put = {
  corpora?: {
    guessed_language_code?: {
      /**
       * Language iso-code. `ISO 639-1`.
       */
      language_id?: string;
      /**
       * Language name in `English`.
       */
      name?: string;
    };
  };
};

export type _corpora_list = {
  /**
   * Unique numeric `corpus ID` for corpus building.
   */
  id?: number;
  /**
   * Unique numeric `owner ID` (usually you).
   */
  owner_id?: number;
  /**
   * Corpus `owner name` (usually you).
   */
  owner_name?: string;
  /**
   * Unique `corpus name` for corpus querying.
   */
  corpname?: string;
  /**
   * Language iso-code. `ISO 639-1`.
   */
  language_id?: string;
  /**
   * Language name in `English`.
   */
  language_name?: string;
  /**
   * `Tagset ID`. Tagset is list of part-of-speech tags (POS tags) for specified language. They are `preselected` to the most relevant one and can be changed only in user corpora. `Tagsets` can be refered also as `templates`.
   */
  tagset_id?: number;
  /**
   * `Sketch grammar ID`. Sketch grammar is a series of rules written in the CQL query language that search for collocations in a text corpus and categorize them according to their grammatical relations.
   */
  sketch_grammar_id?: string;
  /**
   * `Term grammar ID`. Term grammar tells Sketch Engine which words and phrases should indentify as terms.
   */
  term_grammar_id?: string;
  /**
   * Corpus sizes. `Null` if corpus is not compiled.
   */
  sizes?: {
    /**
     * Total number of `documents` in corpus.
     */
    doccount?: number;
    /**
     * Total number of `paragraphs` in corpus.
     */
    parcount?: number;
    /**
     * Total number of `sentences` in corpus.
     */
    sentcount?: number;
    /**
     * Total number of `words` (tokens minus punctuation etc.) in corpus.
     */
    wordcount?: number;
    /**
     * Total number of `tokens` in corpus.
     */
    tokencount?: number;
  };
  /**
   * Date and time of corpus creation in format `YYYY-MM-DD HH:MM:SS`.
   */
  created?: string;
  /**
   * `True` if corpus documents have been altered since last compilation.
   */
  needs_recompiling?: boolean;
  /**
   * Corpus can be queried a `specific user`. Ignore all corpora where this is false.
   */
  user_can_read?: boolean;
  /**
   * Corpus can be used as a `reference corpus` even by anonymous users.
   */
  user_can_refer?: boolean;
  /**
   * Corpus is owned by you or shared with you. You can upload documents to it.
   */
  user_can_upload?: boolean;
  /**
   * Corpus is owned by you or shared with you with `full privileges`.
   */
  user_can_manage?: boolean;
  /**
   * True if corpus is shared with other users.
   */
  is_shared?: boolean;
  /**
   * If set, the old corpus is deprecated in favor of a new one.
   */
  new_version?: string;
  /**
   * Corpus name. `Given by user.`
   */
  name?: string;
  /**
   * Additional info about corpus.
   */
  info?: string;
  /**
   * List of other corpora (corpus ID) within the `same` multi-lingual set (parallel corpus).
   */
  aligned?: Array<string>;
  /**
   * Structure in which individual documents should be wrapped. Usually `doc`.
   */
  docstructure?: string;
};

export type _corpora_single = {
  /**
   * Unique numeric `corpus ID` for corpus building.
   */
  id?: number;
  /**
   * Unique numeric `owner ID` (usually you).
   */
  owner_id?: number;
  /**
   * Corpus `owner name` (usually you).
   */
  owner_name?: string;
  /**
   * Unique `corpus name` for corpus querying.
   */
  corpname?: string;
  /**
   * Language iso-code. `ISO 639-1`.
   */
  language_id?: string;
  /**
   * Language name in `English`.
   */
  language_name?: string;
  /**
   * `Sketch grammar ID` (name of sketch grammar file). Sketch grammar is a series of rules written in the CQL query language that search for collocations in a text corpus and categorize them according to their grammatical relations. Example: `preloaded/english-penn_tt-3.3.wsdef.m4`.
   */
  sketch_grammar_id?: string;
  /**
   * `Term grammar ID` (name of term grammar file). Term grammar tells Sketch Engine which words and phrases should indentify as terms. Example: `/corpora/wsdef/english-penn_tt-terms-3.1.termdef.m4`.
   */
  term_grammar_id?: string;
  /**
   * Corpus sizes. `Null` if corpus is not compiled.
   */
  sizes?: {
    /**
     * Total number of `documents` in corpus.
     */
    doccount?: number;
    /**
     * Total number of `paragraphs` in corpus.
     */
    parcount?: number;
    /**
     * Total number of `sentences` in corpus.
     */
    sentcount?: number;
    /**
     * Total number of `words` (tokens minus punctuation etc.) in corpus.
     */
    wordcount?: number;
    /**
     * Total number of `tokens` in corpus.
     */
    tokencount?: number;
  };
  /**
   * Date and time of corpus creation in format `YYYY-MM-DD HH:MM:SS`.
   */
  created?: string;
  /**
   * True if corpus documents have been altered since last compilation.
   */
  needs_recompiling?: boolean;
  /**
   * Corpus can be queried by a `specific user`. Ignore all corpora where this is false.
   */
  user_can_read?: boolean;
  /**
   * Corpus can be used as a `reference corpus` even by anonymous users.
   */
  user_can_refer?: boolean;
  /**
   * Corpus is owned by you or shared with and you can upload documents to it.
   */
  user_can_upload?: boolean;
  /**
   * Corpus is owned by you or shared with you with `full privileges`.
   */
  user_can_manage?: boolean;
  /**
   * `True` if corpus is shared with other users.
   */
  is_shared?: boolean;
  /**
   * If set, the old corpus is deprecated in favor of a new one.
   */
  new_version?: string;
  /**
   * Corpus name. `Given by user`.
   */
  name?: string;
  /**
   * Additional info about corpus.
   */
  info?: string;
  /**
   * Other corpora within the `same` multi-lingual set (parallel corpus).
   */
  aligned?: Array<string>;
  /**
   * Structure in which individual documents should be wrapped. Usually `doc`.
   */
  docstructure?: string;
  /**
   * Current state of corpus.
   */
  is_error_corpus?: boolean;
  /**
   * Attributes appearing in corpus documents. Attributes like: `word`, `tag`, `lempos`, `pos`, `lemma`, etc.
   */
  attrlist?: Array<string>;
  /**
   * Tagset ID. Tagset is list of part-of-speech tags (POS tags) for specified language. They are `preselected` to the most relevant one and can be changed only in user corpora. The terms `tagset` and `templates` are interchangeable.
   */
  tagset_id?: number;
  /**
   * Default reference corpus for `keyword extraction`.
   */
  reference_corpus?: string;
  /**
   * Compilation status: `0` if not compiled, `100` if compiled successfully, `-1` if failed, otherwise in progress.
   */
  progress?: number;
  /**
   * Informs about last compilation error, if any error `None`.
   */
  error?: string;
  /**
   * The amount of documents the corpus was build from.
   */
  document_count?: number;
  /**
   * `True` if corpus template is outdated and can be upgraded. The terms `tagset` and `templates` are interchangeable.
   */
  can_be_upgraded?: boolean;
  /**
   * All `structures`/`attributes` that appear in corpus documents.
   */
  available_structures?: Array<{
    /**
     * Structure name.
     */
    name?: string;
    /**
     * Frequency of structure.
     */
    freq?: number;
    attributes?: Array<string>;
  }>;
  /**
   * The structure in which individual documents should be wrapped. Usually `doc`.
   */
  file_structure?: string;
  /**
   * The structure for deduplication. Usually `p` (paragraph) or `Null` (no deduplication).'
   */
  onion_structure?: string;
  /**
   * Set to `True` if you are hard-core.
   */
  expert_mode?: boolean;
  /**
   * Not mandatory. Can be set to enforce document order within the corpus.
   */
  document_order?: Array<number>;
  /**
   * Use `all` structures available in corpus.
   */
  use_all_structure?: boolean;
  /**
   * Available `structures` or `tags` in the corpus. Structures like `s` (sentence), `g` (glue), `doc` (document).
   */
  structures?: Array<{
    /**
     * Structure name.
     */
    name?: string;
    /**
     * A list of used attributes in corpus.
     */
    attributes?: Array<{
      /**
       * The name of used attribute.
       */
      name?: string;
    }>;
  }>;
};

export type _can_be_compiled = {
  result?: {
    /**
     * True, if the corpus does not contain any potential error, which can break compilation.
     */
    can_be_compiled?: boolean;
    /**
     * Description of problem why it cannot be compiled. If none Null. Example: `QUOTA_EXCEEDED` or `EMPTY`.
     */
    reason?: string;
  };
  /**
   * Unexpected server error. If none Null.
   */
  error?: string;
};

export type _get_progress = {
  result?: {
    /**
     * Compilation status: `0` if not compiled, `100` if compiled successfully, `-1` if failed, otherwise in progress.
     */
    progress?: number;
    /**
     * Problem description. If none Null.
     */
    error?: string;
  };
  /**
   * Unexpected server error. If none Null.
   */
  error?: string;
};

export type _corpora_single_full = {
  /**
   * Unique numeric `corpus ID` for corpus building.
   */
  id?: number;
  /**
   * Unique numeric `owner ID` (usually you).
   */
  owner_id?: number;
  /**
   * Corpus `owner name` (usually you).
   */
  owner_name?: string;
  /**
   * Unique `corpus name` for corpus querying.
   */
  corpname?: string;
  /**
   * Language iso-code. `ISO 639-1`.
   */
  language_id?: string;
  /**
   * Language name in `English`.
   */
  language_name?: string;
  /**
   * `Sketch grammar ID` (name of sketch grammar file). Sketch grammar is a series of rules written in the CQL query language that search for collocations in a text corpus and categorize them according to their grammatical relations. Example: `preloaded/english-penn_tt-3.3.wsdef.m4`.
   */
  sketch_grammar_id?: string;
  /**
   * `Term grammar ID` (name of term grammar file). Term grammar tells Sketch Engine which words and phrases should indentify as terms. Example: `/corpora/wsdef/english-penn_tt-terms-3.1.termdef.m4`.
   */
  term_grammar_id?: string;
  /**
   * Corpus sizes. `Null` if corpus is not compiled.
   */
  sizes?: {
    /**
     * Total number of `documents` in corpus.
     */
    doccount?: number;
    /**
     * Total number of `paragraphs` in corpus.
     */
    parcount?: number;
    /**
     * Total number of `sentences` in corpus.
     */
    sentcount?: number;
    /**
     * Total number of `words` (tokens minus punctuation etc.) in corpus.
     */
    wordcount?: number;
    /**
     * Total number of `tokens` in corpus.
     */
    tokencount?: number;
  };
  /**
   * TODO
   */
  is_sgdev?: boolean;
  /**
   * TOOD
   */
  is_featured?: boolean;
  /**
   * TODO
   */
  access_level?: boolean;
  /**
   * TODO
   */
  access_on_demand?: boolean;
  /**
   * TODO
   */
  terms_of_use?: string;
  /**
   * TODO
   */
  sort_to_end?: boolean;
  tags?: Array<string>;
  /**
   * Date and time of corpus creation in format `YYYY-MM-DD HH:MM:SS`.
   */
  created?: string;
  /**
   * True if corpus documents have been altered since last compilation.
   */
  needs_recompiling?: boolean;
  /**
   * Corpus can be queried by a `specific user`. Ignore all corpora where this is false.
   */
  user_can_read?: boolean;
  /**
   * Corpus can be used as a `reference corpus` even by anonymous users.
   */
  user_can_refer?: boolean;
  /**
   * Corpus is owned by you or shared with and you can upload documents to it.
   */
  user_can_upload?: boolean;
  /**
   * Corpus is owned by you or shared with you with `full privileges`.
   */
  user_can_manage?: boolean;
  /**
   * `True` if corpus is shared with other users.
   */
  is_shared?: boolean;
  /**
   * If set, the old corpus is deprecated in favor of a new one.
   */
  new_version?: string;
  /**
   * Corpus name. `Given by user`.
   */
  name?: string;
  /**
   * Additional info about corpus.
   */
  info?: string;
  /**
   * Default word sketch definition. Example: `/corpora/wsdef/serbian-multext-rft1-1.0.wsdef.txt`.
   */
  wsdef?: string;
  /**
   * Default term definition.
   */
  termdef?: string;
  /**
   * Is this corpus developing over time to keep track in vocabulary changes, grammar and language usage. If yes what time period does the corpus cover.
   */
  diachronic?: string;
  /**
   * Other corpora within the `same` multi-lingual set (parallel corpus).
   */
  aligned?: Array<string>;
  /**
   * Structure in which individual documents should be wrapped. Usually `doc`.
   */
  docstructure?: string;
  /**
   * Current state of corpus.
   */
  is_error_corpus?: boolean;
  /**
   * Attributes appearing in corpus documents. Attributes like: `word`, `tag`, `lempos`, `pos`, `lemma`, etc.
   */
  attrlist?: Array<string>;
  /**
   * Tagset ID. Tagset is list of part-of-speech tags (POS tags) for specified language. They are `preselected` to the most relevant one and can be changed only in user corpora. The terms `tagset` and `templates` are interchangeable.
   */
  tagset_id?: number;
  /**
   * Default reference corpus for `keyword extraction`.
   */
  reference_corpus?: string;
  /**
   * Compilation status: `0` if not compiled, `100` if compiled successfully, `-1` if failed, otherwise in progress.
   */
  progress?: number;
  /**
   * Informs about last compilation error, if any error `None`.
   */
  error?: string;
  /**
   * The amount of documents the corpus was build from.
   */
  document_count?: number;
  /**
   * `True` if corpus template is outdated and can be upgraded. The terms `tagset` and `templates` are interchangeable.
   */
  can_be_upgraded?: boolean;
  /**
   * All `structures`/`attributes` that appear in corpus documents.
   */
  available_structures?: Array<{
    /**
     * Structure name.
     */
    name?: string;
    /**
     * Frequency of structure.
     */
    freq?: number;
    attributes?: Array<string>;
  }>;
  /**
   * The structure in which individual documents should be wrapped. Usually `doc`.
   */
  file_structure?: string;
  /**
   * The structure for deduplication. Usually `p` (paragraph) or `Null` (no deduplication).'
   */
  onion_structure?: string;
  /**
   * Set to `True` if you are hard-core.
   */
  expert_mode?: boolean;
  /**
   * Not mandatory. Can be set to enforce document order within the corpus.
   */
  document_order?: Array<number>;
  /**
   * Use `all` structures available in corpus.
   */
  use_all_structure?: boolean;
  /**
   * Available `structures` or `tags` in the corpus. Structures like `s` (sentence), `g` (glue), `doc` (document).
   */
  structures?: Array<{
    /**
     * Structure name.
     */
    name?: string;
    /**
     * A list of used attributes in corpus.
     */
    attributes?: Array<{
      /**
       * The name of used attribute.
       */
      name?: string;
    }>;
  }>;
};

export type _documents_get = Array<{
  /**
   * Unique numeric `document ID` to identify individual documents from which the corpus was created.
   */
  id?: number;
  /**
   * The name of the document.
   */
  filename_display?: string;
  /**
   * Parameters for plaintext extraction.
   */
  parameters?: {
    /**
     * File format. Possible formats: `.csv`, `.doc`, `.docx`, `.htm`, `.html` etc..
     */
    type?: string;
    /**
     * Encoding standard of the document. Usually `UTF-8`.
     */
    encoding?: string;
    /**
     * TMX (translation memory exchange). Language of document used for parallel corpus creation.
     */
    tmx_lang?: string;
    /**
     * Alignment structure to be used for multilingual documents, `align` is the most used structure. Used within segment distinction, which sentence is in which language and to put sentences with the same meaning into one segment.
     */
    tmx_struct?: string;
    /**
     * Convert `all-caps` text to `normal case`.
     */
    unlegalese?: boolean;
    permutation?: Array<number>;
    /**
     * Automatically insert paragraph breaks (\<p>) in place of blank lines.
     */
    auto_paragraphs?: string;
    /**
     * Represent the list of unimportant words, in a specified language, from an NLP point of view.
     */
    justext_stoplist?: string;
    /**
     * Placeholder for empty segments in multilingual documents. The segments which have no counterpart in a second language of parallel corpus.
     */
    tmx_untranslated?: string;
  };
  /**
   * Is document temporary or not.
   */
  temporary?: boolean;
  /**
   * Total number of `words` (tokens minus punctuation etc.) in document.
   */
  word_count?: number;
  /**
   * Progress of `vertical file` creation.
   */
  vertical_progress?: number;
  /**
   * An error occured while creating the vertical file. If the creation was succesfull the value is `Null`.
   */
  vertical_error?: string;
  /**
   * Metadata of document. For example, additional `attributes and values`.
   */
  metadata?: {
    [key: string]: unknown;
  };
}>;

export type _documents_post = Array<{
  /**
   * Unique numeric `document ID` to identify individual documents from which the corpus was created.
   */
  id?: number;
  /**
   * The name of the document.
   */
  filename_display?: string;
  /**
   * Parameters for plaintext extraction.
   */
  parameters?: {
    /**
     * File format. Possible formats: `.csv`, `.doc`, `.docx`, `.htm`, `.html` etc..
     */
    type?: string;
    /**
     * Alignment structure to be used for multilingual documents, `align` is the most used structure. Used within segment distinction, which sentence is in which language and to put sentences with the same meaning into one segment.
     */
    tmx_struct?: string;
    /**
     * Placeholder for empty segments in multilingual documents. The segments which have no counterpart in a second language of parallel corpus.
     */
    tmx_untranslated?: string;
    /**
     * Convert `all-caps` text to `normal case`.
     */
    unlegalese?: boolean;
    /**
     * Represent the list of unimportant words, in a specified language, from an NLP point of view.
     */
    justext_stoplist?: string;
    /**
     * TMX (translation memory exchange). Language of document used for parallel corpus creation.
     */
    tmx_lang?: string;
    permutation?: Array<number>;
  };
  /**
   * Is document temporary or not.
   */
  temporary?: boolean;
  /**
   * Total number of `words` (tokens minus punctuation etc.) in document.
   */
  word_count?: number;
  /**
   * Progress of `vertical file` creation.
   */
  vertical_progress?: number;
  /**
   * An error occured while creating the vertical file. If the creation was succesfull the value is `Null`.
   */
  vertical_error?: string;
  /**
   * Metadata of document. For example, additional `attributes and values`.
   */
  metadata?: {
    [key: string]: unknown;
  };
}>;

// export type _doc_preview = {
//   /**
//    * Showcase of few lines from the file the corpus was created from (1kB).
//    */
//   result?: string;
//   /**
//    * Unexpected server error. If none Null.
//    */
//   error?: string;
// };

export type _rpc_expand_archive = {
  /**
   * Returns fileset ID.
   */
  result?: number;
  /**
   * Unexpected server error. If none Null.
   */
  error?: string;
};

export type _not_found_404 = {
  error?: string;
};

export type _not_found_RPC = {
  /**
   * Result of succesfully finished request otherwise Null.
   */
  result?: boolean;
  error?: string;
};

export type _bad_request_RPC_1 = {
  /**
   * Result of succesfully finished request otherwise Null.
   */
  result?: boolean;
  /**
   * Examples: `READ_ONLY`, `INVALID_CORPUS_IDS`. You do not have permissions for it or you inputted IDs are not correct.
   */
  error?: string;
};

export type _bad_request_RPC_8 = {
  /**
   * Result of succesfully finished request otherwise Null.
   */
  result?: boolean;
  /**
   * Examples: `QUOTA_EXCEEDED`, `READ_ONLY`, `INVALID_CORPUS_IDS`, `CORPUS_BUSY`.
   */
  error?: string;
};

export type _bad_request_RPC_9 = {
  /**
   * Result of succesfully finished request otherwise Null.
   */
  result?: boolean;
  /**
   * Examples: `QUOTA_EXCEEDED`, `READ_ONLY`.
   */
  error?: string;
};

export type _bad_request_10 = {
  /**
   * Examples: `QUOTA_EXCEEDED`, `READ_ONLY`, `CORPUS_BUSY`, `DAILY_TAGGING_EXCEEDED`, `INVALID_URL`, `NO_DATA`.
   */
  error?: string;
};

export type _bad_request_11 = {
  /**
   * Examples: `READ_ONLY`, `CORPUS_BUSY`, `INVALID_METADATA`.
   */
  error?: string;
};

export type _bad_request_12 = {
  /**
   * Examples: `READ_ONLY`, `CORPUS_BUSY`.
   */
  error?: string;
};

export type _bad_request_13 = {
  /**
   * Examples: `READ_ONLY`.
   */
  error?: string;
};

export type _bad_request_14 = {
  /**
   * Examples: `READ_ONLY`, `QUOTA_EXCEEDED`, `DAILY_TAGGING_EXCEEDED`, `CORPUS_BUSY`.
   */
  error?: string;
};

export type _bad_request_20 = {
  /**
   * Examples: `READ_ONLY`, `QUOTA_EXCEEDED`, `DAILY_TAGGING_EXCEEDED`, `NO_DATA`.
   */
  error?: string;
};

export type _forbidden_normal = {
  /**
   * Example: `Permission denied`. You do not have required permissions for specified corpus, document, template or other stuff. Permissions like (read, manage, edit, delete, superuser, etc.).
   */
  error?: string;
};

export type _forbidden = {
  /**
   * Result of succesfully finished request otherwise Null.
   */
  result?: boolean;
  /**
   * Example: `Permission denied`. You do not have required permissions for specified corpus, document, template or other stuff. Permissions like (read, manage, edit, delete, superuser, etc.).
   */
  error?: string;
};

export type _unauthorized = {
  /**
   * Exampple: `Unauthorized`. You need to authorize first, use API key from Sketch Engine.
   */
  error?: string;
};

export type _unauthorized_rpc = {
  /**
   * Result of succesfully finished request otherwise Null.
   */
  result?: boolean;
  /**
   * Example: `Unauthorized`. You need to authorize first, use API key from Sketch Engine.
   */
  error?: string;
};

export type _fileset = {
  /**
   * Fileset creation status: `0` if not started, `100` if finished succesfully, -1 if failed, otherwise in progress. Example: downloading content for corpus creation from the Internet.
   */
  progress?: number;
  /**
   * Duration of action with filesets (in seconds).
   */
  time_elapsed?: number;
  /**
   * Description of problem. If none Null.
   */
  error?: string;
  /**
   * Fileset ID.
   */
  id?: number;
  /**
   * Fileset name. `Given by user (except the main one with ID = 0).`
   */
  name?: string;
  /**
   * Total number of `words` (tokens minus punctuation etc.) in document.
   */
  word_count?: number;
  web_crawl?: {
    /**
     * `Source URL` from where the words were downloaded/extracted: website, documents...
     */
    input_type?: string;
    seed_words?: Array<string>;
    urls?: Array<string>;
    site?: Array<string>;
    /**
     * The amount of data `downloaded` by a web-crawler to create corpus.
     */
    data_downloaded?: number;
    /**
     * Counter of files found by web-crawler during crawling, `waiting` to be processed.
     */
    remaining_files_count?: number;
    /**
     * Counter of `already processed` files.
     */
    processed_files_count?: number;
    /**
     * Counter of files which `cannot` be processed because `invalid content type`, `size`, `duplication` etc..
     */
    unprocessed_files_count?: number;
    /**
     * Counter of files containing content like `navigation links`, `advertisement`, `headers`, `footers` etc..
     */
    invalid_content_types_count?: number;
    /**
     * Counter for files whose format cannot be converted to one of the supported formats.
     */
    unable_to_convert_count?: number;
    /**
     * Counter for files with repeating content.
     */
    duplicate_count?: number;
    /**
     * Duration of words gathering with web-crawler (in seconds).
     */
    time_elapsed?: number;
    /**
     * Average time to process single file (in seconds).
     */
    average_file_processing_time?: number;
  };
};

export type _filesets_get_progress = {
  result?: {
    /**
     * Fileset creation status: `0` if not started, `100` if finished succesfully, -1 if failed, otherwise in progress. Example: downloading content for corpus creation from the Internet.
     */
    progress?: number;
    /**
     * Duration of action with filesets (in second).
     */
    time_elapsed?: number;
    /**
     * Description of problem why it cannot be done.
     */
    error?: string;
    /**
     * Amount of words(tokens minus punctuation etc.) downloaded by web-crawler.
     */
    word_count?: number;
  };
  /**
   * Unexpected server error.
   */
  error?: string;
};

export type _fileset_creation = {
  /**
   * Fileset creation status: `0` if not started, `100` if finished succesfully, -1 if failed, otherwise in progress. Example: downloading content for corpus creation from the Internet.
   */
  progress?: number;
  /**
   * Duration of action with filesets (in seconds).
   */
  time_elapsed?: number;
  /**
   * Description of problem. If none Null.
   */
  error?: string;
  /**
   * Fileset ID.
   */
  id?: number;
  /**
   * Fileset name. `Given by user (except the main one with ID = 0).`
   */
  name?: string;
  /**
   * Total number of `words` (tokens minus punctuation etc.) in document.
   */
  word_count?: number;
  web_crawl?: {
    /**
     * `Source URL` from where the words were downloaded/extracted: website, documents...
     */
    input_type?: string;
    seed_words?: Array<string>;
    urls?: Array<string>;
    site?: Array<string>;
    /**
     * The amount of data `downloaded` by a web-crawler to create corpus.
     */
    data_downloaded?: number;
    /**
     * Counter of files found by web-crawler during crawling, `waiting` to be processed.
     */
    remaining_files_count?: number;
    /**
     * Counter of `already processed` files.
     */
    processed_files_count?: number;
    /**
     * Counter of files which `cannot` be processed because `invalid content type`, `size`, `duplication` etc..
     */
    unprocessed_files_count?: number;
    /**
     * Counter of files containing content like `navigation links`, `advertisement`, `headers`, `footers` etc..
     */
    invalid_content_types_count?: number;
    /**
     * Cannot return count.
     */
    unable_to_retrieve_count?: number;
    /**
     * Counter for sizes that are bigger or smaller as defined limits (max_file_size, min_file_size).
     */
    invalid_size_count?: number;
    /**
     * Counter for sizes that are bigger or smaller as defined limits (max_file_size, min_file_size).
     */
    invalid_cleaned_size_count?: number;
    /**
     * Amounth of filter usage.
     */
    keywords_filter_applied_count?: number;
    /**
     * Counter for files whose format cannot be converted to one of the supported formats.
     */
    unable_to_convert_count?: number;
    /**
     * Counter for files with repeating content.
     */
    duplicate_count?: number;
    /**
     * Duration of words gathering with web-crawler (in seconds).
     */
    time_elapsed?: number;
    /**
     * Average time to process single file (in seconds).
     */
    average_file_processing_time?: number;
  };
};

export type _language = {
  /**
   * Language iso-code. `ISO 639-1`.
   */
  id?: string;
  /**
   * Language name in `English`.
   */
  name?: string;
  /**
   * Language name in that language.
   */
  autonym?: string;
  /**
   * `Tagset ID.` Tagset is list of part-of-speech tags (POS tags) for specified language. Defaulty preselected to the most relevant one. For user corpora can be changed. The terms `tagset` and `templates` are interchangeable.
   */
  default_tagset_id?: string;
  /**
   * Default `reference` corpus.
   */
  reference_corpus?: string;
  /**
   * True if `term extraction` is supported.
   */
  has_term_grammar?: boolean;
  /**
   * Used script. Example: `Latin`, `Cyrillic`, etc.
   */
  script?: string;
};

export type _rpc_style = {
  /**
   * Represent whether request was finished successfully or not.
   */
  result?: boolean;
  /**
   * Unexpected server error. Example: `QUOTA_EXCEEDED`. If none Null.
   */
  error?: string;
};

export type _somefiles_post = {
  data?: {
    /**
     * An alphanumeric `somefile ID`.
     */
    id?: string;
    /**
     * Name of `multilingual file`.
     */
    name?: string;
    /**
     * File type of multilingual file: `.tmx`, .`.xlsx`, etc.
     */
    file_type?: string;
    /**
     * Unique numeric `owner ID` (usually you).
     */
    owner_id?: number;
    /**
     * Is document temporary or not.
     */
    temporary?: boolean;
    /**
     * Encoding standard of the document. Usually `UTF-8`.
     */
    encoding?: string;
    /**
     * An object of automatically guessed languages of inserted files during multilingual corpus creation. Maximum: `2`, because Sketch Engine support multilingual corpora only from 2 languages yet.
     */
    guessed_languages?: {
      language_1?: string;
      language_2?: string;
    };
  };
};

export type _template = {
  /**
   * Alphanumeric `template/tagset ID`. The terms `tagset` and `templates` are interchangeable.
   */
  id?: string;
  /**
   * Name of `template/tagset file`.
   */
  name?: string;
  /**
   * Unique numeric `owner ID` (usually you). If tagset/template is preloaded `Null`.
   */
  owner_id?: number;
  /**
   * Tagset/template `owner name` (usually you). If tagset/template is preloaded `Null`.
   */
  owner_name?: string;
  /**
   * Vertical creation is supported. False for legacy templates.
   */
  has_pipeline?: boolean;
  /**
   * Morphological tagging is supported.
   */
  has_tags?: boolean;
  /**
   * Lemmatization is supported.
   */
  has_lemmas?: boolean;
  /**
   * A list of attributes which can appear in corpus.
   */
  static_attributes?: Array<string>;
  /**
   * A list of used structures. Examples `<s>`, `<g>`.
   */
  structures?: Array<string>;
  /**
   * `URL` leading to template/tagset documentation.
   */
  tagsetdoc?: string;
  /**
   * Content of tagset.
   */
  content?: string;
  /**
   * Not ID, as you probably imagine, but filename of preselected sketchgrammar (`.m4` format).
   */
  default_sketchgrammar_id?: string;
  /**
   * Not ID, as you probably imagine, but filename of preselected sketchgrammar (`.m4` format).
   */
  default_termgrammar_id?: string;
  sharing?: {
    users?: Array<{
      /**
       * The ID of user you share template with.
       */
      id?: number;
      /**
       * The name of user you share template with.
       */
      name?: string;
      /**
       * The email of user you share template with.
       */
      email?: string;
    }>;
    user_group?: Array<{
      /**
       * The ID of group you share template with.
       */
      id?: number;
      /**
       * The name of group you share template with.
       */
      name?: string;
    }>;
  };
};

export type _template_put = {
  /**
   * Alphanumeric `template/tagset ID`. The terms `tagset` and `templates` are interchangeable.
   */
  id?: string;
  /**
   * Name of `template/tagset file`.
   */
  name?: string;
  /**
   * Unique numeric `owner ID` (usually you). If tagset/template is preloaded `Null`.
   */
  owner_id?: number;
  /**
   * Tagset/template `owner name` (usually you). If tagset/template is preloaded `Null`.
   */
  owner_name?: string;
  /**
   * Vertical creation is supported. False for legacy templates.
   */
  has_pipeline?: boolean;
  /**
   * Morphological tagging is supported.
   */
  has_tags?: boolean;
  /**
   * Lemmatization is supported.
   */
  has_lemmas?: boolean;
  /**
   * A list of attributes which can appear in corpus.
   */
  static_attributes?: Array<string>;
  /**
   * A list of used structures. Examples `<s>`, `<g>`.
   */
  structures?: Array<string>;
  /**
   * `URL` leading to template/tagset documentation.
   */
  tagsetdoc?: string;
  /**
   * Content of tagset.
   */
  content?: string;
  /**
   * Not ID, as you probably imagine, but filename of preselected sketchgrammar (`.m4` format).
   */
  default_sketchgrammar_id?: string;
  /**
   * Not ID, as you probably imagine, but filename of preselected sketchgrammar (`.m4` format).
   */
  default_termgrammar_id?: string;
  sharing?: {
    users?: Array<{
      /**
       * The ID of user you share template with.
       */
      id?: number;
    }>;
    user_group?: Array<{
      /**
       * The ID of group you share template with.
       */
      id?: number;
    }>;
  };
};

export type _get_used_space = {
  result?: {
    /**
     * The current maximal amount of words in the user's corpora.
     */
    space_used?: number;
    /**
     * Default maximal amount of words. The default is set to `1 000 000` words. It can be changed.
     */
    space_total?: number;
  };
  /**
   * Unexpected server error. If none Null.
   */
  error?: string;
};

/**
 * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
 */
export type Parameter_corpname = string;

/**
 * The name of the `subcorpus`. The `default` value `empty string` refers to the entire corpus. An example for `preloaded/bnc2_tt21` can be `Written Academic` or `1960-1974`.
 */
export type Parameter_usesubcorp = string;

/**
 * A list of grammatical relations from the correspoding `sketch grammar`.
 */
export type Parameter_gramrels = 0 | 1;

/**
 * Results of the last corpcheck (if available in the compilation log).
 */
export type Parameter_corpcheck = 0 | 1;

/**
 * The content of the registry file (registry_dump and registry_text).
 */
export type Parameter_registry = 0 | 1;

/**
 * A parameter to obtain the list of subcorpora and their sizes (e.g. number of tokens, words).
 */
export type Parameter_subcorpora = 0 | 1;

/**
 * The lexicon sizes of structure attributes.
 */
export type Parameter_struct_attr_stats = 0 | 1;

/**
 * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
 */
export type Parameter_format = "json" | "xml" | "csv" | "tsv" | "txt" | "xls";

/**
 * Sets the corpus attribute you want to work with. Some corpora may have specific additional attributes.
 *
 * Basic examples:
 * - word
 * - lc
 * - lemma
 * - lemma_lc
 * - lempos
 * - lempos_lc
 * - tag
 * - pos
 *
 * For list of available word list attributes see **ca/api/corpora/{corpusId}**
 */
export type Parameter_wlattr = string;

/**
 * The type of frequency. The values stand for: `frq` -> absolute or raw frequency, `docf` -> document frequency. `arf` -> average reduced frequency.
 */
export type Parameter_wlnums = "frq" | "docf" | "arf";

/**
 * Sets the minimum frequency limit. Items with a lower frequency will not be included.
 */
export type Parameter_wlminfreq = number;

/**
 * Sets the number of items to be returned in the API response. It is not limited for user corpora, in preloaded corpora can be some limitation. This parameter is often used with wlpage to help with pagination in frontend development.
 */
export type Parameter_wlmaxitems = number;

/**
 * Sets a regex to filter the results. Relevant only in a simple wordlist.
 */
export type Parameter_wlpat = string;

/**
 * Sets the sorting of the results. The default is `frq`, i.e. by absolute frequency. Docf means document frequency.
 */
export type Parameter_wlsort = "frq" | "docf";

/**
 * Defines the allow list (formerly known as whitelist), the list of words which should be included in the list. See also `wlblacklist`.
 */
export type Parameter_wlfile = string;

/**
 * A deny list (formerly known as blacklist) is a list of items that should be excluded from the result. The values should be be separated by a newline symbol (without commas between values). In the URL, the newline symbol is `
`.
 */
export type Parameter_wlblacklist = string;

/**
 * Selects a structure attribute (reffered to as text type in the web interface). Corpora have different numbers structure attributes and their values. You can find them in the response of the `corpus_info` method in `freqttattrs` or `subcorpattrs` keys. Not all of them have attributes to show.
 */
export type Parameter_avattr = string;

/**
 * A regex to filter the results. Empty string defaults to `.*` (match everything).
 */
export type Parameter_avpat = string;

/**
 * The number of items to return.
 */
export type Parameter_avmaxitems = number;

/**
 * The starting index from which to return the results.
 */
export type Parameter_avfrom = number;

/**
 * The part of speech of the lemma. The concrete values depend on the corpus. If the corpus contains the `lempos` attribute and `lpos` is empty, the result defaults to the most frequent part of speech of the lemma.
 */
export type Parameter_lpos = "-n" | "-v" | "-j" | "-a" | "-d" | "-i";

/**
 * Switches the asynchronous processing on/off. ON = partial results are returned as soon as the first page is filled with results. OFF = results are returned only after the search is completed. Normally, ON is used in the web interface and OFF when using the API.
 */
export type Parameter_asyn = 0 | 1;

/**
 * An optinal way of **wraping parameters**. It is possible to send all parametres via this parameter only.
 *
 * The most frequent uses are:
 *
 * `queryselector`: To select the query type. Supported options are: **cqlrow**, **iqueryrow**, **lemmarow**, **charrow**, **phraserow**, **wordrow**. For more information see concordance_query parameters.
 *
 * `iquery`: Use with `iqueryrow`.
 *
 * `cql`: Use with `cqlrow`.
 *
 * `lemma`: Use with `lemmarow`.
 *
 * `lpos`: The part of speech of the lemma.
 *
 * `qmcase`: Sets the attribute to its lowercased version, i.e. the data are extracted from a lowercased version of the corpus. It used for case insensitive analysis. 1 = case sensitive), 0 = lowercased corpus/case insensitive).
 *
 *
 * `char`: Use with charrow.
 *
 * `phrase`: Use with phraserow.
 *
 * `word`: Use with wordrow
 *
 *
 * `name`:
 *
 * `pnfilter`:
 *
 * `inclkwic`:
 *
 * `filfpos`:
 *
 * `filtpos`:
 *
 * `desc`:
 *
 * `q`:
 *
 *
 */
export type Parameter_json = {
  [key: string]: unknown;
};

/**
 * The CQL query. Regexes are supported for `lemma`, `phrase` and `word` types. The `iquery` supports simplified wildcards (see concordance_query[iquery]). If you decide to use the concordance_query in a json parameter, you do not have to use this parameter.
 */
export type Parameter_q = string;

/**
 * The number of items in one response.
 */
export type Parameter_fmaxitems = number;

/**
 * The identifier of the sorted column. Use `frq` (default) to sort by frequency.
 */
export type Parameter_freq_sort = "frq" | "rel";

/**
 * The (positional attribute)[https://www.sketchengine.eu/my_keywords/positional-attribute/] (lemma, word form etc.) used in the computation.
 */
export type Parameter_cattr = string;

/**
 * The left boundary of the window in which the collocations should be identified. Defined by the token position left or right of KWIC.
 */
export type Parameter_cfromw = number;

/**
 * The right boundary of the window in which the collocations should be identified. Defined by the token position left or right of KWIC.
 */
export type Parameter_ctow = number;

/**
 * The minimum frequency of the token in the corpus.
 */
export type Parameter_cminfreq = number;

/**
 * The minimum frequency of the token in the window defined by `cfromw` and `ctow`.
 */
export type Parameter_cminbgr = number;

/**
 * Sets the maximum number of items in the response.
 */
export type Parameter_cmaxitems = number;

/**
 * Defines the types of statistics (association measures) to be computed.
 *
 * `t` -> T-score
 *
 * `m` -> MI
 *
 * `3` -> MI3
 *
 * `l` -> log likelihood
 *
 * `s` -> min. sensitivity
 *
 * `p` -> MI.log_f
 *
 * `r` -> relative freq.
 *
 * `f` -> absolute freq.
 *
 * `d` -> logDice.
 *
 * To send one value just type the value for example `t`. If you need to send more values write it as `["t","m","d","3","l","s","p"]`.
 */
export type Parameter_cbgrfns = string;

/**
 * Function according to which the result is sorted.
 */
export type Parameter_csortfn =
  | "t"
  | "m"
  | "3"
  | "l"
  | "s"
  | "p"
  | "r"
  | "f"
  | "d";

/**
 * Name of the subcorpus.
 */
export type Parameter_subcname = string;

/**
 * Set to `1` if the corpus should be deleted. Only user subcorpora can be deleted. Nothing will be deleted if left empty (default value == 0).
 */
export type Parameter_delete = 0 | 1;

/**
 * Corpus name of the reference corpus, it must have the same processing (the same attributes, the same term grammar).
 */
export type Parameter_ref_corpname = string;

/**
 * The smoothing parameter for (simple maths) [https://www.sketchengine.eu/documentation/simple-maths/].
 */
export type Parameter_simple_n = "1" | "0";

/**
 * A list of comma-delimited attributes that are returned together with each token. Other examples are:`word, lc, lemma, tag` etc..
 */
export type Parameter_attrs = string;

/**
 * Limits the results to items containing only alphanumeric characters.
 */
export type Parameter_alnum = 1 | 0;

/**
 * Limits the results to items containing at least one alphanumberic character. Words such as 16-year-old or 3D will be included.
 */
export type Parameter_onealpha = 1 | 0;

export type Parameter_max_keywords = number;

/**
 * The minimum n-gram length. Usually used with `ngrams_max_n` and `usengrams`.
 */
export type Parameter_ngrams_n = 2 | 3 | 4 | 5;

/**
 * Sets the case sensitivity of the corpus, i.e. the data are extracted from a lowercased version of the corpus. It is used for case insensitive analysis. Parameter "1" means case sensitivity, "0" means case insensitivity.
 */
export type Parameter_wlicase = 1 | 0;

/**
 * Sets the maximum frequency limit in the wordlist.
 */
export type Parameter_wlmaxfreq = number;

/**
 * Includes, or excludes, nonwords in the in the result. Nonwords are tokens which do not start with letter of the alphabet (e.g. numbers, punctuation). The regex to match the nonwords is `[^[:alpha:]].*`. Certain specialized corpora may use their own specific definition of nonwords.
 */
export type Parameter_include_nonwords = 1 | 0;

/**
 * Parameter that represents if the wordlist is created from the first 10 milions lines of corpus. One if yes, no if he wordlist is created from the whole corpus.
 * @deprecated
 */
export type Parameter_random = 1 | 0;

/**
 * The attribute applied to tokens in the query which do not have an attribute specified explicitly as part of the query.
 */
export type Parameter_default_attr = string;

/**
 * The text type (metadata) for which statistics should be calculated from the concordance. The default is `bncdoc.alltyp` (all available text types are included). Text types (attributes and there values) differ between corpora). You can find them in the response of `corpus_info` method in `freqttattrs` or `subcorpattrs` keys. Not all of them have attributes to show.
 */
export type Parameter_refs = string;

/**
 * Determines which tokens will be returned with additional attributes defined in `attrs`. `kw` will add the attributes to the KWIC only. `all` will return them with all tokens.
 */
export type Parameter_attr_allpos = "kw" | "all";

/**
 * Switches between sentence view and the KWIC view. `sen` returns complete sentences without trimming them. `kwic` returns the KWIC view with the query in the centre and some context left and right.
 */
export type Parameter_viewmode = "sen" | "kwic";

/**
 * Only used with error-annotated corpora. It determines what should be highlighted. It is set to 'q' for corpora without error annotation. Meaning of individual options:
 *
 * - `q` -> to higlight query result.
 *
 * - `e` -> to higlight errors.
 *
 * - `c` -> to highlight corrections.
 *
 * - `b` -> to highlight both erros and corrections.
 *
 * Example of such a corpus can be `preloaded/enwiki_error_sample_sentences`.
 */
export type Parameter_cup_hl = "q" | "e" | "c" | "b";

/**
 * A list of comma-delimited structures (=structure tags) that should be included in the result.
 */
export type Parameter_structs = string;

/**
 * The number of the page that should be returned.
 */
export type Parameter_fromp = number;

/**
 * The number of lines in the concordance.
 */
export type Parameter_pagesize = number;

/**
 * The size of the left context in KWIC view. Number of tokens.
 */
export type Parameter_kwicleftctx = string;

/**
 * The size of the right context in KWIC view. Number of tokens.
 */
export type Parameter_kwicrightctx = string;

/**
 * The maximum n-gram length. The maximum is `6`.
 */
export type Parameter_ngrams_max_n = 2 | 3 | 4 | 5 | 6;

/**
 * Represent if n-grams should be extracted or just simple keywords.
 */
export type Parameter_usengrams = 0 | 1;

/**
 * Parameter to set the format of ouput. Is it always set to `simple`, for the `struct_wordlist` is another enpoint called `struct_wordlist`.
 * @deprecated
 */
export type Parameter_wltype = "simple" | "struct_wordlist";

/**
 * Sets the attributes used for generating the wordlist. Up to 3 attributes are allowed (see wlstruct_attr2 and wlstruct_attr3). Some corpora may contain additional specific attributes.
 */
export type Parameter_wlstruct_attr1 = "word" | "lemma" | "tag" | "lempos";

/**
 * Includes the relative frequency of each item in the result.
 */
export type Parameter_relfreq = 1 | 0;

/**
 * Calculate the document frequency for each item in the result. Must be used with `addfreqs` set to `docf`.
 */
export type Parameter_reldocf = 1 | 0;

/**
 * To select page of the response. The number of items on the page is specified by parameter wlmaxitems.
 */
export type Parameter_wlpage = number;

/**
 * The number of the response batch (page). The number of items in each batch is specified by `fmaxitems`.
 */
export type Parameter_fpage = number;

/**
 * If there are more attributes (e.g. m1attr, m2attr), the results can be grouped by the first column/attribute.
 */
export type Parameter_group = 1 | 0;

/**
 * Includes the percentage of the concordance in the result.
 */
export type Parameter_showpoc = 1 | 0;

/**
 * Includes relative in text types value in the result.
 */
export type Parameter_showreltt = 1 | 0;

/**
 * Includes the relative frequency in the result.
 */
export type Parameter_showrel = 1 | 0;

/**
 * The number of attributes for which the frequencies should be counted.
 */
export type Parameter_freqlevel = 1 | 2 | 3 | 4 | 5 | 6;

/**
 * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/) or structure attributes (metadata/text types) of any token in the concordance.\n\n A maximum of 6 attributes is allowed (e.g. ml2attr, ml3attr). At least one attribute is required.
 */
export type Parameter_ml1attr = string;

/**
 *  Position of the selected attribute in the concordance. **Minus** means **left** context (-1<0). **Plus** means **right** context (6>0). **At least one attribute is required, the others are optional.** Every attribute (ml1attr, ml2attr. etc.) needs it's **own** context position (e.g. if 3 attributes are selected three context position **needs to** be set ml1ctx, ml2ctx, ml3ctx).
 *
 *
 * **Positions can be referenced as follows:**
 *
 * `integer number` - where **0** is the first token in **KWIC**, **-1** the rightmost token in the left context etc.
 *
 * `1:x` - where **x** is one of the corpus structures (e.g. “doc” or “s” if the corpus has the particular markup). Its meaning is the first token in the structure, except when it is the right boundary of a range - then it is the last token in the structure. Also, other numbers can be used, e.g. -2:x, 3:x, etc. (-1 is the same as 1 with meaning “structure containing KWIC”)
 *
 * `a<0` - where **a** stands for a position reference as described in the first two points with meaning '**a** positions before/after the firs KWIC position' (so this is equivalent to **a**)
 *
 * `a>0` - where **a** stands for the same position reference with meaning 'positions before/after the last KWIC position'
 *
 * in the previous two points, if **0** is substituted with a natural number **k**, it means 'before/after **k**-th collocation' instead of 'before/after KWIC'. Collocations are special token groups in the context, that can be added using positive filters (see below).
 *
 *
 * `Ranges` can be referenced as a~b where **a**, **b** stand for token identifiers as above. Examples of positions and ranges:
 *
 * `-1<0` - rightmost token in the left context
 *
 * `3>0` - third token in right context
 *
 * `0>0` - last token in KWIC
 *
 * `0<0` - first token in KWIC
 *
 * `0<0~0>0` - range of KWIC
 *
 * `-1<0~1>0` - range of KWIC with one token from the left context and one from the right context
 *
 * `1:s` - first token in the sentence containing KWIC (or its first token)
 *
 * `1:s>0` - first token in the sentence containing KWIC (or its last token)
 *
 * `0<1` - first token of the first-added collocation.
 *
 *
 * `Examples:`
 *
 * sword/ **1>0~3>0**
 *
 * sword/ **1>0~3>0**
 *
 * slemma/ **0<0~0>0**
 *
 * sword/i **-1**
 *
 * sword/ **0** word/ir **-1<0** tag/r **-2<0**
 *
 *
 */
export type Parameter_ml1ctx = string;

/**
 * Additional optional attribute that should be included in the result.
 */
export type Parameter_wlstruct_attr2 = "word" | "lemma" | "tag" | "lempos";

/**
 * Additional optional attribute that should be included in the result.
 */
export type Parameter_wlstruct_attr3 = "word" | "lemma" | "tag" | "lempos";

/**
 * Switches to the `lc` attribute, i.e. the lower-cased version of the corpus to allow case insensitive searching. `1` means that case sensitivity is off, `0` means it is on.
 */
export type Parameter_icase = 1 | 0;

/**
 * The position of the first token of KWIC in the corpus.
 */
export type Parameter_pos = number;

/**
 * N-grams which are sub-ngrams of a longer n-gram will be grouped together with the longer n-gram. Nesting only works when a `ngrams_n` and `ngrams_max_n` are different values.
 */
export type Parameter_nest_ngrams = 1 | 0;

/**
 * Switches between the computation of keywords, terms, key n-grams and key collocations. With keywords and n-grams, it also sets the attribute to be used for the computation.
 *
 * For keywords, set to the required attribute, usually `lc`, `word` or `lemma`.
 *
 * For n-grams, set to the required attribute and set `usengrams`, `ngrams_n` and `ngrams_max_n`.
 *
 * For terms, set the attribute to `TERM`.
 *
 * For collocations (word sketch triples, equivalent of using the Word Sketch with AS A LIST option in the web interace), set the attribute to `WSCOLLOC`. Consider using `wlpat`.
 */
export type Parameter_attr = "lemma" | "word" | "TERM" | "WSCOLLOC";

export type Parameter_res = number;

export type Parameter_normalize = number;

export type Parameter_fc_lemword_window_type = string;

export type Parameter_fc_lemword_wsize = number;

export type Parameter_fc_lemword_type = string;

export type Parameter_fc_pos_wsize = number;

export type Parameter_fc_pos_type = string;

export type Parameter_fc_pos_window_type = number;

/**
 * The query type. You can send it directly or via the `json` parameter, the results are the same.
 */
export type Parameter_concordance_query_queryselector =
  | "iquery"
  | "cqlrow"
  | "lemmarow"
  | "charrow"
  | "wordrow"
  | "phraserow";

/**
 * Only works when `queryselector` is set to `iqueryrow`. Type a word or phrase.
 *
 * These special wildcards are supported .
 *
 * Use the `asterisk (*)` for any number of unspecified characters. Use a `question mark (?)` for exactly one unspecified character. Use the `pipe (|)` to include more than one word or phrase. Use `two hyphens (--)` to find a word which is  hyphenated, non-hyphenated or spelt as two separate words.
 */
export type Parameter_concordance_query_iquery = string;

/**
 * Only works when `queryselector` is set to `cqlrow`. Type the query using the [cql](https://www.sketchengine.eu/documentation/corpus-querying/) query language.
 */
export type Parameter_concordance_query_cql = string;

/**
 * Only works when `queryselector` is set to `lemmarow`. Type the lemma. Regex is supported.
 */
export type Parameter_concordance_query_lemma = string;

/**
 * Only works when `queryselector` is set to `charrow`. Type the characters that the tokens should contain. Regex is supported.
 */
export type Parameter_concordance_query_char = string;

/**
 * Only works when `queryselector` is set to `wordrow`. Type the word form. Regex is supported.
 */
export type Parameter_concordance_query_word = string;

/**
 * Only works when `queryselector` is set to `phraserow`. Type the phrase. Regex is supported.
 */
export type Parameter_concordance_query_phrase = string;

/**
 * (Only for error-annotated corpora.) Determines what should be highlighted. Corr means **correction** and err means **error**. An **example** of such a corpus is `preloaded/enwiki_error_sample_sentences`.
 */
export type Parameter_errcorr_switch = "corr" | "err";

/**
 * (Only for error-annotated corpora). Determines which error type to higlight. An example of such a corpus is `preloaded/enwiki_error_sample_sentences`.
 */
export type Parameter_cup_err_code =
  | ".*"
  | "lexicosemantic"
  | "punct"
  | "spelling"
  | "style"
  | "typographical"
  | "unclassified";

/**
 * (Only for error-annotated corpora.) An error token to search.
 */
export type Parameter_cup_err = string;

/**
 * (Only for error-annotated corpora.) A correction token to search.
 */
export type Parameter_cup_corr = string;

/**
 * Only used by the web interface. Indicates the number of tokens that should be highlighted in red.
 */
export type Parameter_hitlen = number;

/**
 * Size of the left context in tokens.
 */
export type Parameter_detail_left_ctx = number;

/**
 * Size of the right context in tokens.
 */
export type Parameter_detail_right_ctx = number;

/**
 * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/). Just like `ml1attr` but optional.
 */
export type Parameter_ml2attr = string;

/**
 * Position of the selected attribute in the concordance. Minus means left context (-1<0). Plus means right context (6>0). Just like ml1ctx but optional.
 */
export type Parameter_ml2ctx = string;

/**
 * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/). Just like `ml1attr` but optional.
 */
export type Parameter_ml3attr = string;

/**
 * Position of the selected attribute in the concordance. Minus means left context (-1<0). Plus means right context (6>0). Just like ml1ctx but optional.
 */
export type Parameter_ml3ctx = string;

/**
 * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/). Just like `ml1attr` but optional.
 */
export type Parameter_ml4attr = string;

/**
 * Position of the selected attribute in the concordance. Minus means left context (-1<0). Plus means right context (6>0). Just like ml1ctx but optional.
 */
export type Parameter_ml4ctx = string;

/**
 * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/). Just like `ml1attr` but optional.
 */
export type Parameter_ml5attr = string;

/**
 * Position of the selected attribute in the concordance. Minus means left context (-1<0). Plus means right context (6>0). Just like ml1ctx but optional.
 */
export type Parameter_ml5ctx = string;

/**
 * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/). Just like `ml1attr` but optional.
 */
export type Parameter_ml6attr = string;

/**
 * Position of the selected attribute in the concordance. Minus means left context (-1<0). Plus means right context (6>0). Just like ml1ctx but optional.
 */
export type Parameter_ml6ctx = string;

/**
 * Sets the minimum frequency of the item.
 */
export type Parameter_minfreq_extract_keywords = string;

/**
 * Sets the maximum frequency of the item.
 */
export type Parameter_maxfreq_extract_keywords = number;

/**
 * Represent what kind of frequnecy should be calculated. When used with `reldocf` it is set to `docf` to calculate document frequency.
 */
export type Parameter_addfreqs = string;

/**
 * An optinal way of **wraping parameters**. It is possible to send all relevant parametres via this parameter only. It is classic JSON format.
 *
 * The most frequent uses are:
 *
 * `queryselector`: To select the query type. Supported options are: **cqlrow**, **iqueryrow**, **lemmarow**, **charrow**, **phraserow**, **wordrow**.
 *
 * `iquery`: Use with `iqueryrow`.
 *
 * `cql`: Use with `cqlrow`.
 *
 * `lemma`: Use with `lemmarow`.
 *
 * `lpos`: The part of speech of the lemma.
 *
 * `qmcase`: Sets the attribute to its lowercased version, i.e. the data are extracted from a lowercased version of the corpus. It used for case insensitive analysis. 1 = case sensitive), 0 = lowercased corpus/case insensitive).
 *
 * `char`: Use with charrow.
 *
 * `phrase`: Use with phraserow.
 *
 * `word`: Use with wordrow
 *
 * `cbgrfns`: If you need to send more cbgrfnsDefines the types of statistics (association measures) to be computed. See example.
 */
export type Parameter_json_collx = {
  [key: string]: unknown;
};

/**
 * Set to `1` if new subcorpus should be created. Subcorpus will not be created if left empty (default value == 0).
 */
export type Parameter_create = 0 | 1;

/**
 * Query for creating subcorpora from concordance.
 *
 * The search criteria are specified within brackets following a prefix like `alemma` or `aword`. This prefix often indicates the type of linguistic search (e.g., lemma-based, word-based). The criteria within the brackets can include checks for specific words, lemmas, parts of speech and more, using operators like | (OR), & (AND), and regular expressions.
 *
 * The lists of available attributes, pos tags for specific corpus can be obtained via `/search/corp_info`.
 *
 *
 * `Examples:`
 *
 *
 * - Simple word or lemma search in the BNC corpus:
 *
 * **q=alemma,[lc="test" | lemma_lc="test"]**
 *
 *
 * - Search for nouns with the lemma 'test' in a case-sensitive manner:
 *
 * **q=alemma,[lempos_lc="(test)-n"]**
 *
 *
 * - Search for verbs with the lemma 'test', case-insensitive:
 *
 * **q=alemma,[lempos="(test)-v"]**
 *
 *
 * - Searching for a specific phrase 'test' in a case-sensitive manner:
 *
 * **q=aword,[word="test"]**
 *
 *
 * - Searching for the numeral '1955':
 *
 * **q=alemma,[word="1955" & tag="CD"]**
 *
 *
 * - Regex-based search for words containing the character 'h':
 *
 * **q=alemma,[word=".\*h.\*"]**
 *
 *
 * - Complex search involving the lemma 'book' followed by up to three words, then a verb:
 *
 * **q=alemma,[lemma="book"][]{1,3}[tag="V.\*"]**
 */
export type Parameter_q_subcorp = string;

/**
 * Which corpus structure should be used in new subcorpus. Used withim concordance type of subcorpus. The lists of structures can be obtained via `/search/corp_info`.
 */
export type Parameter_struct = string;

/**
 * The name of subcorpus you want to rename.
 */
export type Parameter_subcorp_id = string;

/**
 * A new name for the subcorpus.
 */
export type Parameter_new_subcorp_name = string;

/**
 * Used to specify text types for a subcorpus. Takes a JSON object as input, where the key-value pairs define the specific attributes. The attributes can vary depending on the corpus.
 *
 * When using the json parameter in a query, you can define a JSON object with one or more attributes. Each attribute can have a single value or an array of values.
 *
 *
 * The list of available text types for specific corpus can be obtained via `/search/corp_info` just add `sca_` in front the name (see examples).
 *
 *
 * `Examples:`
 *
 *
 * - To create a subcorpus based on a specific spoken text type from the BNC corpus:
 *
 * **{"sca_bncdoc.alltyp":["Spoken context-governed"]}**
 *
 *
 * - To filter texts from the BNC corpus that are both spoken context-governed and spoken demographic:
 *
 * **{"sca_bncdoc.alltyp":["Spoken context-governed","Spoken demographic"]}**
 *
 *
 * - To select texts from the BNC corpus from a specific time period (1960-1974):
 *
 * **{"sca_bncdoc.alltim":["1960-1974"]}**
 *
 *
 * - To create a subcorpus with texts from specific authors and time periods, along with regional specifications:
 *
 * **{"sca_bncdoc.author": ["Author1","Author2",...],"sca_bncdoc.alltim": ["1985-1993","1975-1984"], "sca_bncdoc.wripp": ["UK (unspecific)","Ireland"]}**
 *
 *
 * - To filter texts from the Ententen corpus based on domain and topic:
 *
 * **{"sca_doc.tld":["org","com"], "sca_doc.topic": ["arts","beauty & fashion","cars & bikes","culture & entertainment"]}**
 *
 *
 * - For a user-specific corpus, filtering based on document ID and filename:
 *
 * **{"sca_doc.id":["file29173711"],"sca_doc.filename":["Filename.pdf"]}**
 */
export type Parameter_json_subcorp = {
  [key: string]: unknown;
};

/**
 *  **Minus** means **left** context (-1<0). **Plus** means **right** context (6>0).
 *
 *
 * **Positions can be referenced as follows:**
 *
 * `integer number` - where **0** is the first token, **-1** the rightmost token in the left context etc.
 *
 * `1:x` - where **x** is one of the corpus structures (e.g. “doc” or “s” if the corpus has the particular markup). Its meaning is the first token in the structure, except when it is the right boundary of a range - then it is the last token in the structure. Also, other numbers can be used, e.g. -2:x, 3:x, etc. (-1 is the same as 1 with meaning “structure containing searched word”)
 *
 * `a<0` - where **a** stands for a position reference as described in the first two points with meaning '**a** positions before/after the first searched word position' (so this is equivalent to **a**)
 *
 * `a>0` - where **a** stands for the same position reference with meaning 'positions before/after the last searched word position'
 *
 * in the previous two points, if **0** is substituted with a natural number **k**, it means 'before/after **k**-th collocation' instead of 'before/after KWIC'. Collocations are special token groups in the context, that can be added using positive filters (see below).
 *
 *
 * `Ranges` can be referenced as a~b where **a**, **b** stand for token identifiers as above. Examples of positions and ranges:
 *
 * `-1<0` - rightmost token in the left context
 *
 * `3>0` - third token in right context
 *
 * `0>0` - last token
 *
 * `0<0` - first token
 *
 * `0<0~0>0` - range
 *
 * `-1<0~1>0` - range with one token from the left context and one from the right context
 *
 * `1:s` - first token in the sentence containing searched word (or its first token)
 *
 * `1:s>0` - first token in the sentence containing searched word (or its last token)
 *
 * `0<1` - first token of the first-added collocation.
 *
 *
 * `Examples:`
 *
 * sword/ **1>0~3>0**
 *
 * sword/ **1>0~3>0**
 *
 * slemma/ **0<0~0>0**
 *
 * sword/i **-1**
 *
 * sword/ **0**
 *
 * word/ir **-1<0**
 *
 * tag/r **-2<0**
 *
 *
 */
export type Parameter_ctx = string;

/**
 * A diachronic attribute to be selected. Available attributes **can differ** in corpora. Examples can be **doc.year** or **doc.month**.
 */
export type Parameter_diaattr = string;

/**
 * `1`: display results during calculation
 * `0`: display results after all data has been calculated (can take quite a lot of time).
 */
export type Parameter_sse = "1" | "0";

/**
 * Determines which periods are included in the results. It signifies the percentage above the average size, acting as a **limit**. When relative frequency (rel_frq) surpasses this limit, it is discarded (moved to **removed_freqdist** object).
 */
export type Parameter_threshold = string;

/**
 * An optinal way of **wraping parameters**. It is possible to send all relevant parametres via this parameter only. It is classic JSON format.
 *
 * `wordlist`: words for which the relative frequency should be counted.
 *
 *
 */
export type Parameter_json_freqdist = {
  [key: string]: unknown;
};

/**
 * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
 */
export type Parameter_corpname_freqdist = string;

/**
 * A wordlist of words for which the relative frequency should be counted. No exact example is here because it is already set in `JSON` parameter.\Example: ['the','a','lion'].
 */
export type Parameter_wordlist = string;

/**
 * Numeric corpus ID. For corpora querying.
 */
export type Parameter_corpus_id = number;

/**
 * Document ID. For document querying.
 */
export type Parameter_document_id = number;

/**
 * Numerical template ID, but preloaded templates do not have ID but you can query them by their name. Example: `UNIVERSAL_3`.
 */
export type Parameter_template_id = string;

/**
 * Name of log file. Name 'last.log' show the newest log for that corpus.
 */
export type Parameter_logname = string;

/**
 * ID of file subdirectories. If sets to 0 it will return top-level folder of documents, so if you have a web corpora with folders web1 and web2 it will return web1.
 */
export type Parameter_fileset_id = number;

/**
 * ID of file subdirectory. **0** stands for default document directory with name `upload`.
 */
export type Parameter_fileset_id_query = number;

/**
 * Alphanumeric multilanguage file ID
 */
export type Parameter_somefile_id = string;

/**
 * File format in which the corpus should be downloaded. Just three formats are supported.
 */
// export type Parameter_format = string;

/**
 * The contents of each file will be enclosed in a XML like structure of the specified name with the filename as its id attribute and the URL (if available) as the url attribute. If empty document boundaries will be lost. Example: `doc`.
 */
export type Parameter_file_structure = string;

/**
 * Required when you want to download parallel corpora, **when format == tmx.** Specify aligned corpus name.
 */
export type Parameter_aligned = string;

/**
 * Delay tagging by given number of `seconds`.
 */
export type Parameter_wait_with_tagging = number;

/**
 * Start corpus compiling after web-crawler finishes downloading content from the internet.
 */
export type Parameter_compile_when_finished = number;

export type GetCorpInfoData = {
  /**
   * Results of the last corpcheck (if available in the compilation log).
   */
  corpcheck?: 0 | 1;
  /**
   * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
   */
  corpname: string;
  /**
   * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
   */
  format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
  /**
   * A list of grammatical relations from the correspoding `sketch grammar`.
   */
  gramrels?: 0 | 1;
  /**
   * The content of the registry file (registry_dump and registry_text).
   */
  registry?: 0 | 1;
  /**
   * The lexicon sizes of structure attributes.
   */
  structAttrStats?: 0 | 1;
  /**
   * A parameter to obtain the list of subcorpora and their sizes (e.g. number of tokens, words).
   */
  subcorpora?: 0 | 1;
  /**
   * The name of the `subcorpus`. The `default` value `empty string` refers to the entire corpus. An example for `preloaded/bnc2_tt21` can be `Written Academic` or `1960-1974`.
   */
  usesubcorp?: string;
};

export type GetCorpInfoResponse = _corp_info;

export type GetWordListData = {
  /**
   * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
   */
  corpname: string;
  /**
   * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
   */
  format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
  /**
   * Includes, or excludes, nonwords in the in the result. Nonwords are tokens which do not start with letter of the alphabet (e.g. numbers, punctuation). The regex to match the nonwords is `[^[:alpha:]].*`. Certain specialized corpora may use their own specific definition of nonwords.
   */
  includeNonwords?: 1 | 0;
  /**
   * N-grams which are sub-ngrams of a longer n-gram will be grouped together with the longer n-gram. Nesting only works when a `ngrams_n` and `ngrams_max_n` are different values.
   */
  nestNgrams?: 1 | 0;
  /**
   * The maximum n-gram length. The maximum is `6`.
   */
  ngramsMaxN?: 2 | 3 | 4 | 5 | 6;
  /**
   * The minimum n-gram length. Usually used with `ngrams_max_n` and `usengrams`.
   */
  ngramsN?: 2 | 3 | 4 | 5;
  /**
   * Parameter that represents if the wordlist is created from the first 10 milions lines of corpus. One if yes, no if he wordlist is created from the whole corpus.
   * @deprecated
   */
  random?: 1 | 0;
  /**
   * Calculate the document frequency for each item in the result. Must be used with `addfreqs` set to `docf`.
   */
  reldocf?: 1 | 0;
  /**
   * Includes the relative frequency of each item in the result.
   */
  relfreq?: 1 | 0;
  /**
   * The smoothing parameter for (simple maths) [https://www.sketchengine.eu/documentation/simple-maths/].
   */
  simpleN?: "1" | "0";
  /**
   * Represent if n-grams should be extracted or just simple keywords.
   */
  usengrams?: 0 | 1;
  /**
   * The name of the `subcorpus`. The `default` value `empty string` refers to the entire corpus. An example for `preloaded/bnc2_tt21` can be `Written Academic` or `1960-1974`.
   */
  usesubcorp?: string;
  /**
   * Sets the corpus attribute you want to work with. Some corpora may have specific additional attributes.
   *
   * Basic examples:
   * - word
   * - lc
   * - lemma
   * - lemma_lc
   * - lempos
   * - lempos_lc
   * - tag
   * - pos
   *
   * For list of available word list attributes see **ca/api/corpora/{corpusId}**
   */
  wlattr: string;
  /**
     * A deny list (formerly known as blacklist) is a list of items that should be excluded from the result. The values should be be separated by a newline symbol (without commas between values). In the URL, the newline symbol is `
    `.
     */
  wlblacklist?: string;
  /**
   * Defines the allow list (formerly known as whitelist), the list of words which should be included in the list. See also `wlblacklist`.
   */
  wlfile?: string;
  /**
   * Sets the case sensitivity of the corpus, i.e. the data are extracted from a lowercased version of the corpus. It is used for case insensitive analysis. Parameter "1" means case sensitivity, "0" means case insensitivity.
   */
  wlicase?: 1 | 0;
  /**
   * Sets the maximum frequency limit in the wordlist.
   */
  wlmaxfreq?: number;
  /**
   * Sets the number of items to be returned in the API response. It is not limited for user corpora, in preloaded corpora can be some limitation. This parameter is often used with wlpage to help with pagination in frontend development.
   */
  wlmaxitems?: number;
  /**
   * Sets the minimum frequency limit. Items with a lower frequency will not be included.
   */
  wlminfreq?: number;
  /**
   * The type of frequency. The values stand for: `frq` -> absolute or raw frequency, `docf` -> document frequency. `arf` -> average reduced frequency.
   */
  wlnums?: "frq" | "docf" | "arf";
  /**
   * To select page of the response. The number of items on the page is specified by parameter wlmaxitems.
   */
  wlpage?: number;
  /**
   * Sets a regex to filter the results. Relevant only in a simple wordlist.
   */
  wlpat?: string;
  /**
   * Sets the sorting of the results. The default is `frq`, i.e. by absolute frequency. Docf means document frequency.
   */
  wlsort?: "frq" | "docf";
  /**
   * Parameter to set the format of ouput. Is it always set to `simple`, for the `struct_wordlist` is another enpoint called `struct_wordlist`.
   * @deprecated
   */
  wltype?: "simple" | "struct_wordlist";
};

export type GetWordListResponse = _wordlist;

export type GetStructWordListData = {
  /**
   * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
   */
  corpname: string;
  /**
   * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
   */
  format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
  /**
   * Includes, or excludes, nonwords in the in the result. Nonwords are tokens which do not start with letter of the alphabet (e.g. numbers, punctuation). The regex to match the nonwords is `[^[:alpha:]].*`. Certain specialized corpora may use their own specific definition of nonwords.
   */
  includeNonwords?: 1 | 0;
  /**
   * Parameter that represents if the wordlist is created from the first 10 milions lines of corpus. One if yes, no if he wordlist is created from the whole corpus.
   * @deprecated
   */
  random?: 1 | 0;
  /**
   * Calculate the document frequency for each item in the result. Must be used with `addfreqs` set to `docf`.
   */
  reldocf?: 1 | 0;
  /**
   * Includes the relative frequency of each item in the result.
   */
  relfreq?: 1 | 0;
  /**
   * Sets the corpus attribute you want to work with. Some corpora may have specific additional attributes.
   *
   * Basic examples:
   * - word
   * - lc
   * - lemma
   * - lemma_lc
   * - lempos
   * - lempos_lc
   * - tag
   * - pos
   *
   * For list of available word list attributes see **ca/api/corpora/{corpusId}**
   */
  wlattr: string;
  /**
     * A deny list (formerly known as blacklist) is a list of items that should be excluded from the result. The values should be be separated by a newline symbol (without commas between values). In the URL, the newline symbol is `
    `.
     */
  wlblacklist?: string;
  /**
   * Sets the case sensitivity of the corpus, i.e. the data are extracted from a lowercased version of the corpus. It is used for case insensitive analysis. Parameter "1" means case sensitivity, "0" means case insensitivity.
   */
  wlicase?: 1 | 0;
  /**
   * Sets the maximum frequency limit in the wordlist.
   */
  wlmaxfreq?: number;
  /**
   * Sets the number of items to be returned in the API response. It is not limited for user corpora, in preloaded corpora can be some limitation. This parameter is often used with wlpage to help with pagination in frontend development.
   */
  wlmaxitems?: number;
  /**
   * Sets the minimum frequency limit. Items with a lower frequency will not be included.
   */
  wlminfreq?: number;
  /**
   * The type of frequency. The values stand for: `frq` -> absolute or raw frequency, `docf` -> document frequency. `arf` -> average reduced frequency.
   */
  wlnums?: "frq" | "docf" | "arf";
  /**
   * To select page of the response. The number of items on the page is specified by parameter wlmaxitems.
   */
  wlpage?: number;
  /**
   * Sets a regex to filter the results. Relevant only in a simple wordlist.
   */
  wlpat?: string;
  /**
   * Sets the sorting of the results. The default is `frq`, i.e. by absolute frequency. Docf means document frequency.
   */
  wlsort?: "frq" | "docf";
  /**
   * Sets the attributes used for generating the wordlist. Up to 3 attributes are allowed (see wlstruct_attr2 and wlstruct_attr3). Some corpora may contain additional specific attributes.
   */
  wlstructAttr1: "word" | "lemma" | "tag" | "lempos";
  /**
   * Additional optional attribute that should be included in the result.
   */
  wlstructAttr2?: "word" | "lemma" | "tag" | "lempos";
  /**
   * Additional optional attribute that should be included in the result.
   */
  wlstructAttr3?: "word" | "lemma" | "tag" | "lempos";
  /**
   * Parameter to set the format of ouput. Is it always set to `simple`, for the `struct_wordlist` is another enpoint called `struct_wordlist`.
   * @deprecated
   */
  wltype?: "simple" | "struct_wordlist";
};

export type GetStructWordListResponse = _struct_wordlist;

export type GetConcordanceData = {
  /**
   * Switches the asynchronous processing on/off. ON = partial results are returned as soon as the first page is filled with results. OFF = results are returned only after the search is completed. Normally, ON is used in the web interface and OFF when using the API.
   */
  asyn?: 0 | 1;
  /**
   * Determines which tokens will be returned with additional attributes defined in `attrs`. `kw` will add the attributes to the KWIC only. `all` will return them with all tokens.
   */
  attrAllpos?: "kw" | "all";
  /**
   * A list of comma-delimited attributes that are returned together with each token. Other examples are:`word, lc, lemma, tag` etc..
   */
  attrs?: string;
  /**
   * Only works when `queryselector` is set to `charrow`. Type the characters that the tokens should contain. Regex is supported.
   */
  concordanceQueryChar?: string;
  /**
   * Only works when `queryselector` is set to `cqlrow`. Type the query using the [cql](https://www.sketchengine.eu/documentation/corpus-querying/) query language.
   */
  concordanceQueryCql?: string;
  /**
   * Only works when `queryselector` is set to `iqueryrow`. Type a word or phrase.
   *
   * These special wildcards are supported .
   *
   * Use the `asterisk (*)` for any number of unspecified characters. Use a `question mark (?)` for exactly one unspecified character. Use the `pipe (|)` to include more than one word or phrase. Use `two hyphens (--)` to find a word which is  hyphenated, non-hyphenated or spelt as two separate words.
   */
  concordanceQueryIquery?: string;
  /**
   * Only works when `queryselector` is set to `lemmarow`. Type the lemma. Regex is supported.
   */
  concordanceQueryLemma?: string;
  /**
   * Only works when `queryselector` is set to `phraserow`. Type the phrase. Regex is supported.
   */
  concordanceQueryPhrase?: string;
  /**
   * The query type. You can send it directly or via the `json` parameter, the results are the same.
   */
  concordanceQueryQueryselector?:
    | "iquery"
    | "cqlrow"
    | "lemmarow"
    | "charrow"
    | "wordrow"
    | "phraserow";
  /**
   * Only works when `queryselector` is set to `wordrow`. Type the word form. Regex is supported.
   */
  concordanceQueryWord?: string;
  /**
   * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
   */
  corpname: string;
  /**
   * (Only for error-annotated corpora.) A correction token to search.
   */
  cupCorr?: string;
  /**
   * (Only for error-annotated corpora.) An error token to search.
   */
  cupErr?: string;
  /**
   * (Only for error-annotated corpora). Determines which error type to higlight. An example of such a corpus is `preloaded/enwiki_error_sample_sentences`.
   */
  cupErrCode?:
    | ".*"
    | "lexicosemantic"
    | "punct"
    | "spelling"
    | "style"
    | "typographical"
    | "unclassified";
  /**
   * Only used with error-annotated corpora. It determines what should be highlighted. It is set to 'q' for corpora without error annotation. Meaning of individual options:
   *
   * - `q` -> to higlight query result.
   *
   * - `e` -> to higlight errors.
   *
   * - `c` -> to highlight corrections.
   *
   * - `b` -> to highlight both erros and corrections.
   *
   * Example of such a corpus can be `preloaded/enwiki_error_sample_sentences`.
   */
  cupHl?: "q" | "e" | "c" | "b";
  /**
   * The attribute applied to tokens in the query which do not have an attribute specified explicitly as part of the query.
   */
  defaultAttr?: string;
  /**
   * (Only for error-annotated corpora.) Determines what should be highlighted. Corr means **correction** and err means **error**. An **example** of such a corpus is `preloaded/enwiki_error_sample_sentences`.
   */
  errcorrSwitch?: "corr" | "err";
  /**
   * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
   */
  format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
  /**
   * The number of the page that should be returned.
   */
  fromp?: number;
  /**
   * An optinal way of **wraping parameters**. It is possible to send all parametres via this parameter only.
   *
   * The most frequent uses are:
   *
   * `queryselector`: To select the query type. Supported options are: **cqlrow**, **iqueryrow**, **lemmarow**, **charrow**, **phraserow**, **wordrow**. For more information see concordance_query parameters.
   *
   * `iquery`: Use with `iqueryrow`.
   *
   * `cql`: Use with `cqlrow`.
   *
   * `lemma`: Use with `lemmarow`.
   *
   * `lpos`: The part of speech of the lemma.
   *
   * `qmcase`: Sets the attribute to its lowercased version, i.e. the data are extracted from a lowercased version of the corpus. It used for case insensitive analysis. 1 = case sensitive), 0 = lowercased corpus/case insensitive).
   *
   *
   * `char`: Use with charrow.
   *
   * `phrase`: Use with phraserow.
   *
   * `word`: Use with wordrow
   *
   *
   * `name`:
   *
   * `pnfilter`:
   *
   * `inclkwic`:
   *
   * `filfpos`:
   *
   * `filtpos`:
   *
   * `desc`:
   *
   * `q`:
   *
   *
   */
  json?: {
    [key: string]: unknown;
  };
  /**
   * The size of the left context in KWIC view. Number of tokens.
   */
  kwicleftctx?: string;
  /**
   * The size of the right context in KWIC view. Number of tokens.
   */
  kwicrightctx?: string;
  /**
   * The part of speech of the lemma. The concrete values depend on the corpus. If the corpus contains the `lempos` attribute and `lpos` is empty, the result defaults to the most frequent part of speech of the lemma.
   */
  lpos?: "-n" | "-v" | "-j" | "-a" | "-d" | "-i";
  /**
   * The number of lines in the concordance.
   */
  pagesize?: number;
  /**
   * The CQL query. Regexes are supported for `lemma`, `phrase` and `word` types. The `iquery` supports simplified wildcards (see concordance_query[iquery]). If you decide to use the concordance_query in a json parameter, you do not have to use this parameter.
   */
  q?: string;
  /**
   * The text type (metadata) for which statistics should be calculated from the concordance. The default is `bncdoc.alltyp` (all available text types are included). Text types (attributes and there values) differ between corpora). You can find them in the response of `corpus_info` method in `freqttattrs` or `subcorpattrs` keys. Not all of them have attributes to show.
   */
  refs?: string;
  /**
   * A list of comma-delimited structures (=structure tags) that should be included in the result.
   */
  structs?: string;
  /**
   * The name of the `subcorpus`. The `default` value `empty string` refers to the entire corpus. An example for `preloaded/bnc2_tt21` can be `Written Academic` or `1960-1974`.
   */
  usesubcorp?: string;
  /**
   * Switches between sentence view and the KWIC view. `sen` returns complete sentences without trimming them. `kwic` returns the KWIC view with the query in the centre and some context left and right.
   */
  viewmode?: "sen" | "kwic";
};

export type GetConcordanceResponse = _concordance;

export type GetFullRefData = {
  /**
   * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
   */
  corpname: string;
  /**
   * The position of the first token of KWIC in the corpus.
   */
  pos?: number;
};

export type GetFullRefResponse = _fullref;

export type GetWideCtxData = {
  /**
   * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
   */
  corpname: string;
  /**
   * Size of the left context in tokens.
   */
  detailLeftCtx?: number;
  /**
   * Size of the right context in tokens.
   */
  detailRightCtx?: number;
  /**
   * Only used by the web interface. Indicates the number of tokens that should be highlighted in red.
   */
  hitlen?: number;
  /**
   * The position of the first token of KWIC in the corpus.
   */
  pos?: number;
  /**
   * A list of comma-delimited structures (=structure tags) that should be included in the result.
   */
  structs?: string;
};

export type GetWideCtxResponse = _widectx;

export type GetFreqMlData = {
  /**
   * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
   */
  corpname: string;
  /**
   * The number of items in one response.
   */
  fmaxitems?: number;
  /**
   * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
   */
  format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
  /**
   * The number of the response batch (page). The number of items in each batch is specified by `fmaxitems`.
   */
  fpage?: number;
  /**
   * The number of attributes for which the frequencies should be counted.
   */
  freqlevel?: 1 | 2 | 3 | 4 | 5 | 6;
  /**
   * The identifier of the sorted column. Use `frq` (default) to sort by frequency.
   */
  freqSort?: "frq" | "rel";
  /**
   * If there are more attributes (e.g. m1attr, m2attr), the results can be grouped by the first column/attribute.
   */
  group?: 1 | 0;
  /**
   * An optinal way of **wraping parameters**. It is possible to send all parametres via this parameter only.
   *
   * The most frequent uses are:
   *
   * `queryselector`: To select the query type. Supported options are: **cqlrow**, **iqueryrow**, **lemmarow**, **charrow**, **phraserow**, **wordrow**. For more information see concordance_query parameters.
   *
   * `iquery`: Use with `iqueryrow`.
   *
   * `cql`: Use with `cqlrow`.
   *
   * `lemma`: Use with `lemmarow`.
   *
   * `lpos`: The part of speech of the lemma.
   *
   * `qmcase`: Sets the attribute to its lowercased version, i.e. the data are extracted from a lowercased version of the corpus. It used for case insensitive analysis. 1 = case sensitive), 0 = lowercased corpus/case insensitive).
   *
   *
   * `char`: Use with charrow.
   *
   * `phrase`: Use with phraserow.
   *
   * `word`: Use with wordrow
   *
   *
   * `name`:
   *
   * `pnfilter`:
   *
   * `inclkwic`:
   *
   * `filfpos`:
   *
   * `filtpos`:
   *
   * `desc`:
   *
   * `q`:
   *
   *
   */
  json?: {
    [key: string]: unknown;
  };
  /**
   * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/) or structure attributes (metadata/text types) of any token in the concordance.\n\n A maximum of 6 attributes is allowed (e.g. ml2attr, ml3attr). At least one attribute is required.
   */
  ml1Attr: string;
  /**
   *  Position of the selected attribute in the concordance. **Minus** means **left** context (-1<0). **Plus** means **right** context (6>0). **At least one attribute is required, the others are optional.** Every attribute (ml1attr, ml2attr. etc.) needs it's **own** context position (e.g. if 3 attributes are selected three context position **needs to** be set ml1ctx, ml2ctx, ml3ctx).
   *
   *
   * **Positions can be referenced as follows:**
   *
   * `integer number` - where **0** is the first token in **KWIC**, **-1** the rightmost token in the left context etc.
   *
   * `1:x` - where **x** is one of the corpus structures (e.g. “doc” or “s” if the corpus has the particular markup). Its meaning is the first token in the structure, except when it is the right boundary of a range - then it is the last token in the structure. Also, other numbers can be used, e.g. -2:x, 3:x, etc. (-1 is the same as 1 with meaning “structure containing KWIC”)
   *
   * `a<0` - where **a** stands for a position reference as described in the first two points with meaning '**a** positions before/after the firs KWIC position' (so this is equivalent to **a**)
   *
   * `a>0` - where **a** stands for the same position reference with meaning 'positions before/after the last KWIC position'
   *
   * in the previous two points, if **0** is substituted with a natural number **k**, it means 'before/after **k**-th collocation' instead of 'before/after KWIC'. Collocations are special token groups in the context, that can be added using positive filters (see below).
   *
   *
   * `Ranges` can be referenced as a~b where **a**, **b** stand for token identifiers as above. Examples of positions and ranges:
   *
   * `-1<0` - rightmost token in the left context
   *
   * `3>0` - third token in right context
   *
   * `0>0` - last token in KWIC
   *
   * `0<0` - first token in KWIC
   *
   * `0<0~0>0` - range of KWIC
   *
   * `-1<0~1>0` - range of KWIC with one token from the left context and one from the right context
   *
   * `1:s` - first token in the sentence containing KWIC (or its first token)
   *
   * `1:s>0` - first token in the sentence containing KWIC (or its last token)
   *
   * `0<1` - first token of the first-added collocation.
   *
   *
   * `Examples:`
   *
   * sword/ **1>0~3>0**
   *
   * sword/ **1>0~3>0**
   *
   * slemma/ **0<0~0>0**
   *
   * sword/i **-1**
   *
   * sword/ **0** word/ir **-1<0** tag/r **-2<0**
   *
   *
   */
  ml1Ctx: string;
  /**
   * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/). Just like `ml1attr` but optional.
   */
  ml2Attr?: string;
  /**
   * Position of the selected attribute in the concordance. Minus means left context (-1<0). Plus means right context (6>0). Just like ml1ctx but optional.
   */
  ml2Ctx?: string;
  /**
   * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/). Just like `ml1attr` but optional.
   */
  ml3Attr?: string;
  /**
   * Position of the selected attribute in the concordance. Minus means left context (-1<0). Plus means right context (6>0). Just like ml1ctx but optional.
   */
  ml3Ctx?: string;
  /**
   * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/). Just like `ml1attr` but optional.
   */
  ml4Attr?: string;
  /**
   * Position of the selected attribute in the concordance. Minus means left context (-1<0). Plus means right context (6>0). Just like ml1ctx but optional.
   */
  ml4Ctx?: string;
  /**
   * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/). Just like `ml1attr` but optional.
   */
  ml5Attr?: string;
  /**
   * Position of the selected attribute in the concordance. Minus means left context (-1<0). Plus means right context (6>0). Just like ml1ctx but optional.
   */
  ml5Ctx?: string;
  /**
   * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/). Just like `ml1attr` but optional.
   */
  ml6Attr?: string;
  /**
   * Position of the selected attribute in the concordance. Minus means left context (-1<0). Plus means right context (6>0). Just like ml1ctx but optional.
   */
  ml6Ctx?: string;
  /**
   * The CQL query. Regexes are supported for `lemma`, `phrase` and `word` types. The `iquery` supports simplified wildcards (see concordance_query[iquery]). If you decide to use the concordance_query in a json parameter, you do not have to use this parameter.
   */
  q?: string;
  /**
   * Includes the percentage of the concordance in the result.
   */
  showpoc?: 1 | 0;
  /**
   * Includes the relative frequency in the result.
   */
  showrel?: 1 | 0;
  /**
   * Includes relative in text types value in the result.
   */
  showreltt?: 1 | 0;
  /**
   * The name of the `subcorpus`. The `default` value `empty string` refers to the entire corpus. An example for `preloaded/bnc2_tt21` can be `Written Academic` or `1960-1974`.
   */
  usesubcorp?: string;
};

export type GetFreqMlResponse = _freqml;

export type GetFregDistribData = {
  /**
   * Determines which tokens will be returned with additional attributes defined in `attrs`. `kw` will add the attributes to the KWIC only. `all` will return them with all tokens.
   */
  attrAllpos?: "kw" | "all";
  /**
   * A list of comma-delimited attributes that are returned together with each token. Other examples are:`word, lc, lemma, tag` etc..
   */
  attrs?: string;
  /**
   * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
   */
  corpname: string;
  /**
   * The attribute applied to tokens in the query which do not have an attribute specified explicitly as part of the query.
   */
  defaultAttr?: string;
  fcLemwordType?: string;
  fcLemwordWindowType?: string;
  fcLemwordWsize?: number;
  fcPosType?: string;
  fcPosWindowType?: number;
  fcPosWsize?: number;
  /**
   * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
   */
  format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
  /**
   * An optinal way of **wraping parameters**. It is possible to send all parametres via this parameter only.
   *
   * The most frequent uses are:
   *
   * `queryselector`: To select the query type. Supported options are: **cqlrow**, **iqueryrow**, **lemmarow**, **charrow**, **phraserow**, **wordrow**. For more information see concordance_query parameters.
   *
   * `iquery`: Use with `iqueryrow`.
   *
   * `cql`: Use with `cqlrow`.
   *
   * `lemma`: Use with `lemmarow`.
   *
   * `lpos`: The part of speech of the lemma.
   *
   * `qmcase`: Sets the attribute to its lowercased version, i.e. the data are extracted from a lowercased version of the corpus. It used for case insensitive analysis. 1 = case sensitive), 0 = lowercased corpus/case insensitive).
   *
   *
   * `char`: Use with charrow.
   *
   * `phrase`: Use with phraserow.
   *
   * `word`: Use with wordrow
   *
   *
   * `name`:
   *
   * `pnfilter`:
   *
   * `inclkwic`:
   *
   * `filfpos`:
   *
   * `filtpos`:
   *
   * `desc`:
   *
   * `q`:
   *
   *
   */
  json?: {
    [key: string]: unknown;
  };
  /**
   * The part of speech of the lemma. The concrete values depend on the corpus. If the corpus contains the `lempos` attribute and `lpos` is empty, the result defaults to the most frequent part of speech of the lemma.
   */
  lpos?: "-n" | "-v" | "-j" | "-a" | "-d" | "-i";
  normalize?: number;
  /**
   * The text type (metadata) for which statistics should be calculated from the concordance. The default is `bncdoc.alltyp` (all available text types are included). Text types (attributes and there values) differ between corpora). You can find them in the response of `corpus_info` method in `freqttattrs` or `subcorpattrs` keys. Not all of them have attributes to show.
   */
  refs?: string;
  res?: number;
  /**
   * A list of comma-delimited structures (=structure tags) that should be included in the result.
   */
  structs?: string;
  /**
   * Switches between sentence view and the KWIC view. `sen` returns complete sentences without trimming them. `kwic` returns the KWIC view with the query in the centre and some context left and right.
   */
  viewmode?: "sen" | "kwic";
};

export type GetFregDistribResponse = _freq_distrib;

export type GetFreqDistData = {
  /**
   * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
   */
  corpname: string;
  /**
   *  **Minus** means **left** context (-1<0). **Plus** means **right** context (6>0).
   *
   *
   * **Positions can be referenced as follows:**
   *
   * `integer number` - where **0** is the first token, **-1** the rightmost token in the left context etc.
   *
   * `1:x` - where **x** is one of the corpus structures (e.g. “doc” or “s” if the corpus has the particular markup). Its meaning is the first token in the structure, except when it is the right boundary of a range - then it is the last token in the structure. Also, other numbers can be used, e.g. -2:x, 3:x, etc. (-1 is the same as 1 with meaning “structure containing searched word”)
   *
   * `a<0` - where **a** stands for a position reference as described in the first two points with meaning '**a** positions before/after the first searched word position' (so this is equivalent to **a**)
   *
   * `a>0` - where **a** stands for the same position reference with meaning 'positions before/after the last searched word position'
   *
   * in the previous two points, if **0** is substituted with a natural number **k**, it means 'before/after **k**-th collocation' instead of 'before/after KWIC'. Collocations are special token groups in the context, that can be added using positive filters (see below).
   *
   *
   * `Ranges` can be referenced as a~b where **a**, **b** stand for token identifiers as above. Examples of positions and ranges:
   *
   * `-1<0` - rightmost token in the left context
   *
   * `3>0` - third token in right context
   *
   * `0>0` - last token
   *
   * `0<0` - first token
   *
   * `0<0~0>0` - range
   *
   * `-1<0~1>0` - range with one token from the left context and one from the right context
   *
   * `1:s` - first token in the sentence containing searched word (or its first token)
   *
   * `1:s>0` - first token in the sentence containing searched word (or its last token)
   *
   * `0<1` - first token of the first-added collocation.
   *
   *
   * `Examples:`
   *
   * sword/ **1>0~3>0**
   *
   * sword/ **1>0~3>0**
   *
   * slemma/ **0<0~0>0**
   *
   * sword/i **-1**
   *
   * sword/ **0**
   *
   * word/ir **-1<0**
   *
   * tag/r **-2<0**
   *
   *
   */
  ctx?: string;
  /**
   * A diachronic attribute to be selected. Available attributes **can differ** in corpora. Examples can be **doc.year** or **doc.month**.
   */
  diaattr: string;
  /**
   * An optinal way of **wraping parameters**. It is possible to send all relevant parametres via this parameter only. It is classic JSON format.
   *
   * `wordlist`: words for which the relative frequency should be counted.
   *
   *
   */
  json?: {
    [key: string]: unknown;
  };
  /**
   * `1`: display results during calculation
   * `0`: display results after all data has been calculated (can take quite a lot of time).
   */
  sse: "1" | "0";
  /**
   * Determines which periods are included in the results. It signifies the percentage above the average size, acting as a **limit**. When relative frequency (rel_frq) surpasses this limit, it is discarded (moved to **removed_freqdist** object).
   */
  threshold: string;
  /**
   * Sets the corpus attribute you want to work with. Some corpora may have specific additional attributes.
   *
   * Basic examples:
   * - word
   * - lc
   * - lemma
   * - lemma_lc
   * - lempos
   * - lempos_lc
   * - tag
   * - pos
   *
   * For list of available word list attributes see **ca/api/corpora/{corpusId}**
   */
  wlattr: string;
  /**
   * A wordlist of words for which the relative frequency should be counted. No exact example is here because it is already set in `JSON` parameter.\Example: ['the','a','lion'].
   */
  wordlist?: string;
};

export type GetFreqDistResponse = _freqdist;

export type GetCollxData = {
  /**
   * The (positional attribute)[https://www.sketchengine.eu/my_keywords/positional-attribute/] (lemma, word form etc.) used in the computation.
   */
  cattr?: string;
  /**
   * Defines the types of statistics (association measures) to be computed.
   *
   * `t` -> T-score
   *
   * `m` -> MI
   *
   * `3` -> MI3
   *
   * `l` -> log likelihood
   *
   * `s` -> min. sensitivity
   *
   * `p` -> MI.log_f
   *
   * `r` -> relative freq.
   *
   * `f` -> absolute freq.
   *
   * `d` -> logDice.
   *
   * To send one value just type the value for example `t`. If you need to send more values write it as `["t","m","d","3","l","s","p"]`.
   */
  cbgrfns?: string;
  /**
   * The left boundary of the window in which the collocations should be identified. Defined by the token position left or right of KWIC.
   */
  cfromw?: number;
  /**
   * Sets the maximum number of items in the response.
   */
  cmaxitems?: number;
  /**
   * The minimum frequency of the token in the window defined by `cfromw` and `ctow`.
   */
  cminbgr?: number;
  /**
   * The minimum frequency of the token in the corpus.
   */
  cminfreq?: number;
  /**
   * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
   */
  corpname: string;
  /**
   * Function according to which the result is sorted.
   */
  csortfn?: "t" | "m" | "3" | "l" | "s" | "p" | "r" | "f" | "d";
  /**
   * The right boundary of the window in which the collocations should be identified. Defined by the token position left or right of KWIC.
   */
  ctow?: number;
  /**
   * An optinal way of **wraping parameters**. It is possible to send all relevant parametres via this parameter only. It is classic JSON format.
   *
   * The most frequent uses are:
   *
   * `queryselector`: To select the query type. Supported options are: **cqlrow**, **iqueryrow**, **lemmarow**, **charrow**, **phraserow**, **wordrow**.
   *
   * `iquery`: Use with `iqueryrow`.
   *
   * `cql`: Use with `cqlrow`.
   *
   * `lemma`: Use with `lemmarow`.
   *
   * `lpos`: The part of speech of the lemma.
   *
   * `qmcase`: Sets the attribute to its lowercased version, i.e. the data are extracted from a lowercased version of the corpus. It used for case insensitive analysis. 1 = case sensitive), 0 = lowercased corpus/case insensitive).
   *
   * `char`: Use with charrow.
   *
   * `phrase`: Use with phraserow.
   *
   * `word`: Use with wordrow
   *
   * `cbgrfns`: If you need to send more cbgrfnsDefines the types of statistics (association measures) to be computed. See example.
   */
  json?: {
    [key: string]: unknown;
  };
  /**
   * The CQL query. Regexes are supported for `lemma`, `phrase` and `word` types. The `iquery` supports simplified wildcards (see concordance_query[iquery]). If you decide to use the concordance_query in a json parameter, you do not have to use this parameter.
   */
  q?: string;
  /**
   * The name of the `subcorpus`. The `default` value `empty string` refers to the entire corpus. An example for `preloaded/bnc2_tt21` can be `Written Academic` or `1960-1974`.
   */
  usesubcorp?: string;
};

export type GetCollxResponse = _collx;

export type GetSubCorpData = {
  /**
   * Set to `1` if the corpus should be deleted. Only user subcorpora can be deleted. Nothing will be deleted if left empty (default value == 0).
   */
  _delete?: 0 | 1;
  /**
   * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
   */
  corpname: string;
  /**
   * Set to `1` if new subcorpus should be created. Subcorpus will not be created if left empty (default value == 0).
   */
  create?: 0 | 1;
  /**
   * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
   */
  format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
  /**
   * Used to specify text types for a subcorpus. Takes a JSON object as input, where the key-value pairs define the specific attributes. The attributes can vary depending on the corpus.
   *
   * When using the json parameter in a query, you can define a JSON object with one or more attributes. Each attribute can have a single value or an array of values.
   *
   *
   * The list of available text types for specific corpus can be obtained via `/search/corp_info` just add `sca_` in front the name (see examples).
   *
   *
   * `Examples:`
   *
   *
   * - To create a subcorpus based on a specific spoken text type from the BNC corpus:
   *
   * **{"sca_bncdoc.alltyp":["Spoken context-governed"]}**
   *
   *
   * - To filter texts from the BNC corpus that are both spoken context-governed and spoken demographic:
   *
   * **{"sca_bncdoc.alltyp":["Spoken context-governed","Spoken demographic"]}**
   *
   *
   * - To select texts from the BNC corpus from a specific time period (1960-1974):
   *
   * **{"sca_bncdoc.alltim":["1960-1974"]}**
   *
   *
   * - To create a subcorpus with texts from specific authors and time periods, along with regional specifications:
   *
   * **{"sca_bncdoc.author": ["Author1","Author2",...],"sca_bncdoc.alltim": ["1985-1993","1975-1984"], "sca_bncdoc.wripp": ["UK (unspecific)","Ireland"]}**
   *
   *
   * - To filter texts from the Ententen corpus based on domain and topic:
   *
   * **{"sca_doc.tld":["org","com"], "sca_doc.topic": ["arts","beauty & fashion","cars & bikes","culture & entertainment"]}**
   *
   *
   * - For a user-specific corpus, filtering based on document ID and filename:
   *
   * **{"sca_doc.id":["file29173711"],"sca_doc.filename":["Filename.pdf"]}**
   */
  json?: {
    [key: string]: unknown;
  };
  /**
   * Query for creating subcorpora from concordance.
   *
   * The search criteria are specified within brackets following a prefix like `alemma` or `aword`. This prefix often indicates the type of linguistic search (e.g., lemma-based, word-based). The criteria within the brackets can include checks for specific words, lemmas, parts of speech and more, using operators like | (OR), & (AND), and regular expressions.
   *
   * The lists of available attributes, pos tags for specific corpus can be obtained via `/search/corp_info`.
   *
   *
   * `Examples:`
   *
   *
   * - Simple word or lemma search in the BNC corpus:
   *
   * **q=alemma,[lc="test" | lemma_lc="test"]**
   *
   *
   * - Search for nouns with the lemma 'test' in a case-sensitive manner:
   *
   * **q=alemma,[lempos_lc="(test)-n"]**
   *
   *
   * - Search for verbs with the lemma 'test', case-insensitive:
   *
   * **q=alemma,[lempos="(test)-v"]**
   *
   *
   * - Searching for a specific phrase 'test' in a case-sensitive manner:
   *
   * **q=aword,[word="test"]**
   *
   *
   * - Searching for the numeral '1955':
   *
   * **q=alemma,[word="1955" & tag="CD"]**
   *
   *
   * - Regex-based search for words containing the character 'h':
   *
   * **q=alemma,[word=".\*h.\*"]**
   *
   *
   * - Complex search involving the lemma 'book' followed by up to three words, then a verb:
   *
   * **q=alemma,[lemma="book"][]{1,3}[tag="V.\*"]**
   */
  q?: string;
  /**
   * Which corpus structure should be used in new subcorpus. Used withim concordance type of subcorpus. The lists of structures can be obtained via `/search/corp_info`.
   */
  struct?: string;
  /**
   * Name of the subcorpus.
   */
  subcname: string;
};

export type GetSubCorpResponse = _subcorp;

export type SubcorpusRenameData = {
  /**
   * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
   */
  corpname: string;
  /**
   * A new name for the subcorpus.
   */
  newSubcorpName: string;
  /**
   * The name of subcorpus you want to rename.
   */
  subcorpId: string;
};

export type SubcorpusRenameResponse = _subcorpus_rename;

export type SubcorpusInfoData = {
  /**
   * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
   */
  corpname: string;
  /**
   * Name of the subcorpus.
   */
  subcname: string;
};

export type SubcorpusInfoResponse = _subcorp_info;

export type GetExtractKeywordsData = {
  /**
   * Represent what kind of frequnecy should be calculated. When used with `reldocf` it is set to `docf` to calculate document frequency.
   */
  addfreqs?: string;
  /**
   * Limits the results to items containing only alphanumeric characters.
   */
  alnum?: 1 | 0;
  /**
   * Switches between the computation of keywords, terms, key n-grams and key collocations. With keywords and n-grams, it also sets the attribute to be used for the computation.
   *
   * For keywords, set to the required attribute, usually `lc`, `word` or `lemma`.
   *
   * For n-grams, set to the required attribute and set `usengrams`, `ngrams_n` and `ngrams_max_n`.
   *
   * For terms, set the attribute to `TERM`.
   *
   * For collocations (word sketch triples, equivalent of using the Word Sketch with AS A LIST option in the web interace), set the attribute to `WSCOLLOC`. Consider using `wlpat`.
   */
  attr?: "lemma" | "word" | "TERM" | "WSCOLLOC";
  /**
   * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
   */
  corpname: string;
  /**
   * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
   */
  format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
  /**
   * Switches to the `lc` attribute, i.e. the lower-cased version of the corpus to allow case insensitive searching. `1` means that case sensitivity is off, `0` means it is on.
   */
  icase?: 1 | 0;
  /**
   * Includes, or excludes, nonwords in the in the result. Nonwords are tokens which do not start with letter of the alphabet (e.g. numbers, punctuation). The regex to match the nonwords is `[^[:alpha:]].*`. Certain specialized corpora may use their own specific definition of nonwords.
   */
  includeNonwords?: 1 | 0;
  /**
   * Sets the maximum frequency of the item.
   */
  maxfreq?: number;
  maxKeywords?: number;
  /**
   * Sets the minimum frequency of the item.
   */
  minfreq?: string;
  /**
   * The maximum n-gram length. The maximum is `6`.
   */
  ngramsMaxN?: 2 | 3 | 4 | 5 | 6;
  /**
   * The minimum n-gram length. Usually used with `ngrams_max_n` and `usengrams`.
   */
  ngramsN?: 2 | 3 | 4 | 5;
  /**
   * Limits the results to items containing at least one alphanumberic character. Words such as 16-year-old or 3D will be included.
   */
  onealpha?: 1 | 0;
  /**
   * Corpus name of the reference corpus, it must have the same processing (the same attributes, the same term grammar).
   */
  refCorpname: string;
  /**
   * Calculate the document frequency for each item in the result. Must be used with `addfreqs` set to `docf`.
   */
  reldocf?: 1 | 0;
  /**
   * The smoothing parameter for (simple maths) [https://www.sketchengine.eu/documentation/simple-maths/].
   */
  simpleN?: "1" | "0";
  /**
   * Represent if n-grams should be extracted or just simple keywords.
   */
  usengrams?: 0 | 1;
  /**
   * The name of the `subcorpus`. The `default` value `empty string` refers to the entire corpus. An example for `preloaded/bnc2_tt21` can be `Written Academic` or `1960-1974`.
   */
  usesubcorp?: string;
  /**
     * A deny list (formerly known as blacklist) is a list of items that should be excluded from the result. The values should be be separated by a newline symbol (without commas between values). In the URL, the newline symbol is `
    `.
     */
  wlblacklist?: string;
  /**
   * Defines the allow list (formerly known as whitelist), the list of words which should be included in the list. See also `wlblacklist`.
   */
  wlfile?: string;
  /**
   * Sets a regex to filter the results. Relevant only in a simple wordlist.
   */
  wlpat?: string;
};

export type GetExtractKeywordsResponse = _extract_keywords;

export type GetTextTypesWithNormsData = {
  /**
   * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
   */
  corpname: string;
};

export type GetTextTypesWithNormsResponse = _textypes_with_norms;

export type GetAttrValsData = {
  /**
   * Selects a structure attribute (reffered to as text type in the web interface). Corpora have different numbers structure attributes and their values. You can find them in the response of the `corpus_info` method in `freqttattrs` or `subcorpattrs` keys. Not all of them have attributes to show.
   */
  avattr: string;
  /**
   * The starting index from which to return the results.
   */
  avfrom?: number;
  /**
   * The number of items to return.
   */
  avmaxitems?: number;
  /**
   * A regex to filter the results. Empty string defaults to `.*` (match everything).
   */
  avpat?: string;
  /**
   * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
   */
  corpname: string;
  /**
   * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
   */
  format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
  /**
   * Switches to the `lc` attribute, i.e. the lower-cased version of the corpus to allow case insensitive searching. `1` means that case sensitivity is off, `0` means it is on.
   */
  icase?: 1 | 0;
};

export type GetAttrValsResponse = _attr_vals;

export type GetCorporaResponse = {
  data?: Array<_corpora_list>;
};

export type CreateCorpusData = {
  /**
   * Set the language, corpus name and corpus description.
   *
   * - `info` => The additional information for a newly created corpus. (string)
   *
   * - `language_id` => Language iso-code. `ISO 639-1`. (string)
   *
   * - `name` => Unique `corpus name` for a newly created corpus. (string)
   */
  requestBody: _corpora_request;
};

export type CreateCorpusResponse = {
  data?: _corpora_single;
};

export type GetCorpusData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
};

export type GetCorpusResponse = _corpora_single;

export type UpdateCorpusData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   *  - `expert_mode` => Set to **True** if you are hard-core. (boolean)
   *
   * - `name` => Corpus name. **Given by user**. (string)
   *
   * - `info` => Additional info about corpus. (string)
   *
   * - `document_order` => Can be set to enforce document order within the corpus. (list of integers)
   *
   * - `structures` => Available structures or tags in the corpus. Structures like **s** (sentence), **g** (glue), **doc** (document).(list)
   *
   * - `name` => Structure name. Example: **s**. (string)
   *
   * - `attributes` => A list of used attributes in corpus. (list)
   *
   * - `name` => The name of used attribute. (string)
   *
   * - `file_structure` => The structure in which individual documents should be wrapped. Usually **doc**. (string)
   *
   * - `onion_structure` => The structure for deduplication. Usually **p** (paragraph) or **Null** (no deduplication). (string)
   *
   * - `docstructure` => Structure in which individual documents should be wrapped. Usually **doc**. (string)
   *
   * - `sketch_grammar_id` => Name of sketch grammar file. For sketch grammars querying. Sketch grammar is a series of rules written in the CQL query  language that search for collocations in a text corpus and categorize them according to their  grammatical relations. Example: **preloaded/english-penn_tt-3.3.wsdef.m4**. (string)
   *
   * - `term_grammar_ir` => Name of term grammar file. Term grammar tells Sketch Engine which words and phrases should indentify as terms. Example: **corpora/wsdef/english-penn_tt-terms-3.1.termdef.m4**. (string)
   */
  requestBody?: _corpus_update;
};

export type UpdateCorpusResponse = {
  data?: _corpora_single;
};

export type DeleteCorpusData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
};

export type DeleteCorpusResponse = void;

export type CheckCompilableData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
   */
  requestBody?: _empty_request;
};

export type CheckCompilableResponse = _can_be_compiled;

export type GetCompilationProgressData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
   */
  requestBody: _empty_request;
};

export type GetCompilationProgressResponse = _get_progress;

export type CompileCorpusData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   *  `Structures` or `structure attributes` in corpus which should be compiled. Usually: `all`. (string)
   */
  requestBody: _compile_request;
};

export type CompileCorpusResponse = _rpc_style;

export type GetCompilationLogData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * Name of log file. Name 'last.log' show the newest log for that corpus.
   */
  logName: string;
};

export type GetCompilationLogResponse = string;

export type GetCorpusSourceData = {
  /**
   * Required when you want to download parallel corpora, **when format == tmx.** Specify aligned corpus name.
   */
  aligned?: string;
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * The contents of each file will be enclosed in a XML like structure of the specified name with the filename as its id attribute and the URL (if available) as the url attribute. If empty document boundaries will be lost. Example: `doc`.
   */
  fileStructure?: string;
  /**
   * File format in which the corpus should be downloaded. Just three formats are supported.
   */
  format: string;
};

export type GetCorpusSourceResponse = string;

export type CancelJobData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
   */
  requestBody?: _empty_request;
};

export type CancelJobResponse = _rpc_style;

export type CompileAlignedCorpusData = {
  /**
   * List of corpus IDs used in aligned compilation.
   *
   * - `corpus_ids` => A list of **Corpus ID** of multilingual corpora. (integer)
   *
   * - `structures` => Represent if **all** structures should be used during compilation (in that case it should be contain just **all**) or just some of them. (string)
   */
  requestBody?: _corpus_ids;
};

export type CompileAlignedCorpusResponse = _rpc_style;

export type SegmentAlignData = {
  /**
   * - `alignstruct` => According to which structure the document should be aligned. Usually, **\<s>**. (string)
   *
   * - `auto` => **True**, when documents are not compiled. Sketch Engine will align them automatically. (boolean)
   *
   * - `corpus_ids` => A list of **Corpus ID** of multilingual corpus. (integer)
   */
  requestBody?: _align_req;
};

export type SegmentAlignResponse = _rpc_style;

export type GetAllDocumentsData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * ID of file subdirectory. **0** stands for default document directory with name `upload`.
   */
  filesetId?: number;
};

export type GetAllDocumentsResponse = _documents_get;

export type CreateNewDocumentData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * ID of file subdirectory. **0** stands for default document directory with name `upload`.
   */
  filesetId?: number;
  /**
   * File to upload.
   */
  formData?: {
    /**
     * File to upload.
     */
    file?: Blob | File;
  };
  /**
   * Delay tagging by given number of `seconds`.
   */
  waitWithTagging?: number;
};

export type CreateNewDocumentResponse = _documents_post;

export type UpdateDocumentMetadataData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * ID of file subdirectory. **0** stands for default document directory with name `upload`.
   */
  filesetId?: number;
  /**
   *   - `id` => Unique numeric `document ID`. (integer)
   *
   * - `metadata` => Pairs of `attribute_name`:`value`.
   */
  requestBody?: _doc_metadata;
};

export type UpdateDocumentMetadataResponse = _documents_get;

export type GetDocumentData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * Document ID. For document querying.
   */
  documentId: number;
};

export type GetDocumentResponse = _documents_get;

export type UpdateDocumentData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * Document ID. For document querying.
   */
  documentId: number;
  /**
   *  - `filename_display` => Name of documents. (string)
   *
   * - `id` => Unique numeric **document ID** to identify individual documents. (integer)
   *
   * - `inProgress` => Represents whether the currently edited document is in use. (boolean)
   *
   * - `isArchive` => Represents if the updated document is in a format like .zip (created via some archive manager). (boolean)
   *
   * - `metadata` => Metadata of document. For example, additional attributes and values.
   *
   * - `parameters` => Parameters for plaintext extraction.
   *
   * - `encoding` => Encoding standard of the document. Usually, **UTF-8**. (string)
   *
   * - `justext_stoplist` => Represent the list of unimportant words, in a specified language, from an NLP point of view. (string)
   *
   * - `permutation` => Changing the order of columns (applies only to **type=vert**). (integer)
   *
   * - `tmx_lang` => TMX (translation memory exchange). Language of document used for parallel corpus creation. (string)
   *
   * - `tmx_struct` => Alignment structure to be used for multilingual documents, **align** is the most used structure. Used within segment distinction, which sentence is in which language and to put sentences with the same meaning into one segment. (string)
   *
   * - `tmx_untranslated` => Placeholder for empty segments in multilingual documents. The segments which have no counterpart in a second language of parallel corpus. (string)
   *
   * - `type` => File format (.csv, .doc, .docx, .htm, .html). (string)
   *
   * - `unlegalese` => Convert **all-caps** text to **normal case**. (boolean)
   *
   * - `temporary` => Is document temporary or not. (boolean)
   *
   * - `word_count` => Total number of **words** (tokens minus punctuation etc.) in document. (integer)
   *
   * - `vertical_progress` => Progress of **vertical file** creation. (integer)
   *
   * - `vertical_error` => An error occured while creating the vertical file. If the creation was succesfull the value is **Null**. (string)
   */
  requestBody?: _doc_put_req;
};

export type UpdateDocumentResponse = _documents_get;

export type DeleteDocumentsData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * Document ID. For document querying.
   */
  documentId: number;
};

export type DeleteDocumentsResponse = void;

export type UpdateDocumentParametersData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * Document ID. For document querying.
   */
  documentId: number;
  /**
   *  - `auto_paragraphs` => Automatically insert paragraph breaks (**\<p>**) in place of blank lines. (string)
   *
   * - `encoding` => Encoding standard of the document. Usually **UTF-8**. (string)
   *
   * - `justext_stoplist` => Represent the list of unimportant words, in a specified language, from an NLP point of view. (string)
   *
   * - `permutation` => Changing the order of columns (applies only to **type=vert**).
   *
   * - `tmx_lang` => TMX (translation memory exchange). Language of document used for parallel corpus creation. (string)
   *
   * - `tmx_struct` => Alignment structure to be used for multilingual documents, **align** is the most used structure. Used within segment distinction, which sentence is in which language and to put sentences with the same meaning into one segment. (string)
   *
   * - `tmx_untranslated` => Placeholder for empty segments in multilingual documents. The segments which have no counterpart in a second language of parallel corpus. (string)
   *
   * - `type` => File format (.csv, .doc, .docx, .htm, .html). (string)
   *
   * - `unlegalese` => Convert **all-caps** text to **normal case**. (boolean)
   */
  requestBody?: _doc_preview;
};

export type UpdateDocumentParametersResponse = _doc_preview;

export type GetDocumentOriginalData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * Document ID. For document querying.
   */
  documentId: number;
};

export type GetDocumentOriginalResponse = Blob | File;

export type GetDocumentPlaintextData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * Document ID. For document querying.
   */
  documentId: number;
};

export type GetDocumentPlaintextResponse = string;

export type GetDocumentVerticalData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * Document ID. For document querying.
   */
  documentId: number;
};

export type GetDocumentVerticalResponse = string;

export type ExpandArchiveData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * Document ID. For document querying.
   */
  documentId: number;
  /**
   *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
   */
  requestBody?: _empty_request;
};

export type ExpandArchiveResponse = _rpc_expand_archive;

export type CancelDocumentJobData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * Document ID. For document querying.
   */
  documentId: number;
  /**
   *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
   */
  requestBody?: _empty_request;
};

export type CancelDocumentJobResponse = _rpc_style;

export type GetProgressData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * Document ID. For document querying.
   */
  documentId: number;
  /**
   *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
   */
  requestBody?: _empty_request;
};

export type GetProgressResponse = _get_progress;

export type GetFileSetsData = {
  /**
   * Start corpus compiling after web-crawler finishes downloading content from the internet.
   */
  compileWhenFinished?: number;
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
};

export type GetFileSetsResponse = {
  data?: Array<_fileset>;
};

export type CreateFileSetData = {
  /**
   * Start corpus compiling after web-crawler finishes downloading content from the internet.
   */
  compileWhenFinished?: number;
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * Setting parameters to improve web-crawler accuracy.
   *
   * - `bl_max_total_kw` => **Blacklist max total keyword**. Means that web page or document will be discarded if it contains more words from the denylist (blacklist) than this limit. (integer)
   *
   * - `bl_max_unique_kw` => **Blacklist max unique keyword**. Means that web page or document will be discarded if it contains more unique words from the denylist (blacklist) than this limit. (integer)
   *
   * - `black_list` => A list (separated by whitespaces) of **blocked words**, words you don't want to see in your future corpus. (string)
   *
   * - `input_type` => Input types the web-crawler will works with. Example: **urls**. (string)
   *
   * - `max_cleaned_file_size` => Web pages and documents with a size **over** this limit (**in kB**) will be ignored. (integer)
   *
   * - `max_file_size` => Web pages and documents with a size **over** this limit (**in kB**) will be ignored. (integer)
   *
   * - `min_cleaned_file_size` => Web pages and documents **smaller** than this limit (**in kB**) after cleaning will be ignored. Cleaning involves conversion to plain text, removing boilerplate text (e.g. navigation menus, legal text, disclaimers and other repetitive content). (integer)
   *
   * - `min_file_size` => Web pages and documents with a **size below** this limit (**in kB**) will be ignored. (integer)
   *
   * - `name` => Texts will be organized into a corpus folder with this name. (string)
   *
   * - `seed_word` => A list of words according to which the URLs were chosen to be searched. (string)
   *
   * - `white_list` => A list (separated by whitespaces) of allowed words, words you want to see in your future corpus. (list of string)
   *
   * - `wl_min_kw_ratio` => **Whitelist minimal keywords ratio**. Means that web page or document will be included only if the percentage of allowlist words compared to total words is higher than this limit. (integer)
   *
   * - `wl_min_total_kw` => **Whitelist minimal total keywords**. Means that web page or document will be included only if it contains more words from the allowlist (whitelist) than this limit. (integer)
   *
   * - `wl_min_unique_kw` => **Whitelist minimal unique keywords**. Means that a web page or document will be included only if it contains more words from the allowlist (whitelist) than this limit. (integer)
   */
  requestBody: _filesets_creation;
};

export type CreateFileSetResponse = {
  data?: _fileset_creation;
};

export type GetFileSetData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * ID of file subdirectories. If sets to 0 it will return top-level folder of documents, so if you have a web corpora with folders web1 and web2 it will return web1.
   */
  filesetId: number;
};

export type GetFileSetResponse = {
  data?: _fileset;
};

export type DeleteFileSetData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * ID of file subdirectories. If sets to 0 it will return top-level folder of documents, so if you have a web corpora with folders web1 and web2 it will return web1.
   */
  filesetId: number;
};

export type DeleteFileSetResponse = void;

export type CancelFileSetJobData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * ID of file subdirectories. If sets to 0 it will return top-level folder of documents, so if you have a web corpora with folders web1 and web2 it will return web1.
   */
  filesetId: number;
  /**
   *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
   */
  requestBody?: _empty_request;
};

export type CancelFileSetJobResponse = _rpc_style;

export type GetFileSetProgressData = {
  /**
   * Numeric corpus ID. For corpora querying.
   */
  corpusId: number;
  /**
   * ID of file subdirectories. If sets to 0 it will return top-level folder of documents, so if you have a web corpora with folders web1 and web2 it will return web1.
   */
  filesetId: number;
  /**
   *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
   */
  requestBody?: _empty_request;
};

export type GetFileSetProgressResponse = _filesets_get_progress;

export type GetLanguagesResponse = {
  data?: Array<_language>;
};

export type UploadAligendDocumentsData = {
  /**
   * Aligned multilingual file (mostly in '.tmx' file type).
   */
  requestBody?: string;
};

export type UploadAligendDocumentsResponse = _somefiles_post;

export type GetAlignedDocumentsData = {
  /**
   * Alphanumeric multilanguage file ID
   */
  somefileId: string;
};

export type GetAlignedDocumentsResponse = _somefiles_post;

export type UpdateAlignedDocsData = {
  /**
   *  - `corpora`
   *
   * - `guessed_language_code`
   *
   * - `language_id` => Language iso-code. **ISO 639-1**. (string)
   *
   * - `name` => Language name in **English**. (string)
   */
  requestBody?: _somefiles_put;
  /**
   * Alphanumeric multilanguage file ID
   */
  somefileId: string;
};

export type UpdateAlignedDocsResponse = {
  data?: _corpora_single_full;
};

export type GetUserTemplateData = {
  /**
   * Numerical template ID, but preloaded templates do not have ID but you can query them by their name. Example: `UNIVERSAL_3`.
   */
  templateId: string;
};

export type GetUserTemplateResponse = _template;

export type UpdateUserTemplateData = {
  /**
   *  - `id` => Alphanumeric **template/tagset ID**. The terms **tagset** and **templates** are interchangeable. (string)
   *
   * - `name` => Name of **template/tagset** file. (string)
   *
   * - `owner_id` => Unique numeric owner ID (usually you). If tagset/template is preloaded Null. (integer)
   *
   * - `owner_name` => Tagset/template owner name (usually you). If tagset/template is preloaded Null. (string)
   *
   * - `has_pipeline` => Vertical creation is supported. False for legacy templates. (boolean)
   *
   * - `has_tags` => Morphological tagging is supported. (boolean)
   *
   * - `has_lemmas` => Lemmatization is supported. (boolean)
   *
   * - `static_attributes` => A list of attributes which can appear in corpus.
   *
   * - `structures` => A list of used structures. Examples \<s>, \<g>.
   *
   * - `tagsetdoc` => URL leading to template/tagset documentation. (string)
   *
   * - `content` => Content of tagset. (string)
   *
   * - `default_sketchgrammar_id` => Not ID, as you probably imagine, but filename of preselected sketchgrammar (.m4 format). (string)
   *
   * - `default_termgrammar_id` => Not ID, as you probably imagine, but filename of preselected sketchgrammar (.m4 format). (string)
   *
   * - `sharing` => List.
   *
   * - `users` => The ID of user you share template with.
   *
   * - `id` => The ID of group you share template with.
   */
  requestBody?: _template_put;
  /**
   * Numerical template ID, but preloaded templates do not have ID but you can query them by their name. Example: `UNIVERSAL_3`.
   */
  templateId: string;
};

export type UpdateUserTemplateResponse = {
  data?: _template;
};

export type DeleteUserTemplateData = {
  /**
   * Numerical template ID, but preloaded templates do not have ID but you can query them by their name. Example: `UNIVERSAL_3`.
   */
  templateId: string;
};

export type DeleteUserTemplateResponse = void;

export type GetUsedSpaceData = {
  /**
   *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
   */
  requestBody?: _empty_request;
};

export type GetUsedSpaceResponse = _get_used_space;

export type $OpenApiTs = {
  "/search/corp_info": {
    get: {
      req: {
        /**
         * Results of the last corpcheck (if available in the compilation log).
         */
        corpcheck?: 0 | 1;
        /**
         * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
         */
        corpname: string;
        /**
         * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
         */
        format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
        /**
         * A list of grammatical relations from the correspoding `sketch grammar`.
         */
        gramrels?: 0 | 1;
        /**
         * The content of the registry file (registry_dump and registry_text).
         */
        registry?: 0 | 1;
        /**
         * The lexicon sizes of structure attributes.
         */
        structAttrStats?: 0 | 1;
        /**
         * A parameter to obtain the list of subcorpora and their sizes (e.g. number of tokens, words).
         */
        subcorpora?: 0 | 1;
        /**
         * The name of the `subcorpus`. The `default` value `empty string` refers to the entire corpus. An example for `preloaded/bnc2_tt21` can be `Written Academic` or `1960-1974`.
         */
        usesubcorp?: string;
      };
      res: {
        /**
         * `OK`
         */
        200: _corp_info;
      };
    };
  };
  "/search/wordlist": {
    get: {
      req: {
        /**
         * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
         */
        corpname: string;
        /**
         * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
         */
        format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
        /**
         * Includes, or excludes, nonwords in the in the result. Nonwords are tokens which do not start with letter of the alphabet (e.g. numbers, punctuation). The regex to match the nonwords is `[^[:alpha:]].*`. Certain specialized corpora may use their own specific definition of nonwords.
         */
        includeNonwords?: 1 | 0;
        /**
         * N-grams which are sub-ngrams of a longer n-gram will be grouped together with the longer n-gram. Nesting only works when a `ngrams_n` and `ngrams_max_n` are different values.
         */
        nestNgrams?: 1 | 0;
        /**
         * The maximum n-gram length. The maximum is `6`.
         */
        ngramsMaxN?: 2 | 3 | 4 | 5 | 6;
        /**
         * The minimum n-gram length. Usually used with `ngrams_max_n` and `usengrams`.
         */
        ngramsN?: 2 | 3 | 4 | 5;
        /**
         * Parameter that represents if the wordlist is created from the first 10 milions lines of corpus. One if yes, no if he wordlist is created from the whole corpus.
         * @deprecated
         */
        random?: 1 | 0;
        /**
         * Calculate the document frequency for each item in the result. Must be used with `addfreqs` set to `docf`.
         */
        reldocf?: 1 | 0;
        /**
         * Includes the relative frequency of each item in the result.
         */
        relfreq?: 1 | 0;
        /**
         * The smoothing parameter for (simple maths) [https://www.sketchengine.eu/documentation/simple-maths/].
         */
        simpleN?: "1" | "0";
        /**
         * Represent if n-grams should be extracted or just simple keywords.
         */
        usengrams?: 0 | 1;
        /**
         * The name of the `subcorpus`. The `default` value `empty string` refers to the entire corpus. An example for `preloaded/bnc2_tt21` can be `Written Academic` or `1960-1974`.
         */
        usesubcorp?: string;
        /**
         * Sets the corpus attribute you want to work with. Some corpora may have specific additional attributes.
         *
         * Basic examples:
         * - word
         * - lc
         * - lemma
         * - lemma_lc
         * - lempos
         * - lempos_lc
         * - tag
         * - pos
         *
         * For list of available word list attributes see **ca/api/corpora/{corpusId}**
         */
        wlattr: string;
        /**
                 * A deny list (formerly known as blacklist) is a list of items that should be excluded from the result. The values should be be separated by a newline symbol (without commas between values). In the URL, the newline symbol is `
                `.
                 */
        wlblacklist?: string;
        /**
         * Defines the allow list (formerly known as whitelist), the list of words which should be included in the list. See also `wlblacklist`.
         */
        wlfile?: string;
        /**
         * Sets the case sensitivity of the corpus, i.e. the data are extracted from a lowercased version of the corpus. It is used for case insensitive analysis. Parameter "1" means case sensitivity, "0" means case insensitivity.
         */
        wlicase?: 1 | 0;
        /**
         * Sets the maximum frequency limit in the wordlist.
         */
        wlmaxfreq?: number;
        /**
         * Sets the number of items to be returned in the API response. It is not limited for user corpora, in preloaded corpora can be some limitation. This parameter is often used with wlpage to help with pagination in frontend development.
         */
        wlmaxitems?: number;
        /**
         * Sets the minimum frequency limit. Items with a lower frequency will not be included.
         */
        wlminfreq?: number;
        /**
         * The type of frequency. The values stand for: `frq` -> absolute or raw frequency, `docf` -> document frequency. `arf` -> average reduced frequency.
         */
        wlnums?: "frq" | "docf" | "arf";
        /**
         * To select page of the response. The number of items on the page is specified by parameter wlmaxitems.
         */
        wlpage?: number;
        /**
         * Sets a regex to filter the results. Relevant only in a simple wordlist.
         */
        wlpat?: string;
        /**
         * Sets the sorting of the results. The default is `frq`, i.e. by absolute frequency. Docf means document frequency.
         */
        wlsort?: "frq" | "docf";
        /**
         * Parameter to set the format of ouput. Is it always set to `simple`, for the `struct_wordlist` is another enpoint called `struct_wordlist`.
         * @deprecated
         */
        wltype?: "simple" | "struct_wordlist";
      };
      res: {
        /**
         * `OK`
         */
        200: _wordlist;
      };
    };
  };
  "/search/struct_wordlist": {
    get: {
      req: {
        /**
         * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
         */
        corpname: string;
        /**
         * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
         */
        format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
        /**
         * Includes, or excludes, nonwords in the in the result. Nonwords are tokens which do not start with letter of the alphabet (e.g. numbers, punctuation). The regex to match the nonwords is `[^[:alpha:]].*`. Certain specialized corpora may use their own specific definition of nonwords.
         */
        includeNonwords?: 1 | 0;
        /**
         * Parameter that represents if the wordlist is created from the first 10 milions lines of corpus. One if yes, no if he wordlist is created from the whole corpus.
         * @deprecated
         */
        random?: 1 | 0;
        /**
         * Calculate the document frequency for each item in the result. Must be used with `addfreqs` set to `docf`.
         */
        reldocf?: 1 | 0;
        /**
         * Includes the relative frequency of each item in the result.
         */
        relfreq?: 1 | 0;
        /**
         * Sets the corpus attribute you want to work with. Some corpora may have specific additional attributes.
         *
         * Basic examples:
         * - word
         * - lc
         * - lemma
         * - lemma_lc
         * - lempos
         * - lempos_lc
         * - tag
         * - pos
         *
         * For list of available word list attributes see **ca/api/corpora/{corpusId}**
         */
        wlattr: string;
        /**
                 * A deny list (formerly known as blacklist) is a list of items that should be excluded from the result. The values should be be separated by a newline symbol (without commas between values). In the URL, the newline symbol is `
                `.
                 */
        wlblacklist?: string;
        /**
         * Sets the case sensitivity of the corpus, i.e. the data are extracted from a lowercased version of the corpus. It is used for case insensitive analysis. Parameter "1" means case sensitivity, "0" means case insensitivity.
         */
        wlicase?: 1 | 0;
        /**
         * Sets the maximum frequency limit in the wordlist.
         */
        wlmaxfreq?: number;
        /**
         * Sets the number of items to be returned in the API response. It is not limited for user corpora, in preloaded corpora can be some limitation. This parameter is often used with wlpage to help with pagination in frontend development.
         */
        wlmaxitems?: number;
        /**
         * Sets the minimum frequency limit. Items with a lower frequency will not be included.
         */
        wlminfreq?: number;
        /**
         * The type of frequency. The values stand for: `frq` -> absolute or raw frequency, `docf` -> document frequency. `arf` -> average reduced frequency.
         */
        wlnums?: "frq" | "docf" | "arf";
        /**
         * To select page of the response. The number of items on the page is specified by parameter wlmaxitems.
         */
        wlpage?: number;
        /**
         * Sets a regex to filter the results. Relevant only in a simple wordlist.
         */
        wlpat?: string;
        /**
         * Sets the sorting of the results. The default is `frq`, i.e. by absolute frequency. Docf means document frequency.
         */
        wlsort?: "frq" | "docf";
        /**
         * Sets the attributes used for generating the wordlist. Up to 3 attributes are allowed (see wlstruct_attr2 and wlstruct_attr3). Some corpora may contain additional specific attributes.
         */
        wlstructAttr1: "word" | "lemma" | "tag" | "lempos";
        /**
         * Additional optional attribute that should be included in the result.
         */
        wlstructAttr2?: "word" | "lemma" | "tag" | "lempos";
        /**
         * Additional optional attribute that should be included in the result.
         */
        wlstructAttr3?: "word" | "lemma" | "tag" | "lempos";
        /**
         * Parameter to set the format of ouput. Is it always set to `simple`, for the `struct_wordlist` is another enpoint called `struct_wordlist`.
         * @deprecated
         */
        wltype?: "simple" | "struct_wordlist";
      };
      res: {
        /**
         * `OK`
         */
        200: _struct_wordlist;
      };
    };
  };
  "/search/concordance": {
    get: {
      req: {
        /**
         * Switches the asynchronous processing on/off. ON = partial results are returned as soon as the first page is filled with results. OFF = results are returned only after the search is completed. Normally, ON is used in the web interface and OFF when using the API.
         */
        asyn?: 0 | 1;
        /**
         * Determines which tokens will be returned with additional attributes defined in `attrs`. `kw` will add the attributes to the KWIC only. `all` will return them with all tokens.
         */
        attrAllpos?: "kw" | "all";
        /**
         * A list of comma-delimited attributes that are returned together with each token. Other examples are:`word, lc, lemma, tag` etc..
         */
        attrs?: string;
        /**
         * Only works when `queryselector` is set to `charrow`. Type the characters that the tokens should contain. Regex is supported.
         */
        concordanceQueryChar?: string;
        /**
         * Only works when `queryselector` is set to `cqlrow`. Type the query using the [cql](https://www.sketchengine.eu/documentation/corpus-querying/) query language.
         */
        concordanceQueryCql?: string;
        /**
         * Only works when `queryselector` is set to `iqueryrow`. Type a word or phrase.
         *
         * These special wildcards are supported .
         *
         * Use the `asterisk (*)` for any number of unspecified characters. Use a `question mark (?)` for exactly one unspecified character. Use the `pipe (|)` to include more than one word or phrase. Use `two hyphens (--)` to find a word which is  hyphenated, non-hyphenated or spelt as two separate words.
         */
        concordanceQueryIquery?: string;
        /**
         * Only works when `queryselector` is set to `lemmarow`. Type the lemma. Regex is supported.
         */
        concordanceQueryLemma?: string;
        /**
         * Only works when `queryselector` is set to `phraserow`. Type the phrase. Regex is supported.
         */
        concordanceQueryPhrase?: string;
        /**
         * The query type. You can send it directly or via the `json` parameter, the results are the same.
         */
        concordanceQueryQueryselector?:
          | "iquery"
          | "cqlrow"
          | "lemmarow"
          | "charrow"
          | "wordrow"
          | "phraserow";
        /**
         * Only works when `queryselector` is set to `wordrow`. Type the word form. Regex is supported.
         */
        concordanceQueryWord?: string;
        /**
         * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
         */
        corpname: string;
        /**
         * (Only for error-annotated corpora.) A correction token to search.
         */
        cupCorr?: string;
        /**
         * (Only for error-annotated corpora.) An error token to search.
         */
        cupErr?: string;
        /**
         * (Only for error-annotated corpora). Determines which error type to higlight. An example of such a corpus is `preloaded/enwiki_error_sample_sentences`.
         */
        cupErrCode?:
          | ".*"
          | "lexicosemantic"
          | "punct"
          | "spelling"
          | "style"
          | "typographical"
          | "unclassified";
        /**
         * Only used with error-annotated corpora. It determines what should be highlighted. It is set to 'q' for corpora without error annotation. Meaning of individual options:
         *
         * - `q` -> to higlight query result.
         *
         * - `e` -> to higlight errors.
         *
         * - `c` -> to highlight corrections.
         *
         * - `b` -> to highlight both erros and corrections.
         *
         * Example of such a corpus can be `preloaded/enwiki_error_sample_sentences`.
         */
        cupHl?: "q" | "e" | "c" | "b";
        /**
         * The attribute applied to tokens in the query which do not have an attribute specified explicitly as part of the query.
         */
        defaultAttr?: string;
        /**
         * (Only for error-annotated corpora.) Determines what should be highlighted. Corr means **correction** and err means **error**. An **example** of such a corpus is `preloaded/enwiki_error_sample_sentences`.
         */
        errcorrSwitch?: "corr" | "err";
        /**
         * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
         */
        format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
        /**
         * The number of the page that should be returned.
         */
        fromp?: number;
        /**
         * An optinal way of **wraping parameters**. It is possible to send all parametres via this parameter only.
         *
         * The most frequent uses are:
         *
         * `queryselector`: To select the query type. Supported options are: **cqlrow**, **iqueryrow**, **lemmarow**, **charrow**, **phraserow**, **wordrow**. For more information see concordance_query parameters.
         *
         * `iquery`: Use with `iqueryrow`.
         *
         * `cql`: Use with `cqlrow`.
         *
         * `lemma`: Use with `lemmarow`.
         *
         * `lpos`: The part of speech of the lemma.
         *
         * `qmcase`: Sets the attribute to its lowercased version, i.e. the data are extracted from a lowercased version of the corpus. It used for case insensitive analysis. 1 = case sensitive), 0 = lowercased corpus/case insensitive).
         *
         *
         * `char`: Use with charrow.
         *
         * `phrase`: Use with phraserow.
         *
         * `word`: Use with wordrow
         *
         *
         * `name`:
         *
         * `pnfilter`:
         *
         * `inclkwic`:
         *
         * `filfpos`:
         *
         * `filtpos`:
         *
         * `desc`:
         *
         * `q`:
         *
         *
         */
        json?: {
          [key: string]: unknown;
        };
        /**
         * The size of the left context in KWIC view. Number of tokens.
         */
        kwicleftctx?: string;
        /**
         * The size of the right context in KWIC view. Number of tokens.
         */
        kwicrightctx?: string;
        /**
         * The part of speech of the lemma. The concrete values depend on the corpus. If the corpus contains the `lempos` attribute and `lpos` is empty, the result defaults to the most frequent part of speech of the lemma.
         */
        lpos?: "-n" | "-v" | "-j" | "-a" | "-d" | "-i";
        /**
         * The number of lines in the concordance.
         */
        pagesize?: number;
        /**
         * The CQL query. Regexes are supported for `lemma`, `phrase` and `word` types. The `iquery` supports simplified wildcards (see concordance_query[iquery]). If you decide to use the concordance_query in a json parameter, you do not have to use this parameter.
         */
        q?: string;
        /**
         * The text type (metadata) for which statistics should be calculated from the concordance. The default is `bncdoc.alltyp` (all available text types are included). Text types (attributes and there values) differ between corpora). You can find them in the response of `corpus_info` method in `freqttattrs` or `subcorpattrs` keys. Not all of them have attributes to show.
         */
        refs?: string;
        /**
         * A list of comma-delimited structures (=structure tags) that should be included in the result.
         */
        structs?: string;
        /**
         * The name of the `subcorpus`. The `default` value `empty string` refers to the entire corpus. An example for `preloaded/bnc2_tt21` can be `Written Academic` or `1960-1974`.
         */
        usesubcorp?: string;
        /**
         * Switches between sentence view and the KWIC view. `sen` returns complete sentences without trimming them. `kwic` returns the KWIC view with the query in the centre and some context left and right.
         */
        viewmode?: "sen" | "kwic";
      };
      res: {
        /**
         * `OK`
         */
        200: _concordance;
      };
    };
  };
  "/search/fullref": {
    get: {
      req: {
        /**
         * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
         */
        corpname: string;
        /**
         * The position of the first token of KWIC in the corpus.
         */
        pos?: number;
      };
      res: {
        /**
         * `OK`
         */
        200: _fullref;
      };
    };
  };
  "/search/widectx": {
    get: {
      req: {
        /**
         * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
         */
        corpname: string;
        /**
         * Size of the left context in tokens.
         */
        detailLeftCtx?: number;
        /**
         * Size of the right context in tokens.
         */
        detailRightCtx?: number;
        /**
         * Only used by the web interface. Indicates the number of tokens that should be highlighted in red.
         */
        hitlen?: number;
        /**
         * The position of the first token of KWIC in the corpus.
         */
        pos?: number;
        /**
         * A list of comma-delimited structures (=structure tags) that should be included in the result.
         */
        structs?: string;
      };
      res: {
        /**
         * `OK`
         */
        200: _widectx;
      };
    };
  };
  "/search/freqml": {
    get: {
      req: {
        /**
         * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
         */
        corpname: string;
        /**
         * The number of items in one response.
         */
        fmaxitems?: number;
        /**
         * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
         */
        format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
        /**
         * The number of the response batch (page). The number of items in each batch is specified by `fmaxitems`.
         */
        fpage?: number;
        /**
         * The number of attributes for which the frequencies should be counted.
         */
        freqlevel?: 1 | 2 | 3 | 4 | 5 | 6;
        /**
         * The identifier of the sorted column. Use `frq` (default) to sort by frequency.
         */
        freqSort?: "frq" | "rel";
        /**
         * If there are more attributes (e.g. m1attr, m2attr), the results can be grouped by the first column/attribute.
         */
        group?: 1 | 0;
        /**
         * An optinal way of **wraping parameters**. It is possible to send all parametres via this parameter only.
         *
         * The most frequent uses are:
         *
         * `queryselector`: To select the query type. Supported options are: **cqlrow**, **iqueryrow**, **lemmarow**, **charrow**, **phraserow**, **wordrow**. For more information see concordance_query parameters.
         *
         * `iquery`: Use with `iqueryrow`.
         *
         * `cql`: Use with `cqlrow`.
         *
         * `lemma`: Use with `lemmarow`.
         *
         * `lpos`: The part of speech of the lemma.
         *
         * `qmcase`: Sets the attribute to its lowercased version, i.e. the data are extracted from a lowercased version of the corpus. It used for case insensitive analysis. 1 = case sensitive), 0 = lowercased corpus/case insensitive).
         *
         *
         * `char`: Use with charrow.
         *
         * `phrase`: Use with phraserow.
         *
         * `word`: Use with wordrow
         *
         *
         * `name`:
         *
         * `pnfilter`:
         *
         * `inclkwic`:
         *
         * `filfpos`:
         *
         * `filtpos`:
         *
         * `desc`:
         *
         * `q`:
         *
         *
         */
        json?: {
          [key: string]: unknown;
        };
        /**
         * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/) or structure attributes (metadata/text types) of any token in the concordance.\n\n A maximum of 6 attributes is allowed (e.g. ml2attr, ml3attr). At least one attribute is required.
         */
        ml1Attr: string;
        /**
         *  Position of the selected attribute in the concordance. **Minus** means **left** context (-1<0). **Plus** means **right** context (6>0). **At least one attribute is required, the others are optional.** Every attribute (ml1attr, ml2attr. etc.) needs it's **own** context position (e.g. if 3 attributes are selected three context position **needs to** be set ml1ctx, ml2ctx, ml3ctx).
         *
         *
         * **Positions can be referenced as follows:**
         *
         * `integer number` - where **0** is the first token in **KWIC**, **-1** the rightmost token in the left context etc.
         *
         * `1:x` - where **x** is one of the corpus structures (e.g. “doc” or “s” if the corpus has the particular markup). Its meaning is the first token in the structure, except when it is the right boundary of a range - then it is the last token in the structure. Also, other numbers can be used, e.g. -2:x, 3:x, etc. (-1 is the same as 1 with meaning “structure containing KWIC”)
         *
         * `a<0` - where **a** stands for a position reference as described in the first two points with meaning '**a** positions before/after the firs KWIC position' (so this is equivalent to **a**)
         *
         * `a>0` - where **a** stands for the same position reference with meaning 'positions before/after the last KWIC position'
         *
         * in the previous two points, if **0** is substituted with a natural number **k**, it means 'before/after **k**-th collocation' instead of 'before/after KWIC'. Collocations are special token groups in the context, that can be added using positive filters (see below).
         *
         *
         * `Ranges` can be referenced as a~b where **a**, **b** stand for token identifiers as above. Examples of positions and ranges:
         *
         * `-1<0` - rightmost token in the left context
         *
         * `3>0` - third token in right context
         *
         * `0>0` - last token in KWIC
         *
         * `0<0` - first token in KWIC
         *
         * `0<0~0>0` - range of KWIC
         *
         * `-1<0~1>0` - range of KWIC with one token from the left context and one from the right context
         *
         * `1:s` - first token in the sentence containing KWIC (or its first token)
         *
         * `1:s>0` - first token in the sentence containing KWIC (or its last token)
         *
         * `0<1` - first token of the first-added collocation.
         *
         *
         * `Examples:`
         *
         * sword/ **1>0~3>0**
         *
         * sword/ **1>0~3>0**
         *
         * slemma/ **0<0~0>0**
         *
         * sword/i **-1**
         *
         * sword/ **0** word/ir **-1<0** tag/r **-2<0**
         *
         *
         */
        ml1Ctx: string;
        /**
         * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/). Just like `ml1attr` but optional.
         */
        ml2Attr?: string;
        /**
         * Position of the selected attribute in the concordance. Minus means left context (-1<0). Plus means right context (6>0). Just like ml1ctx but optional.
         */
        ml2Ctx?: string;
        /**
         * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/). Just like `ml1attr` but optional.
         */
        ml3Attr?: string;
        /**
         * Position of the selected attribute in the concordance. Minus means left context (-1<0). Plus means right context (6>0). Just like ml1ctx but optional.
         */
        ml3Ctx?: string;
        /**
         * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/). Just like `ml1attr` but optional.
         */
        ml4Attr?: string;
        /**
         * Position of the selected attribute in the concordance. Minus means left context (-1<0). Plus means right context (6>0). Just like ml1ctx but optional.
         */
        ml4Ctx?: string;
        /**
         * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/). Just like `ml1attr` but optional.
         */
        ml5Attr?: string;
        /**
         * Position of the selected attribute in the concordance. Minus means left context (-1<0). Plus means right context (6>0). Just like ml1ctx but optional.
         */
        ml5Ctx?: string;
        /**
         * Used to count the frequency of positional attributes [attributes](https://www.sketchengine.eu/my_keywords/positional-attribute/). Just like `ml1attr` but optional.
         */
        ml6Attr?: string;
        /**
         * Position of the selected attribute in the concordance. Minus means left context (-1<0). Plus means right context (6>0). Just like ml1ctx but optional.
         */
        ml6Ctx?: string;
        /**
         * The CQL query. Regexes are supported for `lemma`, `phrase` and `word` types. The `iquery` supports simplified wildcards (see concordance_query[iquery]). If you decide to use the concordance_query in a json parameter, you do not have to use this parameter.
         */
        q?: string;
        /**
         * Includes the percentage of the concordance in the result.
         */
        showpoc?: 1 | 0;
        /**
         * Includes the relative frequency in the result.
         */
        showrel?: 1 | 0;
        /**
         * Includes relative in text types value in the result.
         */
        showreltt?: 1 | 0;
        /**
         * The name of the `subcorpus`. The `default` value `empty string` refers to the entire corpus. An example for `preloaded/bnc2_tt21` can be `Written Academic` or `1960-1974`.
         */
        usesubcorp?: string;
      };
      res: {
        /**
         * `OK`
         */
        200: _freqml;
      };
    };
  };
  "/search/freq_distrib": {
    get: {
      req: {
        /**
         * Determines which tokens will be returned with additional attributes defined in `attrs`. `kw` will add the attributes to the KWIC only. `all` will return them with all tokens.
         */
        attrAllpos?: "kw" | "all";
        /**
         * A list of comma-delimited attributes that are returned together with each token. Other examples are:`word, lc, lemma, tag` etc..
         */
        attrs?: string;
        /**
         * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
         */
        corpname: string;
        /**
         * The attribute applied to tokens in the query which do not have an attribute specified explicitly as part of the query.
         */
        defaultAttr?: string;
        fcLemwordType?: string;
        fcLemwordWindowType?: string;
        fcLemwordWsize?: number;
        fcPosType?: string;
        fcPosWindowType?: number;
        fcPosWsize?: number;
        /**
         * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
         */
        format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
        /**
         * An optinal way of **wraping parameters**. It is possible to send all parametres via this parameter only.
         *
         * The most frequent uses are:
         *
         * `queryselector`: To select the query type. Supported options are: **cqlrow**, **iqueryrow**, **lemmarow**, **charrow**, **phraserow**, **wordrow**. For more information see concordance_query parameters.
         *
         * `iquery`: Use with `iqueryrow`.
         *
         * `cql`: Use with `cqlrow`.
         *
         * `lemma`: Use with `lemmarow`.
         *
         * `lpos`: The part of speech of the lemma.
         *
         * `qmcase`: Sets the attribute to its lowercased version, i.e. the data are extracted from a lowercased version of the corpus. It used for case insensitive analysis. 1 = case sensitive), 0 = lowercased corpus/case insensitive).
         *
         *
         * `char`: Use with charrow.
         *
         * `phrase`: Use with phraserow.
         *
         * `word`: Use with wordrow
         *
         *
         * `name`:
         *
         * `pnfilter`:
         *
         * `inclkwic`:
         *
         * `filfpos`:
         *
         * `filtpos`:
         *
         * `desc`:
         *
         * `q`:
         *
         *
         */
        json?: {
          [key: string]: unknown;
        };
        /**
         * The part of speech of the lemma. The concrete values depend on the corpus. If the corpus contains the `lempos` attribute and `lpos` is empty, the result defaults to the most frequent part of speech of the lemma.
         */
        lpos?: "-n" | "-v" | "-j" | "-a" | "-d" | "-i";
        normalize?: number;
        /**
         * The text type (metadata) for which statistics should be calculated from the concordance. The default is `bncdoc.alltyp` (all available text types are included). Text types (attributes and there values) differ between corpora). You can find them in the response of `corpus_info` method in `freqttattrs` or `subcorpattrs` keys. Not all of them have attributes to show.
         */
        refs?: string;
        res?: number;
        /**
         * A list of comma-delimited structures (=structure tags) that should be included in the result.
         */
        structs?: string;
        /**
         * Switches between sentence view and the KWIC view. `sen` returns complete sentences without trimming them. `kwic` returns the KWIC view with the query in the centre and some context left and right.
         */
        viewmode?: "sen" | "kwic";
      };
      res: {
        /**
         * `OK`
         */
        200: _freq_distrib;
      };
    };
  };
  "/search/freqdist": {
    get: {
      req: {
        /**
         * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
         */
        corpname: string;
        /**
         *  **Minus** means **left** context (-1<0). **Plus** means **right** context (6>0).
         *
         *
         * **Positions can be referenced as follows:**
         *
         * `integer number` - where **0** is the first token, **-1** the rightmost token in the left context etc.
         *
         * `1:x` - where **x** is one of the corpus structures (e.g. “doc” or “s” if the corpus has the particular markup). Its meaning is the first token in the structure, except when it is the right boundary of a range - then it is the last token in the structure. Also, other numbers can be used, e.g. -2:x, 3:x, etc. (-1 is the same as 1 with meaning “structure containing searched word”)
         *
         * `a<0` - where **a** stands for a position reference as described in the first two points with meaning '**a** positions before/after the first searched word position' (so this is equivalent to **a**)
         *
         * `a>0` - where **a** stands for the same position reference with meaning 'positions before/after the last searched word position'
         *
         * in the previous two points, if **0** is substituted with a natural number **k**, it means 'before/after **k**-th collocation' instead of 'before/after KWIC'. Collocations are special token groups in the context, that can be added using positive filters (see below).
         *
         *
         * `Ranges` can be referenced as a~b where **a**, **b** stand for token identifiers as above. Examples of positions and ranges:
         *
         * `-1<0` - rightmost token in the left context
         *
         * `3>0` - third token in right context
         *
         * `0>0` - last token
         *
         * `0<0` - first token
         *
         * `0<0~0>0` - range
         *
         * `-1<0~1>0` - range with one token from the left context and one from the right context
         *
         * `1:s` - first token in the sentence containing searched word (or its first token)
         *
         * `1:s>0` - first token in the sentence containing searched word (or its last token)
         *
         * `0<1` - first token of the first-added collocation.
         *
         *
         * `Examples:`
         *
         * sword/ **1>0~3>0**
         *
         * sword/ **1>0~3>0**
         *
         * slemma/ **0<0~0>0**
         *
         * sword/i **-1**
         *
         * sword/ **0**
         *
         * word/ir **-1<0**
         *
         * tag/r **-2<0**
         *
         *
         */
        ctx?: string;
        /**
         * A diachronic attribute to be selected. Available attributes **can differ** in corpora. Examples can be **doc.year** or **doc.month**.
         */
        diaattr: string;
        /**
         * An optinal way of **wraping parameters**. It is possible to send all relevant parametres via this parameter only. It is classic JSON format.
         *
         * `wordlist`: words for which the relative frequency should be counted.
         *
         *
         */
        json?: {
          [key: string]: unknown;
        };
        /**
         * `1`: display results during calculation
         * `0`: display results after all data has been calculated (can take quite a lot of time).
         */
        sse: "1" | "0";
        /**
         * Determines which periods are included in the results. It signifies the percentage above the average size, acting as a **limit**. When relative frequency (rel_frq) surpasses this limit, it is discarded (moved to **removed_freqdist** object).
         */
        threshold: string;
        /**
         * Sets the corpus attribute you want to work with. Some corpora may have specific additional attributes.
         *
         * Basic examples:
         * - word
         * - lc
         * - lemma
         * - lemma_lc
         * - lempos
         * - lempos_lc
         * - tag
         * - pos
         *
         * For list of available word list attributes see **ca/api/corpora/{corpusId}**
         */
        wlattr: string;
        /**
         * A wordlist of words for which the relative frequency should be counted. No exact example is here because it is already set in `JSON` parameter.\Example: ['the','a','lion'].
         */
        wordlist?: string;
      };
      res: {
        /**
         * `OK`
         */
        200: _freqdist;
      };
    };
  };
  "/search/collx": {
    get: {
      req: {
        /**
         * The (positional attribute)[https://www.sketchengine.eu/my_keywords/positional-attribute/] (lemma, word form etc.) used in the computation.
         */
        cattr?: string;
        /**
         * Defines the types of statistics (association measures) to be computed.
         *
         * `t` -> T-score
         *
         * `m` -> MI
         *
         * `3` -> MI3
         *
         * `l` -> log likelihood
         *
         * `s` -> min. sensitivity
         *
         * `p` -> MI.log_f
         *
         * `r` -> relative freq.
         *
         * `f` -> absolute freq.
         *
         * `d` -> logDice.
         *
         * To send one value just type the value for example `t`. If you need to send more values write it as `["t","m","d","3","l","s","p"]`.
         */
        cbgrfns?: string;
        /**
         * The left boundary of the window in which the collocations should be identified. Defined by the token position left or right of KWIC.
         */
        cfromw?: number;
        /**
         * Sets the maximum number of items in the response.
         */
        cmaxitems?: number;
        /**
         * The minimum frequency of the token in the window defined by `cfromw` and `ctow`.
         */
        cminbgr?: number;
        /**
         * The minimum frequency of the token in the corpus.
         */
        cminfreq?: number;
        /**
         * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
         */
        corpname: string;
        /**
         * Function according to which the result is sorted.
         */
        csortfn?: "t" | "m" | "3" | "l" | "s" | "p" | "r" | "f" | "d";
        /**
         * The right boundary of the window in which the collocations should be identified. Defined by the token position left or right of KWIC.
         */
        ctow?: number;
        /**
         * An optinal way of **wraping parameters**. It is possible to send all relevant parametres via this parameter only. It is classic JSON format.
         *
         * The most frequent uses are:
         *
         * `queryselector`: To select the query type. Supported options are: **cqlrow**, **iqueryrow**, **lemmarow**, **charrow**, **phraserow**, **wordrow**.
         *
         * `iquery`: Use with `iqueryrow`.
         *
         * `cql`: Use with `cqlrow`.
         *
         * `lemma`: Use with `lemmarow`.
         *
         * `lpos`: The part of speech of the lemma.
         *
         * `qmcase`: Sets the attribute to its lowercased version, i.e. the data are extracted from a lowercased version of the corpus. It used for case insensitive analysis. 1 = case sensitive), 0 = lowercased corpus/case insensitive).
         *
         * `char`: Use with charrow.
         *
         * `phrase`: Use with phraserow.
         *
         * `word`: Use with wordrow
         *
         * `cbgrfns`: If you need to send more cbgrfnsDefines the types of statistics (association measures) to be computed. See example.
         */
        json?: {
          [key: string]: unknown;
        };
        /**
         * The CQL query. Regexes are supported for `lemma`, `phrase` and `word` types. The `iquery` supports simplified wildcards (see concordance_query[iquery]). If you decide to use the concordance_query in a json parameter, you do not have to use this parameter.
         */
        q?: string;
        /**
         * The name of the `subcorpus`. The `default` value `empty string` refers to the entire corpus. An example for `preloaded/bnc2_tt21` can be `Written Academic` or `1960-1974`.
         */
        usesubcorp?: string;
      };
      res: {
        /**
         * `OK`
         */
        200: _collx;
      };
    };
  };
  "/search/subcorp": {
    get: {
      req: {
        /**
         * Set to `1` if the corpus should be deleted. Only user subcorpora can be deleted. Nothing will be deleted if left empty (default value == 0).
         */
        _delete?: 0 | 1;
        /**
         * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
         */
        corpname: string;
        /**
         * Set to `1` if new subcorpus should be created. Subcorpus will not be created if left empty (default value == 0).
         */
        create?: 0 | 1;
        /**
         * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
         */
        format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
        /**
         * Used to specify text types for a subcorpus. Takes a JSON object as input, where the key-value pairs define the specific attributes. The attributes can vary depending on the corpus.
         *
         * When using the json parameter in a query, you can define a JSON object with one or more attributes. Each attribute can have a single value or an array of values.
         *
         *
         * The list of available text types for specific corpus can be obtained via `/search/corp_info` just add `sca_` in front the name (see examples).
         *
         *
         * `Examples:`
         *
         *
         * - To create a subcorpus based on a specific spoken text type from the BNC corpus:
         *
         * **{"sca_bncdoc.alltyp":["Spoken context-governed"]}**
         *
         *
         * - To filter texts from the BNC corpus that are both spoken context-governed and spoken demographic:
         *
         * **{"sca_bncdoc.alltyp":["Spoken context-governed","Spoken demographic"]}**
         *
         *
         * - To select texts from the BNC corpus from a specific time period (1960-1974):
         *
         * **{"sca_bncdoc.alltim":["1960-1974"]}**
         *
         *
         * - To create a subcorpus with texts from specific authors and time periods, along with regional specifications:
         *
         * **{"sca_bncdoc.author": ["Author1","Author2",...],"sca_bncdoc.alltim": ["1985-1993","1975-1984"], "sca_bncdoc.wripp": ["UK (unspecific)","Ireland"]}**
         *
         *
         * - To filter texts from the Ententen corpus based on domain and topic:
         *
         * **{"sca_doc.tld":["org","com"], "sca_doc.topic": ["arts","beauty & fashion","cars & bikes","culture & entertainment"]}**
         *
         *
         * - For a user-specific corpus, filtering based on document ID and filename:
         *
         * **{"sca_doc.id":["file29173711"],"sca_doc.filename":["Filename.pdf"]}**
         */
        json?: {
          [key: string]: unknown;
        };
        /**
         * Query for creating subcorpora from concordance.
         *
         * The search criteria are specified within brackets following a prefix like `alemma` or `aword`. This prefix often indicates the type of linguistic search (e.g., lemma-based, word-based). The criteria within the brackets can include checks for specific words, lemmas, parts of speech and more, using operators like | (OR), & (AND), and regular expressions.
         *
         * The lists of available attributes, pos tags for specific corpus can be obtained via `/search/corp_info`.
         *
         *
         * `Examples:`
         *
         *
         * - Simple word or lemma search in the BNC corpus:
         *
         * **q=alemma,[lc="test" | lemma_lc="test"]**
         *
         *
         * - Search for nouns with the lemma 'test' in a case-sensitive manner:
         *
         * **q=alemma,[lempos_lc="(test)-n"]**
         *
         *
         * - Search for verbs with the lemma 'test', case-insensitive:
         *
         * **q=alemma,[lempos="(test)-v"]**
         *
         *
         * - Searching for a specific phrase 'test' in a case-sensitive manner:
         *
         * **q=aword,[word="test"]**
         *
         *
         * - Searching for the numeral '1955':
         *
         * **q=alemma,[word="1955" & tag="CD"]**
         *
         *
         * - Regex-based search for words containing the character 'h':
         *
         * **q=alemma,[word=".\*h.\*"]**
         *
         *
         * - Complex search involving the lemma 'book' followed by up to three words, then a verb:
         *
         * **q=alemma,[lemma="book"][]{1,3}[tag="V.\*"]**
         */
        q?: string;
        /**
         * Which corpus structure should be used in new subcorpus. Used withim concordance type of subcorpus. The lists of structures can be obtained via `/search/corp_info`.
         */
        struct?: string;
        /**
         * Name of the subcorpus.
         */
        subcname: string;
      };
      res: {
        /**
         * `OK`
         */
        200: _subcorp;
      };
    };
  };
  "/search/subcorpus_rename": {
    get: {
      req: {
        /**
         * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
         */
        corpname: string;
        /**
         * A new name for the subcorpus.
         */
        newSubcorpName: string;
        /**
         * The name of subcorpus you want to rename.
         */
        subcorpId: string;
      };
      res: {
        /**
         * `OK`
         */
        200: _subcorpus_rename;
      };
    };
  };
  "/search/subcorp_info": {
    get: {
      req: {
        /**
         * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
         */
        corpname: string;
        /**
         * Name of the subcorpus.
         */
        subcname: string;
      };
      res: {
        /**
         * `OK`
         */
        200: _subcorp_info;
      };
    };
  };
  "/search/extract_keywords": {
    get: {
      req: {
        /**
         * Represent what kind of frequnecy should be calculated. When used with `reldocf` it is set to `docf` to calculate document frequency.
         */
        addfreqs?: string;
        /**
         * Limits the results to items containing only alphanumeric characters.
         */
        alnum?: 1 | 0;
        /**
         * Switches between the computation of keywords, terms, key n-grams and key collocations. With keywords and n-grams, it also sets the attribute to be used for the computation.
         *
         * For keywords, set to the required attribute, usually `lc`, `word` or `lemma`.
         *
         * For n-grams, set to the required attribute and set `usengrams`, `ngrams_n` and `ngrams_max_n`.
         *
         * For terms, set the attribute to `TERM`.
         *
         * For collocations (word sketch triples, equivalent of using the Word Sketch with AS A LIST option in the web interace), set the attribute to `WSCOLLOC`. Consider using `wlpat`.
         */
        attr?: "lemma" | "word" | "TERM" | "WSCOLLOC";
        /**
         * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
         */
        corpname: string;
        /**
         * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
         */
        format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
        /**
         * Switches to the `lc` attribute, i.e. the lower-cased version of the corpus to allow case insensitive searching. `1` means that case sensitivity is off, `0` means it is on.
         */
        icase?: 1 | 0;
        /**
         * Includes, or excludes, nonwords in the in the result. Nonwords are tokens which do not start with letter of the alphabet (e.g. numbers, punctuation). The regex to match the nonwords is `[^[:alpha:]].*`. Certain specialized corpora may use their own specific definition of nonwords.
         */
        includeNonwords?: 1 | 0;
        /**
         * Sets the maximum frequency of the item.
         */
        maxfreq?: number;
        maxKeywords?: number;
        /**
         * Sets the minimum frequency of the item.
         */
        minfreq?: string;
        /**
         * The maximum n-gram length. The maximum is `6`.
         */
        ngramsMaxN?: 2 | 3 | 4 | 5 | 6;
        /**
         * The minimum n-gram length. Usually used with `ngrams_max_n` and `usengrams`.
         */
        ngramsN?: 2 | 3 | 4 | 5;
        /**
         * Limits the results to items containing at least one alphanumberic character. Words such as 16-year-old or 3D will be included.
         */
        onealpha?: 1 | 0;
        /**
         * Corpus name of the reference corpus, it must have the same processing (the same attributes, the same term grammar).
         */
        refCorpname: string;
        /**
         * Calculate the document frequency for each item in the result. Must be used with `addfreqs` set to `docf`.
         */
        reldocf?: 1 | 0;
        /**
         * The smoothing parameter for (simple maths) [https://www.sketchengine.eu/documentation/simple-maths/].
         */
        simpleN?: "1" | "0";
        /**
         * Represent if n-grams should be extracted or just simple keywords.
         */
        usengrams?: 0 | 1;
        /**
         * The name of the `subcorpus`. The `default` value `empty string` refers to the entire corpus. An example for `preloaded/bnc2_tt21` can be `Written Academic` or `1960-1974`.
         */
        usesubcorp?: string;
        /**
                 * A deny list (formerly known as blacklist) is a list of items that should be excluded from the result. The values should be be separated by a newline symbol (without commas between values). In the URL, the newline symbol is `
                `.
                 */
        wlblacklist?: string;
        /**
         * Defines the allow list (formerly known as whitelist), the list of words which should be included in the list. See also `wlblacklist`.
         */
        wlfile?: string;
        /**
         * Sets a regex to filter the results. Relevant only in a simple wordlist.
         */
        wlpat?: string;
      };
      res: {
        /**
         * `OK`
         */
        200: _extract_keywords;
      };
    };
  };
  "/search/textypes_with_norms": {
    get: {
      req: {
        /**
         * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
         */
        corpname: string;
      };
      res: {
        /**
         * `OK`
         */
        200: _textypes_with_norms;
      };
    };
  };
  "/search/attr_vals": {
    get: {
      req: {
        /**
         * Selects a structure attribute (reffered to as text type in the web interface). Corpora have different numbers structure attributes and their values. You can find them in the response of the `corpus_info` method in `freqttattrs` or `subcorpattrs` keys. Not all of them have attributes to show.
         */
        avattr: string;
        /**
         * The starting index from which to return the results.
         */
        avfrom?: number;
        /**
         * The number of items to return.
         */
        avmaxitems?: number;
        /**
         * A regex to filter the results. Empty string defaults to `.*` (match everything).
         */
        avpat?: string;
        /**
         * Corpus name. To query your own corpus (e.g. username john, corpus mycorpus), `use` value `user/john/mycorpus`.
         */
        corpname: string;
        /**
         * The `format` of the output. `Empty value` is interpreted as `JSON`. Not every endpoint supports all formats.
         */
        format?: "json" | "xml" | "csv" | "tsv" | "txt" | "xls";
        /**
         * Switches to the `lc` attribute, i.e. the lower-cased version of the corpus to allow case insensitive searching. `1` means that case sensitivity is off, `0` means it is on.
         */
        icase?: 1 | 0;
      };
      res: {
        /**
         * `OK`
         */
        200: _attr_vals;
      };
    };
  };
  "/ca/api/corpora": {
    get: {
      res: {
        /**
         * `OK`
         */
        200: {
          data?: Array<_corpora_list>;
        };
      };
    };
    post: {
      req: {
        /**
         * Set the language, corpus name and corpus description.
         *
         * - `info` => The additional information for a newly created corpus. (string)
         *
         * - `language_id` => Language iso-code. `ISO 639-1`. (string)
         *
         * - `name` => Unique `corpus name` for a newly created corpus. (string)
         */
        requestBody: _corpora_request;
      };
      res: {
        /**
         * `Created`
         */
        201: {
          data?: _corpora_single;
        };
      };
    };
  };
  "/ca/api/corpora/{corpusId}": {
    get: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
      };
      res: {
        /**
         * `OK`
         */
        200: _corpora_single;
        /**
         * `Forbidden`
         */
        403: _forbidden_normal;
        /**
         * `Not Found`
         */
        404: _not_found_404;
      };
    };
    put: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         *  - `expert_mode` => Set to **True** if you are hard-core. (boolean)
         *
         * - `name` => Corpus name. **Given by user**. (string)
         *
         * - `info` => Additional info about corpus. (string)
         *
         * - `document_order` => Can be set to enforce document order within the corpus. (list of integers)
         *
         * - `structures` => Available structures or tags in the corpus. Structures like **s** (sentence), **g** (glue), **doc** (document).(list)
         *
         * - `name` => Structure name. Example: **s**. (string)
         *
         * - `attributes` => A list of used attributes in corpus. (list)
         *
         * - `name` => The name of used attribute. (string)
         *
         * - `file_structure` => The structure in which individual documents should be wrapped. Usually **doc**. (string)
         *
         * - `onion_structure` => The structure for deduplication. Usually **p** (paragraph) or **Null** (no deduplication). (string)
         *
         * - `docstructure` => Structure in which individual documents should be wrapped. Usually **doc**. (string)
         *
         * - `sketch_grammar_id` => Name of sketch grammar file. For sketch grammars querying. Sketch grammar is a series of rules written in the CQL query  language that search for collocations in a text corpus and categorize them according to their  grammatical relations. Example: **preloaded/english-penn_tt-3.3.wsdef.m4**. (string)
         *
         * - `term_grammar_ir` => Name of term grammar file. Term grammar tells Sketch Engine which words and phrases should indentify as terms. Example: **corpora/wsdef/english-penn_tt-terms-3.1.termdef.m4**. (string)
         */
        requestBody?: _corpus_update;
      };
      res: {
        /**
         * `OK`
         */
        200: {
          data?: _corpora_single;
        };
        /**
         * `Forbidden`
         */
        403: _forbidden_normal;
        /**
         * `Not Found`
         */
        404: _not_found_404;
      };
    };
    delete: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
      };
      res: {
        /**
         * `No Content`
         */
        204: void;
        /**
         * `Forbidden`
         */
        403: _forbidden_normal;
        /**
         * `Not Found`
         */
        404: _not_found_404;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/can_be_compiled": {
    post: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
         */
        requestBody?: _empty_request;
      };
      res: {
        /**
         * `OK`
         */
        200: _can_be_compiled;
        /**
         * `Unauthorized`
         */
        401: _unauthorized;
        /**
         * `Forbidden`
         */
        403: _forbidden;
        /**
         * `Not Found`
         */
        404: _not_found_RPC;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/get_progress": {
    post: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
         */
        requestBody: _empty_request;
      };
      res: {
        /**
         * `OK`
         */
        200: _get_progress;
        /**
         * `Forbidden`
         */
        403: _forbidden;
        /**
         * `Not Found`
         */
        404: _not_found_RPC;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/compile": {
    post: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         *  `Structures` or `structure attributes` in corpus which should be compiled. Usually: `all`. (string)
         */
        requestBody: _compile_request;
      };
      res: {
        /**
         * `OK`
         */
        200: _rpc_style;
        /**
         * `Bad Request`
         */
        400: _bad_request_RPC_9;
        /**
         * `Unauthorized`
         */
        401: _unauthorized_rpc;
        /**
         * `Forbidden`
         */
        403: _forbidden;
        /**
         * `Not Found`
         */
        404: _not_found_RPC;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/logs/{logName}": {
    get: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * Name of log file. Name 'last.log' show the newest log for that corpus.
         */
        logName: string;
      };
      res: {
        /**
         * `OK`
         */
        200: string;
        /**
         * `Unauthorized`
         */
        401: unknown;
        /**
         * `Forbidden` (you need `read` permission)
         */
        403: unknown;
        /**
         * `Not Found`
         */
        404: unknown;
        /**
         * `Method Not Allowed`
         */
        405: unknown;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/download": {
    get: {
      req: {
        /**
         * Required when you want to download parallel corpora, **when format == tmx.** Specify aligned corpus name.
         */
        aligned?: string;
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * The contents of each file will be enclosed in a XML like structure of the specified name with the filename as its id attribute and the URL (if available) as the url attribute. If empty document boundaries will be lost. Example: `doc`.
         */
        fileStructure?: string;
        /**
         * File format in which the corpus should be downloaded. Just three formats are supported.
         */
        format: string;
      };
      res: {
        /**
         * `OK`
         */
        200: string;
        /**
         * `Bad Request` Examples: `ALIGNED_NOT_FOUND`, `ALIGNED_FORBIDDEN`, `INVALID_FORMAT`.
         */
        400: unknown;
        /**
         * `Unauthorized`
         */
        401: unknown;
        /**
         * `Forbidden`
         */
        403: unknown;
        /**
         * `Not Found`
         */
        404: unknown;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/cancel_job": {
    post: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
         */
        requestBody?: _empty_request;
      };
      res: {
        /**
         * `OK`
         */
        200: _rpc_style;
        /**
         * `Unauthorized`
         */
        401: _unauthorized_rpc;
        /**
         * `Forbidden`
         */
        403: _forbidden;
      };
    };
  };
  "/ca/api/corpora/compile_aligned": {
    post: {
      req: {
        /**
         * List of corpus IDs used in aligned compilation.
         *
         * - `corpus_ids` => A list of **Corpus ID** of multilingual corpora. (integer)
         *
         * - `structures` => Represent if **all** structures should be used during compilation (in that case it should be contain just **all**) or just some of them. (string)
         */
        requestBody?: _corpus_ids;
      };
      res: {
        /**
         * `OK`
         */
        200: _rpc_style;
        /**
         * `Bad Request`
         */
        400: _bad_request_RPC_8;
        /**
         * `Forbidden`
         */
        403: _forbidden;
      };
    };
  };
  "/ca/api/corpora/align": {
    post: {
      req: {
        /**
         * - `alignstruct` => According to which structure the document should be aligned. Usually, **\<s>**. (string)
         *
         * - `auto` => **True**, when documents are not compiled. Sketch Engine will align them automatically. (boolean)
         *
         * - `corpus_ids` => A list of **Corpus ID** of multilingual corpus. (integer)
         */
        requestBody?: _align_req;
      };
      res: {
        /**
         * `OK`
         */
        200: _rpc_style;
        /**
         * `Bad Request`
         */
        400: _bad_request_RPC_1;
        /**
         * `Unauthorized`
         */
        401: _unauthorized_rpc;
        /**
         * `Forbidden`
         */
        403: _forbidden;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/documents": {
    get: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * ID of file subdirectory. **0** stands for default document directory with name `upload`.
         */
        filesetId?: number;
      };
      res: {
        /**
         * `OK`
         */
        200: _documents_get;
        /**
         * `Unauthorized`
         */
        401: _unauthorized;
        /**
         * `Forbidden` (you need `view` permission).
         */
        403: _forbidden_normal;
        /**
         * `Not Found` (HTML response).
         */
        404: unknown;
      };
    };
    post: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * ID of file subdirectory. **0** stands for default document directory with name `upload`.
         */
        filesetId?: number;
        /**
         * File to upload.
         */
        formData?: {
          /**
           * File to upload.
           */
          file?: Blob | File;
        };
        /**
         * Delay tagging by given number of `seconds`.
         */
        waitWithTagging?: number;
      };
      res: {
        /**
         * `Created`
         */
        201: _documents_post;
        /**
         * `Bad Request`
         */
        400: _bad_request_10;
        /**
         * `Unauthorized`
         */
        401: _unauthorized;
        /**
         * `Forbidden` (you need `upload` permission).
         */
        403: _forbidden_normal;
        /**
         * `Not Found` (HTML response).
         */
        404: unknown;
      };
    };
    put: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * ID of file subdirectory. **0** stands for default document directory with name `upload`.
         */
        filesetId?: number;
        /**
         *   - `id` => Unique numeric `document ID`. (integer)
         *
         * - `metadata` => Pairs of `attribute_name`:`value`.
         */
        requestBody?: _doc_metadata;
      };
      res: {
        /**
         * `OK`
         */
        200: _documents_get;
        /**
         * `Bad Request`
         */
        400: _bad_request_11;
        /**
         * `Unauthorized`
         */
        401: _unauthorized;
        /**
         * `Forbidden` (you need `edit` permission).
         */
        403: _forbidden_normal;
        /**
         * `Not Found` (HTML response).
         */
        404: unknown;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/documents/{documentId}": {
    get: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * Document ID. For document querying.
         */
        documentId: number;
      };
      res: {
        /**
         * `OK`
         */
        200: _documents_get;
        /**
         * `Unauthorized`
         */
        401: _unauthorized;
        /**
         * `Forbidden` (you need `view` permission).
         */
        403: _forbidden_normal;
        /**
         * `Not Found`
         */
        404: _not_found_404;
      };
    };
    put: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * Document ID. For document querying.
         */
        documentId: number;
        /**
         *  - `filename_display` => Name of documents. (string)
         *
         * - `id` => Unique numeric **document ID** to identify individual documents. (integer)
         *
         * - `inProgress` => Represents whether the currently edited document is in use. (boolean)
         *
         * - `isArchive` => Represents if the updated document is in a format like .zip (created via some archive manager). (boolean)
         *
         * - `metadata` => Metadata of document. For example, additional attributes and values.
         *
         * - `parameters` => Parameters for plaintext extraction.
         *
         * - `encoding` => Encoding standard of the document. Usually, **UTF-8**. (string)
         *
         * - `justext_stoplist` => Represent the list of unimportant words, in a specified language, from an NLP point of view. (string)
         *
         * - `permutation` => Changing the order of columns (applies only to **type=vert**). (integer)
         *
         * - `tmx_lang` => TMX (translation memory exchange). Language of document used for parallel corpus creation. (string)
         *
         * - `tmx_struct` => Alignment structure to be used for multilingual documents, **align** is the most used structure. Used within segment distinction, which sentence is in which language and to put sentences with the same meaning into one segment. (string)
         *
         * - `tmx_untranslated` => Placeholder for empty segments in multilingual documents. The segments which have no counterpart in a second language of parallel corpus. (string)
         *
         * - `type` => File format (.csv, .doc, .docx, .htm, .html). (string)
         *
         * - `unlegalese` => Convert **all-caps** text to **normal case**. (boolean)
         *
         * - `temporary` => Is document temporary or not. (boolean)
         *
         * - `word_count` => Total number of **words** (tokens minus punctuation etc.) in document. (integer)
         *
         * - `vertical_progress` => Progress of **vertical file** creation. (integer)
         *
         * - `vertical_error` => An error occured while creating the vertical file. If the creation was succesfull the value is **Null**. (string)
         */
        requestBody?: _doc_put_req;
      };
      res: {
        /**
         * `OK`
         */
        200: _documents_get;
        /**
         * `Bad Request`
         */
        400: _bad_request_11;
        /**
         * `Unauthorized`
         */
        401: _unauthorized;
        /**
         * `Forbidden` (you need `edit` permission).
         */
        403: _forbidden_normal;
        /**
         * `Not Found`
         */
        404: _not_found_404;
      };
    };
    delete: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * Document ID. For document querying.
         */
        documentId: number;
      };
      res: {
        /**
         * `No Content`
         */
        204: void;
        /**
         * `Bad Request`
         */
        400: _bad_request_11;
        /**
         * `Unauthorized`
         */
        401: _unauthorized;
        /**
         * `Forbidden` (you need `delete` permission).
         */
        403: _forbidden_normal;
        /**
         * `Not Found`
         */
        404: _not_found_404;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/documents/{documentId}/preview": {
    post: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * Document ID. For document querying.
         */
        documentId: number;
        /**
         *  - `auto_paragraphs` => Automatically insert paragraph breaks (**\<p>**) in place of blank lines. (string)
         *
         * - `encoding` => Encoding standard of the document. Usually **UTF-8**. (string)
         *
         * - `justext_stoplist` => Represent the list of unimportant words, in a specified language, from an NLP point of view. (string)
         *
         * - `permutation` => Changing the order of columns (applies only to **type=vert**).
         *
         * - `tmx_lang` => TMX (translation memory exchange). Language of document used for parallel corpus creation. (string)
         *
         * - `tmx_struct` => Alignment structure to be used for multilingual documents, **align** is the most used structure. Used within segment distinction, which sentence is in which language and to put sentences with the same meaning into one segment. (string)
         *
         * - `tmx_untranslated` => Placeholder for empty segments in multilingual documents. The segments which have no counterpart in a second language of parallel corpus. (string)
         *
         * - `type` => File format (.csv, .doc, .docx, .htm, .html). (string)
         *
         * - `unlegalese` => Convert **all-caps** text to **normal case**. (boolean)
         */
        requestBody?: _doc_preview;
      };
      res: {
        /**
         * `OK`
         */
        200: _doc_preview;
        /**
         * `Forbidden`
         */
        403: _forbidden;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/documents/{documentId}/original": {
    get: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * Document ID. For document querying.
         */
        documentId: number;
      };
      res: {
        /**
         * `OK`
         */
        200: Blob | File;
        /**
         * `Unauthorized`
         */
        401: unknown;
        /**
         * `Forbidden` (you need `view` permission).
         */
        403: unknown;
        /**
         * `Not Found` (Html response).
         */
        404: unknown;
        /**
         * `Method Not Allowed`
         */
        405: unknown;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/documents/{documentId}/plaintext": {
    get: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * Document ID. For document querying.
         */
        documentId: number;
      };
      res: {
        /**
         * `Partial Content`
         */
        206: string;
        /**
         * `Unauthorized`
         */
        401: unknown;
        /**
         * `Forbidden` (you need `view` permission).
         */
        403: unknown;
        /**
         * `Not Found` (Html response).
         */
        404: unknown;
        /**
         * `Method Not Allowed`
         */
        405: unknown;
        /**
         * `Range Not Satisfiable`
         */
        416: unknown;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/documents/{documentId}/vertical": {
    get: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * Document ID. For document querying.
         */
        documentId: number;
      };
      res: {
        /**
         * `Partial Content`
         */
        206: string;
        /**
         * `Unauthorized`
         */
        401: unknown;
        /**
         * `Forbidden`
         */
        403: unknown;
        /**
         * `Not Found` (Html response).
         */
        404: unknown;
        /**
         * `Method Not Allowed`
         */
        405: unknown;
        /**
         * `Range Not Satisfiable`
         */
        416: unknown;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/documents/{documentId}/expand_archive": {
    post: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * Document ID. For document querying.
         */
        documentId: number;
        /**
         *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
         */
        requestBody?: _empty_request;
      };
      res: {
        /**
         * `OK`
         */
        200: _rpc_expand_archive;
        /**
         * `Bad Request`
         */
        400: _bad_request_13;
        /**
         * `Forbidden (you need edit permission)`
         */
        403: _forbidden;
        /**
         * `Not Found` (Html response).
         */
        404: unknown;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/documents/{documentId}/cancel_job": {
    post: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * Document ID. For document querying.
         */
        documentId: number;
        /**
         *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
         */
        requestBody?: _empty_request;
      };
      res: {
        /**
         * `OK`
         */
        200: _rpc_style;
        /**
         * `Forbidden` (you need `edit` permission).
         */
        403: _forbidden;
        /**
         * `Not Found`
         */
        404: _not_found_RPC;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/documents/{documentId}/get_progress": {
    post: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * Document ID. For document querying.
         */
        documentId: number;
        /**
         *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
         */
        requestBody?: _empty_request;
      };
      res: {
        /**
         * `OK`
         */
        200: _get_progress;
        /**
         * `Forbidden` (you need permission to `view` to the specified corpus).
         */
        403: _forbidden;
        /**
         * `Not Found`
         */
        404: _not_found_RPC;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/filesets": {
    get: {
      req: {
        /**
         * Start corpus compiling after web-crawler finishes downloading content from the internet.
         */
        compileWhenFinished?: number;
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
      };
      res: {
        /**
         * `OK`
         */
        200: {
          data?: Array<_fileset>;
        };
        /**
         * `Unauthorized`
         */
        401: _unauthorized;
        /**
         * `Forbidden` (you need permission to `upload` to the specified corpus).
         */
        403: _forbidden_normal;
        /**
         * `Not Found`
         */
        404: _not_found_404;
      };
    };
    post: {
      req: {
        /**
         * Start corpus compiling after web-crawler finishes downloading content from the internet.
         */
        compileWhenFinished?: number;
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * Setting parameters to improve web-crawler accuracy.
         *
         * - `bl_max_total_kw` => **Blacklist max total keyword**. Means that web page or document will be discarded if it contains more words from the denylist (blacklist) than this limit. (integer)
         *
         * - `bl_max_unique_kw` => **Blacklist max unique keyword**. Means that web page or document will be discarded if it contains more unique words from the denylist (blacklist) than this limit. (integer)
         *
         * - `black_list` => A list (separated by whitespaces) of **blocked words**, words you don't want to see in your future corpus. (string)
         *
         * - `input_type` => Input types the web-crawler will works with. Example: **urls**. (string)
         *
         * - `max_cleaned_file_size` => Web pages and documents with a size **over** this limit (**in kB**) will be ignored. (integer)
         *
         * - `max_file_size` => Web pages and documents with a size **over** this limit (**in kB**) will be ignored. (integer)
         *
         * - `min_cleaned_file_size` => Web pages and documents **smaller** than this limit (**in kB**) after cleaning will be ignored. Cleaning involves conversion to plain text, removing boilerplate text (e.g. navigation menus, legal text, disclaimers and other repetitive content). (integer)
         *
         * - `min_file_size` => Web pages and documents with a **size below** this limit (**in kB**) will be ignored. (integer)
         *
         * - `name` => Texts will be organized into a corpus folder with this name. (string)
         *
         * - `seed_word` => A list of words according to which the URLs were chosen to be searched. (string)
         *
         * - `white_list` => A list (separated by whitespaces) of allowed words, words you want to see in your future corpus. (list of string)
         *
         * - `wl_min_kw_ratio` => **Whitelist minimal keywords ratio**. Means that web page or document will be included only if the percentage of allowlist words compared to total words is higher than this limit. (integer)
         *
         * - `wl_min_total_kw` => **Whitelist minimal total keywords**. Means that web page or document will be included only if it contains more words from the allowlist (whitelist) than this limit. (integer)
         *
         * - `wl_min_unique_kw` => **Whitelist minimal unique keywords**. Means that a web page or document will be included only if it contains more words from the allowlist (whitelist) than this limit. (integer)
         */
        requestBody: _filesets_creation;
      };
      res: {
        /**
         * `Created`
         */
        201: {
          data?: _fileset_creation;
        };
        /**
         * `Bad Request`
         */
        400: _bad_request_14;
        /**
         * `Unauthorized`
         */
        401: _unauthorized;
        /**
         * `Forbidden` (you need permission to `upload` to the specified corpus).
         */
        403: _forbidden_normal;
        /**
         * `Not Found`
         */
        404: _not_found_404;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/filesets/{filesetId}": {
    get: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * ID of file subdirectories. If sets to 0 it will return top-level folder of documents, so if you have a web corpora with folders web1 and web2 it will return web1.
         */
        filesetId: number;
      };
      res: {
        /**
         * `OK`
         */
        200: {
          data?: _fileset;
        };
        /**
         * `Unauthorized`
         */
        401: _unauthorized;
        /**
         * `Forbidden` (you need permission to `view`).
         */
        403: _forbidden_normal;
        /**
         * `Not Found`
         */
        404: _not_found_404;
      };
    };
    delete: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * ID of file subdirectories. If sets to 0 it will return top-level folder of documents, so if you have a web corpora with folders web1 and web2 it will return web1.
         */
        filesetId: number;
      };
      res: {
        /**
         * `No Content`
         */
        204: void;
        /**
         * `Bad Request`
         */
        400: _bad_request_12;
        /**
         * `Unauthorized`
         */
        401: _unauthorized;
        /**
         * `Forbidden` (you need permission to `delete`).
         */
        403: _forbidden_normal;
        /**
         * `Not Found`
         */
        404: _not_found_404;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/filesets/{filesetId}/cancel_job": {
    post: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * ID of file subdirectories. If sets to 0 it will return top-level folder of documents, so if you have a web corpora with folders web1 and web2 it will return web1.
         */
        filesetId: number;
        /**
         *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
         */
        requestBody?: _empty_request;
      };
      res: {
        /**
         * `OK`
         */
        200: _rpc_style;
        /**
         * `Unauthorized`
         */
        401: _unauthorized_rpc;
        /**
         * `Forbidden` (you need `upload` permission).
         */
        403: _forbidden;
        /**
         * `Not Found`
         */
        404: _not_found_RPC;
      };
    };
  };
  "/ca/api/corpora/{corpusId}/filesets/{filesetId}/get_progress": {
    post: {
      req: {
        /**
         * Numeric corpus ID. For corpora querying.
         */
        corpusId: number;
        /**
         * ID of file subdirectories. If sets to 0 it will return top-level folder of documents, so if you have a web corpora with folders web1 and web2 it will return web1.
         */
        filesetId: number;
        /**
         *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
         */
        requestBody?: _empty_request;
      };
      res: {
        /**
         * `OK`
         */
        200: _filesets_get_progress;
        /**
         * `Forbidden` (you need `view` permission).
         */
        403: _forbidden;
        /**
         * `Not Found`
         */
        404: _not_found_RPC;
      };
    };
  };
  "/ca/api/languages": {
    get: {
      res: {
        /**
         * `OK`
         */
        200: {
          data?: Array<_language>;
        };
      };
    };
  };
  "/ca/api/somefiles": {
    post: {
      req: {
        /**
         * Aligned multilingual file (mostly in '.tmx' file type).
         */
        requestBody?: string;
      };
      res: {
        /**
         * `Created`
         */
        201: _somefiles_post;
        /**
         * `Bad Request`
         */
        400: _bad_request_20;
      };
    };
  };
  "/ca/api/somefiles/{somefileId}": {
    get: {
      req: {
        /**
         * Alphanumeric multilanguage file ID
         */
        somefileId: string;
      };
      res: {
        /**
         * `OK`
         */
        200: _somefiles_post;
        /**
         * `Not Found`
         */
        404: _not_found_404;
      };
    };
    put: {
      req: {
        /**
         *  - `corpora`
         *
         * - `guessed_language_code`
         *
         * - `language_id` => Language iso-code. **ISO 639-1**. (string)
         *
         * - `name` => Language name in **English**. (string)
         */
        requestBody?: _somefiles_put;
        /**
         * Alphanumeric multilanguage file ID
         */
        somefileId: string;
      };
      res: {
        /**
         * `OK`
         */
        200: {
          data?: _corpora_single_full;
        };
        /**
         * `Bad Request`
         */
        400: _bad_request_13;
        /**
         * `Not Found`
         */
        404: _not_found_404;
      };
    };
  };
  "/ca/api/tagsets/{templateId}": {
    get: {
      req: {
        /**
         * Numerical template ID, but preloaded templates do not have ID but you can query them by their name. Example: `UNIVERSAL_3`.
         */
        templateId: string;
      };
      res: {
        /**
         * `OK`
         */
        200: _template;
        /**
         * `Unauthorized`
         */
        401: _unauthorized;
        /**
         * `Forbidden` (you need permission to `read`).
         */
        403: _forbidden_normal;
        /**
         * `Not Found`
         */
        404: _not_found_404;
      };
    };
    put: {
      req: {
        /**
         *  - `id` => Alphanumeric **template/tagset ID**. The terms **tagset** and **templates** are interchangeable. (string)
         *
         * - `name` => Name of **template/tagset** file. (string)
         *
         * - `owner_id` => Unique numeric owner ID (usually you). If tagset/template is preloaded Null. (integer)
         *
         * - `owner_name` => Tagset/template owner name (usually you). If tagset/template is preloaded Null. (string)
         *
         * - `has_pipeline` => Vertical creation is supported. False for legacy templates. (boolean)
         *
         * - `has_tags` => Morphological tagging is supported. (boolean)
         *
         * - `has_lemmas` => Lemmatization is supported. (boolean)
         *
         * - `static_attributes` => A list of attributes which can appear in corpus.
         *
         * - `structures` => A list of used structures. Examples \<s>, \<g>.
         *
         * - `tagsetdoc` => URL leading to template/tagset documentation. (string)
         *
         * - `content` => Content of tagset. (string)
         *
         * - `default_sketchgrammar_id` => Not ID, as you probably imagine, but filename of preselected sketchgrammar (.m4 format). (string)
         *
         * - `default_termgrammar_id` => Not ID, as you probably imagine, but filename of preselected sketchgrammar (.m4 format). (string)
         *
         * - `sharing` => List.
         *
         * - `users` => The ID of user you share template with.
         *
         * - `id` => The ID of group you share template with.
         */
        requestBody?: _template_put;
        /**
         * Numerical template ID, but preloaded templates do not have ID but you can query them by their name. Example: `UNIVERSAL_3`.
         */
        templateId: string;
      };
      res: {
        /**
         * `OK`
         */
        200: {
          data?: _template;
        };
        /**
         * `Bad Request`
         */
        400: _bad_request_13;
        /**
         * `Unauthorized`
         */
        401: _unauthorized;
        /**
         * `Forbidden` (you need to be `owner` of the template).
         */
        403: _forbidden_normal;
        /**
         * `Not Found`
         */
        404: _not_found_404;
      };
    };
    delete: {
      req: {
        /**
         * Numerical template ID, but preloaded templates do not have ID but you can query them by their name. Example: `UNIVERSAL_3`.
         */
        templateId: string;
      };
      res: {
        /**
         * `No Content`
         */
        204: void;
        /**
         * `Bad Request`
         */
        400: _bad_request_13;
        /**
         * `Unauthorized`
         */
        401: _unauthorized;
        /**
         * `Forbidden` (you need to be `owner` of the template).
         */
        403: _forbidden_normal;
        /**
         * `Not Found`
         */
        404: _not_found_404;
      };
    };
  };
  "/ca/api/users/me/get_used_space": {
    post: {
      req: {
        /**
         *  In this documentation, an empty request is used mostly used with the **RPC style** method where the content in a request is not needed (in most cases). RPC style endpoints focus on performing **one action** right (procedures, command) easier than **REST API**-based endpoints. It is not as scalable as REST API style. RPC is mostly used with HTTP: GET (to fetch information) and POST (to everything else) in CA api is it used with POST HTTP method.
         */
        requestBody?: _empty_request;
      };
      res: {
        /**
         * `OK`
         */
        200: _get_used_space;
        /**
         * `Unauthorized`
         */
        401: _unauthorized_rpc;
      };
    };
  };
};
